<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>原创-spark中parquet文件写入优化 | Nora-一个程序媛的窝</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="标签（空格分隔）： 性能优化  在我们的实际使用中，经常需要将原始的文本文件转换为parquet列存储格式，以便后续查询的时候使用。写parquet能提高后续表查询效率这个事情我们不多说，下面讨论一下写parquet文件的效率问题： 我们来看一下两段程序： 1.使用case class作为df转换 package com.yundata.transtoparquet  import java.la">
<meta name="keywords">
<meta property="og:type" content="article">
<meta property="og:title" content="原创-spark中parquet文件写入优化">
<meta property="og:url" content="http://yoursite.com/2017/04/13/spark中parquet文件写入优化/index.html">
<meta property="og:site_name" content="Nora-一个程序媛的窝">
<meta property="og:description" content="标签（空格分隔）： 性能优化  在我们的实际使用中，经常需要将原始的文本文件转换为parquet列存储格式，以便后续查询的时候使用。写parquet能提高后续表查询效率这个事情我们不多说，下面讨论一下写parquet文件的效率问题： 我们来看一下两段程序： 1.使用case class作为df转换 package com.yundata.transtoparquet  import java.la">
<meta property="og:updated_time" content="2017-04-13T09:22:34.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="原创-spark中parquet文件写入优化">
<meta name="twitter:description" content="标签（空格分隔）： 性能优化  在我们的实际使用中，经常需要将原始的文本文件转换为parquet列存储格式，以便后续查询的时候使用。写parquet能提高后续表查询效率这个事情我们不多说，下面讨论一下写parquet文件的效率问题： 我们来看一下两段程序： 1.使用case class作为df转换 package com.yundata.transtoparquet  import java.la">
  
    <link rel="alternate" href="/atom.xml" title="Nora-一个程序媛的窝" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Nora-一个程序媛的窝</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-spark中parquet文件写入优化" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/13/spark中parquet文件写入优化/" class="article-date">
  <time datetime="2017-04-13T09:22:34.000Z" itemprop="datePublished">2017-04-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/性能优化/">性能优化</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      原创-spark中parquet文件写入优化
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      <span id="busuanzi_container_page_pv">
        <br /><br /> 阅读次数<span id="busuanzi_value_page_pv"></span>
        </span>
      
        <p>标签（空格分隔）： 性能优化</p>
<hr>
<p>在我们的实际使用中，经常需要将原始的文本文件转换为parquet列存储格式，以便后续查询的时候使用。写parquet能提高后续表查询效率这个事情我们不多说，下面讨论一下写parquet文件的效率问题：</p>
<p>我们来看一下两段程序：</p>
<p>1.使用case class作为df转换</p>
<pre><code>package com.yundata.transtoparquet

import java.lang.Exception
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._
import org.apache.spark.sql._
import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.sql.Row

import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary,Statistics}
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel
import org.apache.spark.mllib.util.MLUtils

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

import org.apache.spark.mllib.tree.GradientBoostedTrees
import org.apache.spark.mllib.tree.configuration.BoostingStrategy
import org.apache.spark.mllib.tree.model.GradientBoostedTreesModel
import org.apache.spark.mllib.util.MLUtils

import org.apache.spark.rdd.PairRDDFunctions

import scala.collection.mutable.ArrayBuffer


import org.apache.spark.sql.types.{StructType,StructField,StringType};
import org.apache.spark.sql.Row;


case class formatrow(fs_id:Long,user_id:Long,app_id:Long,parent_path:String,server_filename:String,s3_handle:String,size:Long,server_mtime:String,server_ctime:Long,local_mtime:Long,local_ctime:Long,isdir:Long,isdelete:Long,status:Long,category:Long,object_key:String,extent_int2:Long,recompute_tag:Long,user_range:Long,md5_range:Long,event_day:String) extends Product;


/**
 * Created by robert on 15-5-18.
 */
class transtoparquet(userConfFile: String,sourceFile:String,destFile:String,event_day:String) extends Serializable{



  def run(): Unit = {

    transtoparquetconf(userConfFile);

    val conf=transtoparquetconf.getSparkConf();
    conf.setAppName(conf.get(&quot;spark.app.name&quot;, this.getClass.getName));

    val sc=new SparkContext(conf);
    val sqlContext = new org.apache.spark.sql.SQLContext(sc);
    import sqlContext.implicits._

    sc.hadoopConfiguration.addResource(&quot;hdfs-site.xml&quot;);
    sc.hadoopConfiguration.set(&quot;parquet.enable.summary-metadata&quot;, &quot;false&quot;)

    System.out.println(sc.hadoopConfiguration.get(&quot;dfs.ha.namenodes.bigfile&quot;));

    System.out.println(sc.hadoopConfiguration.toString);



    val txtfile = sc.textFile(sourceFile);

    txtfile.take(10).foreach(println);
    println(sourceFile);


    val txtdf=txtfile.filter(_.split(&quot;\t&quot;).size==18).map(
      x=&gt;{

            var s3_handle=x.split(&quot;\t&quot;)(5);
            var md5_range=Long2long(0);
            if(x.split(&quot;\t&quot;)(5)==&quot;&quot;)
              {
                s3_handle=(new scala.util.Random).nextInt(99).toString(); //取0-99之间的随机数，保证其散列开来
                md5_range=s3_handle.toLong;
              }
            else if(x.split(&quot;\t&quot;)(5).size==32)
            {
                md5_range=((s3_handle.toLowerCase.substring(0, 24).toList.map(&quot;0123456789abcdef&quot;.indexOf(_)).map(BigInt(_)).reduceLeft( _ * 32 + _))%100).toLong
            }
            else
            {
                md5_range = Long2long(-1);
            }
            var user_range=(x.split(&quot;\t&quot;)(0)).toLong/100000000;


            if(user_range&gt;=100)
            {
                user_range=100;
            }

            if(md5_range == Long2long(-1))
              {
                null
              }


            else {
              formatrow((x.split(&quot;\t&quot;)(0)).toLong, x.split(&quot;\t&quot;)(1).toLong, x.split(&quot;\t&quot;)(2).toLong, x.split(&quot;\t&quot;)(3), x.split(&quot;\t&quot;)(4), s3_handle, (x.split(&quot;\t&quot;)(6)).toLong, x.split(&quot;\t&quot;)(7), (x.split(&quot;\t&quot;)(8)).toLong, (x.split(&quot;\t&quot;)(9)).toLong, (x.split(&quot;\t&quot;)(10)).toLong, (x.split(&quot;\t&quot;)(11)).toLong, (x.split(&quot;\t&quot;)(12)).toLong, (x.split(&quot;\t&quot;)(13)).toLong, (x.split(&quot;\t&quot;)(14)).toLong, x.split(&quot;\t&quot;)(15), (x.split(&quot;\t&quot;)(16)).toLong, (x.split(&quot;\t&quot;)(17)).toLong, user_range, md5_range, event_day);
            }
      }
    ).filter(_ != null).toDF();


    txtdf.repartition(5).write.mode(org.apache.spark.sql.SaveMode.Append).partitionBy(&quot;user_range&quot;,&quot;md5_range&quot;,&quot;event_day&quot;).parquet(destFile);

  }



}



object transtoparquet{

  def main(args: Array[String]): Unit = {
    if (args.size != 4) {
      println(&quot;usage: com.yundata.transtoparquet.transtoparquet config srcfile destfile event_day&quot;)
      return
    }
    new transtoparquet(args(0),args(1),args(2),args(3)).run()
  }
  }
</code></pre><p>这个程序实际测试，原始数据9G，压缩后大概是5.5G左右，使用50个核跑了好几个小时，居然都没有写完数据，看executor日志，几B几B地在往hdfs当中去写日志。崩溃，于是，换了一种写法。</p>
<p>2.使用row作为df转换</p>
<pre><code>package com.yundata.transtoparquet


import java.lang.Exception
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._
import org.apache.spark.sql._
import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.sql.Row

import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary,Statistics}
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel
import org.apache.spark.mllib.util.MLUtils

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

import org.apache.spark.mllib.tree.GradientBoostedTrees
import org.apache.spark.mllib.tree.configuration.BoostingStrategy
import org.apache.spark.mllib.tree.model.GradientBoostedTreesModel
import org.apache.spark.mllib.util.MLUtils

import org.apache.spark.rdd.PairRDDFunctions

import scala.collection.mutable.ArrayBuffer


import org.apache.spark.sql.types.{StructType,StructField,StringType,LongType};
import org.apache.spark.sql.Row;

/**
 * Created by robert on 15-5-18.
 */
class transtoparquet(userConfFile: String,sourceFile:String,destFile:String,event_day:String) extends Serializable{

  def run(): Unit = {

    transtoparquetconf(userConfFile);

    val conf=transtoparquetconf.getSparkConf();
    conf.setAppName(conf.get(&quot;spark.app.name&quot;, this.getClass.getName));

    val sc=new SparkContext(conf);
    val sqlContext = new org.apache.spark.sql.SQLContext(sc);
    import sqlContext.implicits._

    sc.hadoopConfiguration.addResource(&quot;hdfs-site.xml&quot;);
    sc.hadoopConfiguration.set(&quot;parquet.enable.summary-metadata&quot;, &quot;false&quot;)

    System.out.println(sc.hadoopConfiguration.get(&quot;dfs.ha.namenodes.bigfile&quot;));

    System.out.println(sc.hadoopConfiguration.toString);



    val txtfile = sc.textFile(sourceFile);

    txtfile.take(10).foreach(println);
    println(sourceFile);

    val schema=StructType(Array(StructField(&quot;fs_id&quot;,LongType,true), StructField(&quot;user_id&quot;,LongType,true),StructField(&quot;app_id&quot;,LongType,true),StructField(&quot;parent_path&quot;,StringType,true),StructField(&quot;server_filename&quot;,StringType,true),StructField(&quot;s3_handle&quot;,StringType,true),StructField(&quot;size&quot;,LongType,true),StructField(&quot;server_mtime&quot;,StringType,true),StructField(&quot;server_ctime&quot;,LongType,true),StructField(&quot;local_mtime&quot;,LongType,true),StructField(&quot;local_ctime&quot;,LongType,true),StructField(&quot;isdir&quot;,LongType,true),StructField(&quot;isdelete&quot;,LongType,true),StructField(&quot;status&quot;,LongType,true),StructField(&quot;category&quot;,LongType,true),StructField(&quot;object_key&quot;,StringType,true),StructField(&quot;extent_int2&quot;,LongType,true),StructField(&quot;recompute_tag&quot;,LongType,true),StructField(&quot;user_range&quot;,LongType,true),StructField(&quot;md5_range&quot;,LongType,true),StructField(&quot;event_day&quot;,StringType,true)));

    val txtrow=txtfile.filter(_.split(&quot;\t&quot;).size==18).map(
      x=&gt;{

            var s3_handle=x.split(&quot;\t&quot;)(5);
            var md5_range=Long2long(0);
            if(x.split(&quot;\t&quot;)(5)==&quot;&quot;)
              {
                s3_handle=(new scala.util.Random).nextInt(99).toString(); //取0-99之间的随机数，保证其散列开来
                md5_range=s3_handle.toLong;
              }
            else if(x.split(&quot;\t&quot;)(5).size==32)
            {
                md5_range=((s3_handle.toLowerCase.substring(0, 24).toList.map(&quot;0123456789abcdef&quot;.indexOf(_)).map(BigInt(_)).reduceLeft( _ * 32 + _))%100).toLong
            }
            else
            {
                md5_range = Long2long(-1);
            }
            var user_range=(x.split(&quot;\t&quot;)(0)).toLong/100000000;


            if(user_range&gt;=100)
            {
                user_range=100;
            }

            if(md5_range == Long2long(-1))
              {
                null
              }
            else {
              Row((x.split(&quot;\t&quot;)(0)).toLong, x.split(&quot;\t&quot;)(1).toLong, x.split(&quot;\t&quot;)(2).toLong, x.split(&quot;\t&quot;)(3), x.split(&quot;\t&quot;)(4), s3_handle, (x.split(&quot;\t&quot;)(6)).toLong, x.split(&quot;\t&quot;)(7), (x.split(&quot;\t&quot;)(8)).toLong, (x.split(&quot;\t&quot;)(9)).toLong, (x.split(&quot;\t&quot;)(10)).toLong, (x.split(&quot;\t&quot;)(11)).toLong, (x.split(&quot;\t&quot;)(12)).toLong, (x.split(&quot;\t&quot;)(13)).toLong, (x.split(&quot;\t&quot;)(14)).toLong, x.split(&quot;\t&quot;)(15), (x.split(&quot;\t&quot;)(16)).toLong, (x.split(&quot;\t&quot;)(17)).toLong, user_range, md5_range, event_day);
            }
      }
    ).filter(_ != null);




    val txtdf=sqlContext.createDataFrame(txtrow, schema);

    txtdf.repartition(5).write.mode(org.apache.spark.sql.SaveMode.Append).partitionBy(&quot;user_range&quot;,&quot;md5_range&quot;,&quot;event_day&quot;).parquet(destFile);
  }
}



object transtoparquet{

  def main(args: Array[String]): Unit = {

    if (args.size != 4) {
      println(&quot;usage: com.yundata.transtoparquet.transtoparquet config srcfile destfile event_day&quot;)
      return
    }
    new transtoparquet(args(0),args(1),args(2),args(3)).run()
  }
</code></pre><p>   }</p>
<p>使用了这个程序之后，很快，18分钟，就将5G parquet数据全部写入了。</p>
<p>在上述程序中，我使用的配置是：</p>
<pre><code>spark.app.name = TRANSTOPARQUET-JOB
spark.master = spark://sparkmaster:8650
spark.cores.max=50
spark.executor.instances=50
spark.executor.memory=2g
spark.speculation=true
spark.driver.maxResultSize=2g
spark.ui.port=8221
spark.ui.retainedStages=20
spark.ui.retainedJobs=20
spark.sql.parquet.compression.codec=snappy
</code></pre><p>但是这个程序还有一个问题是，如果repartition不设置的话，最后写入的文件数会非常多，大概是num(user_range)<em>num(md5_range)</em>num(event_day)*num(repartition)，很可能会瞬间打爆namenode的内存。因此repartition要设置得非常小，这又导致了整个程序会非常慢。</p>
<p>3)优化一下repartition的方式</p>
<p>原先使用repartiton(5)的方式的时候，是随机分区，导致所有的task都基本可能有每一个分区的数据，所以导致每个分区下面都有5个文件，但是如果我按照需要的分区来作哈希的话，例如repartition(user_range,md5_range,event_day)来的话，那么每个分区的数据只会存在在一个最后写入的task任务中，也就保证了整个任务产生的分区数最大是num(user_range)<em>num(md5_range)</em>num(event_day)</p>
<p>而不是原来的num(repartition)<em>num(user_range)</em>num(md5_range)*num(event_day)</p>
<p>因此我们可以随意启并发数。</p>
<p>将写入代码修改为：</p>
<p>txtdf.repartition(txtdf(“user_range”), txtdf(“md5_range”), txtdf(“event_day”)).write.mode(org.apache.spark.sql.SaveMode.Append).partitionBy(“user_range”,”md5_range”,”event_day”).parquet(destFile)</p>
<p>这样修改了之后。数据5分钟左右就全部写入了。</p>
<p>  9208         4556         3297227427 /horus/users/chenxiue</p>
<p>数据的大小也比之前小了一些，因为文件更加集中了。</p>
<p>下面我们对比一下gzip压缩和snappy压缩的效果：</p>
<p>1）压缩比率</p>
<p>原始文件大小：</p>
<p>1          517         9359700501 hdfs://namenode:8700/pika_data/file_meta_data_20170101_part4/2017011421/1483873213877</p>
<p>snappy压缩产出的文件格式类似：part-r-00000-b3ff5d89-8885-42f1-bb3d-e8dc6fb692a0.snappy.parquet</p>
<p>压缩后的文件大小：<br>9206        22157         5501724946 /horus/users/chenxiue</p>
<p>大概花了18分钟左右。</p>
<p>gzip压缩是类似：part-r-00003-be2d54a8-088e-4970-be30-363747930a6e.gz.parquet这样的文件</p>
<p>大概也花了18分钟左右<br>9206        22157         3958666080 /horus/users/chenxiue1</p>
<p>可以看出gzip的压缩比更加大一些。</p>
<p>2）在随机分区得情况下，我们尝试加大最后写入的并发度，看看会不会有加速？</p>
<p>txtdf.repartition(10).write.mode(org.apache.spark.sql.SaveMode.Append).partitionBy(“user_range”,”md5_range”,”event_day”).parquet(destFile);</p>
<p>结果写入花了19分钟？？？为啥？？<br>9208        44158         4259882305 /horus/users/chenxiue<br>发现写入的大小比原先的稍微大一些。</p>
<p>怀疑是数据太小，主要时间花在建立文件上。repartiton越大的话，文件数就越多。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/13/spark中parquet文件写入优化/" data-id="ckaafjsgg0004ci3cmmg8o5qi" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/04/13/在kerberos-HA环境下的ranger编译安装/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          原创-在kerberos+HA环境下的ranger编译安装
        
      </div>
    </a>
  
  
    <a href="/2017/04/19/spark-on-yarn情况下historyserver的配置/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">原创-spark-on-yarn情况下historyserver的配置</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/性能优化/">性能优化</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/源码调试/">源码调试</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/部署文档/">部署文档</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/05/17/hugegraph-cassadra底层存储实现/">hugegraph+cassadra底层存储实现</a>
          </li>
        
          <li>
            <a href="/2020/05/17/spark-graphx计算pagerank源代码分析/">spark graphx计算pagerank源代码分析</a>
          </li>
        
          <li>
            <a href="/2018/03/29/druid安装配置-数据导入/">druid安装配置&amp;数据导入</a>
          </li>
        
          <li>
            <a href="/2018/03/02/spark远程调试/">spark远程调试</a>
          </li>
        
          <li>
            <a href="/2018/01/11/原创－flume写hdfs性能优化/">原创-flume写hdfs性能优化</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 XiueChen<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
      <span id="busuanzi_container_site_uv"><br />本站访客数<span id="busuanzi_value_site_uv"></span>人次
      <span id="busuanzi_container_site_pv"><br />本站访问次数<span id="busuanzi_value_site_pv"></span>
      </span>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>