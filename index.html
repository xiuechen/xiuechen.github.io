<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Nora-一个程序媛的窝</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Nora-一个程序媛的窝">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Nora-一个程序媛的窝">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Nora-一个程序媛的窝">
  
    <link rel="alternate" href="/atom.xml" title="Nora-一个程序媛的窝" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Nora-一个程序媛的窝</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-spark远程调试" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/02/spark远程调试/" class="article-date">
  <time datetime="2018-03-02T07:12:34.000Z" itemprop="datePublished">2018-03-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/02/spark远程调试/">spark远程调试</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      <span id="busuanzi_container_page_pv">
        <br /><br /> 阅读次数<span id="busuanzi_value_page_pv"></span>
        </span>
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/02/spark远程调试/" data-id="cje9m0tlt0003je3c42lujow8" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-原创－flume写hdfs性能优化" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/11/原创－flume写hdfs性能优化/" class="article-date">
  <time datetime="2018-01-11T03:43:57.000Z" itemprop="datePublished">2018-01-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/性能优化/">性能优化</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/11/原创－flume写hdfs性能优化/">原创-flume写hdfs性能优化</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      <span id="busuanzi_container_page_pv">
        <br /><br /> 阅读次数<span id="busuanzi_value_page_pv"></span>
        </span>
      
        <p>标签（空格分隔）： 未分类</p>
<hr>
<p>flume在写hdfs的时候，每接收到一个event就会调用bucketwriter.java的append(event)方法，在append当中每次都会去检查shouldrotate,我们来看一下这块的逻辑：</p>
<pre><code>private boolean shouldRotate() {
    boolean doRotate = false;

    if (writer.isUnderReplicated()) {
      this.isUnderReplicated = true;
      doRotate = true;
    } else {
      this.isUnderReplicated = false;
    }

    if ((rollCount &gt; 0) &amp;&amp; (rollCount &lt;= eventCounter)) {
      LOG.debug(&quot;rolling: rollCount: {}, events: {}&quot;, rollCount, eventCounter);
      doRotate = true;
    }

    if ((rollSize &gt; 0) &amp;&amp; (rollSize &lt;= processSize)) {
      LOG.debug(&quot;rolling: rollSize: {}, bytes: {}&quot;, rollSize, processSize);
      doRotate = true;
    }

    return doRotate;
  }
</code></pre><p>我们可以看到这里每次都会调用writer.isUnderReplicated()，如果副本数低于预期，那么不管有没有达到rollcount或者rollsize就rotate？？（这块可能是说上一个event如果副本数没有达到预期，那么就需要重新开一个新文件去写入，为啥要这样设计？不太清楚。。。）</p>
<pre><code>public boolean isUnderReplicated() {
    try {
      int numBlocks = getNumCurrentReplicas();
      if (numBlocks == -1) {
        return false;
      }
      int desiredBlocks;
      if (configuredMinReplicas != null) {
        desiredBlocks = configuredMinReplicas;
      } else {
        desiredBlocks = getFsDesiredReplication();
      }
      return numBlocks &lt; desiredBlocks;
    } catch (IllegalAccessException e) {
      logger.error(&quot;Unexpected error while checking replication factor&quot;, e);
    } catch (InvocationTargetException e) {
      logger.error(&quot;Unexpected error while checking replication factor&quot;, e);
    } catch (IllegalArgumentException e) {
      logger.error(&quot;Unexpected error while checking replication factor&quot;, e);
    }
    return false;
  }
</code></pre><p>副本获取的代码如下：</p>
<pre><code>/**
   * This method gets the datanode replication count for the current open file.
   *
   * If the pipeline isn&apos;t started yet or is empty, you will get the default
   * replication factor.
   *
   * &lt;p/&gt;If this function returns -1, it means you
   * are not properly running with the HDFS-826 patch.
   * @throws InvocationTargetException
   * @throws IllegalAccessException
   * @throws IllegalArgumentException
   */
  public int getNumCurrentReplicas()
      throws IllegalArgumentException, IllegalAccessException,
          InvocationTargetException {
    if (refGetNumCurrentReplicas != null &amp;&amp; outputStream != null) {
      OutputStream dfsOutputStream = outputStream.getWrappedStream();
      if (dfsOutputStream != null) {
        Object repl = refGetNumCurrentReplicas.invoke(dfsOutputStream, NO_ARGS);
        if (repl instanceof Integer) {
          return ((Integer)repl).intValue();
        }
      }
    }
    return -1;
  }
</code></pre><p>我们可以看到每一次event写入都要去和hdfs交互一次，这个代价非常高。而实际上在我们的工作当中我们并没有用rollcount和rollsize，而是使用了rollinterval，rollinterval的逻辑并不在这里,在open()里面有这么一段逻辑：</p>
<pre><code>if (rollInterval &gt; 0) {
      Callable&lt;Void&gt; action = new Callable&lt;Void&gt;() {
        public Void call() throws Exception {
          LOG.debug(&quot;Rolling file ({}): Roll scheduled after {} sec elapsed.&quot;,
              bucketPath, rollInterval);
          try {
            // Roll the file and remove reference from sfWriters map.
            close(true);
          } catch(Throwable t) {
            LOG.error(&quot;Unexpected error&quot;, t);
          }
          return null;
        }
      };
      timedRollFuture = timedRollerPool.schedule(action, rollInterval,TimeUnit.SECONDS);
</code></pre><p>我们回过头来看一下hdfseventsink类的process方法：</p>
<pre><code>public Status process() throws EventDeliveryException {
    Channel channel = getChannel();
    Transaction transaction = channel.getTransaction();
    List&lt;BucketWriter&gt; writers = Lists.newArrayList();
    transaction.begin();
    try {
      int txnEventCount = 0;
      for (txnEventCount = 0; txnEventCount &lt; batchSize; txnEventCount++) {
        Event event = channel.take();
        if (event == null) {
          break;
        }

        // reconstruct the path name by substituting place holders
        String realPath = BucketPath.escapeString(filePath, event.getHeaders(),
            timeZone, needRounding, roundUnit, roundValue, useLocalTime);
        String realName = BucketPath.escapeString(fileName, event.getHeaders(),
          timeZone, needRounding, roundUnit, roundValue, useLocalTime);

        String lookupPath = realPath + DIRECTORY_DELIMITER + realName;
        BucketWriter bucketWriter;
        HDFSWriter hdfsWriter = null;
        // Callback to remove the reference to the bucket writer from the
        // sfWriters map so that all buffers used by the HDFS file
        // handles are garbage collected.
        WriterCallback closeCallback = new WriterCallback() {
          @Override
          public void run(String bucketPath) {
            LOG.info(&quot;Writer callback called.&quot;);
            synchronized (sfWritersLock) {
              sfWriters.remove(bucketPath);
            }
          }
        };
        synchronized (sfWritersLock) {
          bucketWriter = sfWriters.get(lookupPath);
          // we haven&apos;t seen this file yet, so open it and cache the handle
          if (bucketWriter == null) {
            hdfsWriter = writerFactory.getWriter(fileType);
            bucketWriter = initializeBucketWriter(realPath, realName,
              lookupPath, hdfsWriter, closeCallback);
            sfWriters.put(lookupPath, bucketWriter);
          }
        }

        // track the buckets getting written in this transaction
        if (!writers.contains(bucketWriter)) {
          writers.add(bucketWriter);
        }

        // Write the data to HDFS
        try {
          bucketWriter.append(event);
        } catch (BucketClosedException ex) {
          LOG.info(&quot;Bucket was closed while trying to append, &quot; +
            &quot;reinitializing bucket and writing event.&quot;);
          hdfsWriter = writerFactory.getWriter(fileType);
          bucketWriter = initializeBucketWriter(realPath, realName,
            lookupPath, hdfsWriter, closeCallback);
          synchronized (sfWritersLock) {
            sfWriters.put(lookupPath, bucketWriter);
          }
          bucketWriter.append(event);
        }
      }

      if (txnEventCount == 0) {
        sinkCounter.incrementBatchEmptyCount();
      } else if (txnEventCount == batchSize) {
        sinkCounter.incrementBatchCompleteCount();
      } else {
        sinkCounter.incrementBatchUnderflowCount();
      }

      // flush all pending buckets before committing the transaction
      for (BucketWriter bucketWriter : writers) {
        bucketWriter.flush();
      }

      transaction.commit();

      if (txnEventCount &lt; 1) {
        return Status.BACKOFF;
      } else {
        sinkCounter.addToEventDrainSuccessCount(txnEventCount);
        return Status.READY;
      }
    } catch (IOException eIO) {
      transaction.rollback();
      LOG.warn(&quot;HDFS IO error&quot;, eIO);
      return Status.BACKOFF;
    } catch (Throwable th) {
      transaction.rollback();
      LOG.error(&quot;process failed&quot;, th);
      if (th instanceof Error) {
        throw (Error) th;
      } else {
        throw new EventDeliveryException(th);
      }
    } finally {
      transaction.close();
    }
  }
</code></pre><p>hdfseventsink当中维护了map<lookuppath,bucketwriter>的sfwrites哈希表，每次event来的时候，会去sfwriters当中寻找对应path(根据hdfs.path以及hdfs.fileprefix来决定)的bucketwriter作为当前写入的handle.如果找不到，就创建一个，并且将新的句炳加入到sfwriters当中去。因此实际上对于上述bucketwriter中的rotate来说，只是针对同一个路径内部的，如果没有一个路径内rollsize和rollcount滚动的需求，以及数据一定要多副本写入的需求，完全可以把bucketwriter内部判断rotate的逻辑注释掉，这样能够大大提升flume写入能力.</lookuppath,bucketwriter></p>
<p>agent.sinks.sk_cloudui.hdfs.path = hdfs://bigfile/om/anticrack/cloudui.log/%Y%m%d/%k%M/<br>agent.sinks.sk_cloudui.hdfs.filePrefix = qd01-pcsdata45.qd01.baidu.com</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/01/11/原创－flume写hdfs性能优化/" data-id="cje9m0tm70007je3cj8wjhno4" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-spark-on-yarn情况下historyserver的配置" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/19/spark-on-yarn情况下historyserver的配置/" class="article-date">
  <time datetime="2017-04-19T09:39:44.000Z" itemprop="datePublished">2017-04-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/部署文档/">部署文档</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/19/spark-on-yarn情况下historyserver的配置/">原创-spark-on-yarn情况下historyserver的配置</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      <span id="busuanzi_container_page_pv">
        <br /><br /> 阅读次数<span id="busuanzi_value_page_pv"></span>
        </span>
      
        <p>标签（空格分隔）： 未分类</p>
<hr>
<p>我们经常需要在一个app执行完成之后，去对这个app的执行情况进行分析，不管是分析它如何失败也好，或者是分析这个任务执行过程是否需要优化。那么这时候我们就需要用到historyserver.</p>
<p>首先log分为两种：1)标准输入输出的log 2）spark event log<br>对应yarn页面上的两个按钮：1)logs   2)history</p>
<p>###logs配置<br>对于yarn来说，logs这块需要配置：(所有nodemanager的机器上都需要修改该配置)</p>
<pre><code>&lt;property&gt;
  &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt; 
  &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;
  &lt;value&gt;259200&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;yarn.log.server.url&lt;/name&gt;
    &lt;value&gt;http://hostA:8937/jobhistory/logs/&lt;/value&gt;（yarn会在用户点击历史任务logs的时候跳转到这个url,这个url提供的jobhistory server是mapreduce的功能）
  &lt;/property&gt;

 &lt;property&gt; 
  &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt;
  &lt;value&gt;/tmp/logs/yarn/&lt;/value&gt;(yarn会负责任务结束后将地址转存到这个位置)
  &lt;/property&gt;
</code></pre><p>在hostA上修改mapred-site.xml配置：<br>对于mapred-site.xml来说需要配置：(./sbin/mr-jobhistory-daemon.sh start historyserver)</p>
<pre><code>&lt;property&gt;
&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
&lt;value&gt;hostA:8927&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
&lt;value&gt;hostB:8937&lt;/value&gt;
&lt;/property&gt;
</code></pre><p>(historyserver挂掉，检查相关的几个目录是不是满了)</p>
<p>在hostA上执行$HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver启动historyserver。</p>
<p>这个步骤完成后在yarn页面上点击app的logs按钮就可以看到该app的日志，不管这个app执行完成与否。</p>
<p>###event log配置<br>对于history来说。yarn会负责跳转到app自身指定的history server当中去。(在hostB机器上$SPARK_HOME/sbin/start-history-server.sh启动historyserver)</p>
<pre><code>spark.eventLog.enabled           true
spark.eventLog.dir  hdfs://hostNamenode:8900/spark-event-2.0
spark.yarn.historyServer.address hostB:8651
spark.history.fs.logDirectory hdfs://hostNamenode:8900/spark-event-2.0
spark.history.retainedApplications 1000
spark.history.ui.port 8651
spark.history.fs.cleaner.enabled true
spark.history.fs.cleaner.interval 1d
spark.history.fs.cleaner.maxAge 3d
spark.executor.extraJavaOptions  -XX:+PrintGCDetails -XX:+PrintGCTimeStamps
</code></pre><p>这个完成后，就可以通过history按钮看到spark event log，不管该app执行完成与否。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/19/spark-on-yarn情况下historyserver的配置/" data-id="cje9m0tkj0000je3cpmrzahwb" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-spark中parquet文件写入优化" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/13/spark中parquet文件写入优化/" class="article-date">
  <time datetime="2017-04-13T09:22:34.000Z" itemprop="datePublished">2017-04-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/性能优化/">性能优化</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/13/spark中parquet文件写入优化/">原创-spark中parquet文件写入优化</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      <span id="busuanzi_container_page_pv">
        <br /><br /> 阅读次数<span id="busuanzi_value_page_pv"></span>
        </span>
      
        <p>标签（空格分隔）： 性能优化</p>
<hr>
<p>在我们的实际使用中，经常需要将原始的文本文件转换为parquet列存储格式，以便后续查询的时候使用。写parquet能提高后续表查询效率这个事情我们不多说，下面讨论一下写parquet文件的效率问题：</p>
<p>我们来看一下两段程序：</p>
<p>1.使用case class作为df转换</p>
<pre><code>package com.yundata.transtoparquet

import java.lang.Exception
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._
import org.apache.spark.sql._
import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.sql.Row

import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary,Statistics}
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel
import org.apache.spark.mllib.util.MLUtils

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

import org.apache.spark.mllib.tree.GradientBoostedTrees
import org.apache.spark.mllib.tree.configuration.BoostingStrategy
import org.apache.spark.mllib.tree.model.GradientBoostedTreesModel
import org.apache.spark.mllib.util.MLUtils

import org.apache.spark.rdd.PairRDDFunctions

import scala.collection.mutable.ArrayBuffer


import org.apache.spark.sql.types.{StructType,StructField,StringType};
import org.apache.spark.sql.Row;


case class formatrow(fs_id:Long,user_id:Long,app_id:Long,parent_path:String,server_filename:String,s3_handle:String,size:Long,server_mtime:String,server_ctime:Long,local_mtime:Long,local_ctime:Long,isdir:Long,isdelete:Long,status:Long,category:Long,object_key:String,extent_int2:Long,recompute_tag:Long,user_range:Long,md5_range:Long,event_day:String) extends Product;


/**
 * Created by robert on 15-5-18.
 */
class transtoparquet(userConfFile: String,sourceFile:String,destFile:String,event_day:String) extends Serializable{



  def run(): Unit = {

    transtoparquetconf(userConfFile);

    val conf=transtoparquetconf.getSparkConf();
    conf.setAppName(conf.get(&quot;spark.app.name&quot;, this.getClass.getName));

    val sc=new SparkContext(conf);
    val sqlContext = new org.apache.spark.sql.SQLContext(sc);
    import sqlContext.implicits._

    sc.hadoopConfiguration.addResource(&quot;hdfs-site.xml&quot;);
    sc.hadoopConfiguration.set(&quot;parquet.enable.summary-metadata&quot;, &quot;false&quot;)

    System.out.println(sc.hadoopConfiguration.get(&quot;dfs.ha.namenodes.bigfile&quot;));

    System.out.println(sc.hadoopConfiguration.toString);



    val txtfile = sc.textFile(sourceFile);

    txtfile.take(10).foreach(println);
    println(sourceFile);


    val txtdf=txtfile.filter(_.split(&quot;\t&quot;).size==18).map(
      x=&gt;{

            var s3_handle=x.split(&quot;\t&quot;)(5);
            var md5_range=Long2long(0);
            if(x.split(&quot;\t&quot;)(5)==&quot;&quot;)
              {
                s3_handle=(new scala.util.Random).nextInt(99).toString(); //取0-99之间的随机数，保证其散列开来
                md5_range=s3_handle.toLong;
              }
            else if(x.split(&quot;\t&quot;)(5).size==32)
            {
                md5_range=((s3_handle.toLowerCase.substring(0, 24).toList.map(&quot;0123456789abcdef&quot;.indexOf(_)).map(BigInt(_)).reduceLeft( _ * 32 + _))%100).toLong
            }
            else
            {
                md5_range = Long2long(-1);
            }
            var user_range=(x.split(&quot;\t&quot;)(0)).toLong/100000000;


            if(user_range&gt;=100)
            {
                user_range=100;
            }

            if(md5_range == Long2long(-1))
              {
                null
              }


            else {
              formatrow((x.split(&quot;\t&quot;)(0)).toLong, x.split(&quot;\t&quot;)(1).toLong, x.split(&quot;\t&quot;)(2).toLong, x.split(&quot;\t&quot;)(3), x.split(&quot;\t&quot;)(4), s3_handle, (x.split(&quot;\t&quot;)(6)).toLong, x.split(&quot;\t&quot;)(7), (x.split(&quot;\t&quot;)(8)).toLong, (x.split(&quot;\t&quot;)(9)).toLong, (x.split(&quot;\t&quot;)(10)).toLong, (x.split(&quot;\t&quot;)(11)).toLong, (x.split(&quot;\t&quot;)(12)).toLong, (x.split(&quot;\t&quot;)(13)).toLong, (x.split(&quot;\t&quot;)(14)).toLong, x.split(&quot;\t&quot;)(15), (x.split(&quot;\t&quot;)(16)).toLong, (x.split(&quot;\t&quot;)(17)).toLong, user_range, md5_range, event_day);
            }
      }
    ).filter(_ != null).toDF();


    txtdf.repartition(5).write.mode(org.apache.spark.sql.SaveMode.Append).partitionBy(&quot;user_range&quot;,&quot;md5_range&quot;,&quot;event_day&quot;).parquet(destFile);

  }



}



object transtoparquet{

  def main(args: Array[String]): Unit = {
    if (args.size != 4) {
      println(&quot;usage: com.yundata.transtoparquet.transtoparquet config srcfile destfile event_day&quot;)
      return
    }
    new transtoparquet(args(0),args(1),args(2),args(3)).run()
  }
  }
</code></pre><p>这个程序实际测试，原始数据9G，压缩后大概是5.5G左右，使用50个核跑了好几个小时，居然都没有写完数据，看executor日志，几B几B地在往hdfs当中去写日志。崩溃，于是，换了一种写法。</p>
<p>2.使用row作为df转换</p>
<pre><code>package com.yundata.transtoparquet


import java.lang.Exception
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._
import org.apache.spark.sql._
import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.sql.Row

import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary,Statistics}
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel
import org.apache.spark.mllib.util.MLUtils

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

import org.apache.spark.mllib.tree.GradientBoostedTrees
import org.apache.spark.mllib.tree.configuration.BoostingStrategy
import org.apache.spark.mllib.tree.model.GradientBoostedTreesModel
import org.apache.spark.mllib.util.MLUtils

import org.apache.spark.rdd.PairRDDFunctions

import scala.collection.mutable.ArrayBuffer


import org.apache.spark.sql.types.{StructType,StructField,StringType,LongType};
import org.apache.spark.sql.Row;

/**
 * Created by robert on 15-5-18.
 */
class transtoparquet(userConfFile: String,sourceFile:String,destFile:String,event_day:String) extends Serializable{

  def run(): Unit = {

    transtoparquetconf(userConfFile);

    val conf=transtoparquetconf.getSparkConf();
    conf.setAppName(conf.get(&quot;spark.app.name&quot;, this.getClass.getName));

    val sc=new SparkContext(conf);
    val sqlContext = new org.apache.spark.sql.SQLContext(sc);
    import sqlContext.implicits._

    sc.hadoopConfiguration.addResource(&quot;hdfs-site.xml&quot;);
    sc.hadoopConfiguration.set(&quot;parquet.enable.summary-metadata&quot;, &quot;false&quot;)

    System.out.println(sc.hadoopConfiguration.get(&quot;dfs.ha.namenodes.bigfile&quot;));

    System.out.println(sc.hadoopConfiguration.toString);



    val txtfile = sc.textFile(sourceFile);

    txtfile.take(10).foreach(println);
    println(sourceFile);

    val schema=StructType(Array(StructField(&quot;fs_id&quot;,LongType,true), StructField(&quot;user_id&quot;,LongType,true),StructField(&quot;app_id&quot;,LongType,true),StructField(&quot;parent_path&quot;,StringType,true),StructField(&quot;server_filename&quot;,StringType,true),StructField(&quot;s3_handle&quot;,StringType,true),StructField(&quot;size&quot;,LongType,true),StructField(&quot;server_mtime&quot;,StringType,true),StructField(&quot;server_ctime&quot;,LongType,true),StructField(&quot;local_mtime&quot;,LongType,true),StructField(&quot;local_ctime&quot;,LongType,true),StructField(&quot;isdir&quot;,LongType,true),StructField(&quot;isdelete&quot;,LongType,true),StructField(&quot;status&quot;,LongType,true),StructField(&quot;category&quot;,LongType,true),StructField(&quot;object_key&quot;,StringType,true),StructField(&quot;extent_int2&quot;,LongType,true),StructField(&quot;recompute_tag&quot;,LongType,true),StructField(&quot;user_range&quot;,LongType,true),StructField(&quot;md5_range&quot;,LongType,true),StructField(&quot;event_day&quot;,StringType,true)));

    val txtrow=txtfile.filter(_.split(&quot;\t&quot;).size==18).map(
      x=&gt;{

            var s3_handle=x.split(&quot;\t&quot;)(5);
            var md5_range=Long2long(0);
            if(x.split(&quot;\t&quot;)(5)==&quot;&quot;)
              {
                s3_handle=(new scala.util.Random).nextInt(99).toString(); //取0-99之间的随机数，保证其散列开来
                md5_range=s3_handle.toLong;
              }
            else if(x.split(&quot;\t&quot;)(5).size==32)
            {
                md5_range=((s3_handle.toLowerCase.substring(0, 24).toList.map(&quot;0123456789abcdef&quot;.indexOf(_)).map(BigInt(_)).reduceLeft( _ * 32 + _))%100).toLong
            }
            else
            {
                md5_range = Long2long(-1);
            }
            var user_range=(x.split(&quot;\t&quot;)(0)).toLong/100000000;


            if(user_range&gt;=100)
            {
                user_range=100;
            }

            if(md5_range == Long2long(-1))
              {
                null
              }
            else {
              Row((x.split(&quot;\t&quot;)(0)).toLong, x.split(&quot;\t&quot;)(1).toLong, x.split(&quot;\t&quot;)(2).toLong, x.split(&quot;\t&quot;)(3), x.split(&quot;\t&quot;)(4), s3_handle, (x.split(&quot;\t&quot;)(6)).toLong, x.split(&quot;\t&quot;)(7), (x.split(&quot;\t&quot;)(8)).toLong, (x.split(&quot;\t&quot;)(9)).toLong, (x.split(&quot;\t&quot;)(10)).toLong, (x.split(&quot;\t&quot;)(11)).toLong, (x.split(&quot;\t&quot;)(12)).toLong, (x.split(&quot;\t&quot;)(13)).toLong, (x.split(&quot;\t&quot;)(14)).toLong, x.split(&quot;\t&quot;)(15), (x.split(&quot;\t&quot;)(16)).toLong, (x.split(&quot;\t&quot;)(17)).toLong, user_range, md5_range, event_day);
            }
      }
    ).filter(_ != null);




    val txtdf=sqlContext.createDataFrame(txtrow, schema);

    txtdf.repartition(5).write.mode(org.apache.spark.sql.SaveMode.Append).partitionBy(&quot;user_range&quot;,&quot;md5_range&quot;,&quot;event_day&quot;).parquet(destFile);
  }
}



object transtoparquet{

  def main(args: Array[String]): Unit = {

    if (args.size != 4) {
      println(&quot;usage: com.yundata.transtoparquet.transtoparquet config srcfile destfile event_day&quot;)
      return
    }
    new transtoparquet(args(0),args(1),args(2),args(3)).run()
  }
</code></pre><p>   }</p>
<p>使用了这个程序之后，很快，18分钟，就将5G parquet数据全部写入了。</p>
<p>在上述程序中，我使用的配置是：</p>
<pre><code>spark.app.name = TRANSTOPARQUET-JOB
spark.master = spark://sparkmaster:8650
spark.cores.max=50
spark.executor.instances=50
spark.executor.memory=2g
spark.speculation=true
spark.driver.maxResultSize=2g
spark.ui.port=8221
spark.ui.retainedStages=20
spark.ui.retainedJobs=20
spark.sql.parquet.compression.codec=snappy
</code></pre><p>但是这个程序还有一个问题是，如果repartition不设置的话，最后写入的文件数会非常多，大概是num(user_range)<em>num(md5_range)</em>num(event_day)*num(repartition)，很可能会瞬间打爆namenode的内存。因此repartition要设置得非常小，这又导致了整个程序会非常慢。</p>
<p>3)优化一下repartition的方式</p>
<p>原先使用repartiton(5)的方式的时候，是随机分区，导致所有的task都基本可能有每一个分区的数据，所以导致每个分区下面都有5个文件，但是如果我按照需要的分区来作哈希的话，例如repartition(user_range,md5_range,event_day)来的话，那么每个分区的数据只会存在在一个最后写入的task任务中，也就保证了整个任务产生的分区数最大是num(user_range)<em>num(md5_range)</em>num(event_day)</p>
<p>而不是原来的num(repartition)<em>num(user_range)</em>num(md5_range)*num(event_day)</p>
<p>因此我们可以随意启并发数。</p>
<p>将写入代码修改为：</p>
<p>txtdf.repartition(txtdf(“user_range”), txtdf(“md5_range”), txtdf(“event_day”)).write.mode(org.apache.spark.sql.SaveMode.Append).partitionBy(“user_range”,”md5_range”,”event_day”).parquet(destFile)</p>
<p>这样修改了之后。数据5分钟左右就全部写入了。</p>
<p>  9208         4556         3297227427 /horus/users/chenxiue</p>
<p>数据的大小也比之前小了一些，因为文件更加集中了。</p>
<p>下面我们对比一下gzip压缩和snappy压缩的效果：</p>
<p>1）压缩比率</p>
<p>原始文件大小：</p>
<p>1          517         9359700501 hdfs://namenode:8700/pika_data/file_meta_data_20170101_part4/2017011421/1483873213877</p>
<p>snappy压缩产出的文件格式类似：part-r-00000-b3ff5d89-8885-42f1-bb3d-e8dc6fb692a0.snappy.parquet</p>
<p>压缩后的文件大小：<br>9206        22157         5501724946 /horus/users/chenxiue</p>
<p>大概花了18分钟左右。</p>
<p>gzip压缩是类似：part-r-00003-be2d54a8-088e-4970-be30-363747930a6e.gz.parquet这样的文件</p>
<p>大概也花了18分钟左右<br>9206        22157         3958666080 /horus/users/chenxiue1</p>
<p>可以看出gzip的压缩比更加大一些。</p>
<p>2）在随机分区得情况下，我们尝试加大最后写入的并发度，看看会不会有加速？</p>
<p>txtdf.repartition(10).write.mode(org.apache.spark.sql.SaveMode.Append).partitionBy(“user_range”,”md5_range”,”event_day”).parquet(destFile);</p>
<p>结果写入花了19分钟？？？为啥？？<br>9208        44158         4259882305 /horus/users/chenxiue<br>发现写入的大小比原先的稍微大一些。</p>
<p>怀疑是数据太小，主要时间花在建立文件上。repartiton越大的话，文件数就越多。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/13/spark中parquet文件写入优化/" data-id="cje9m0tlz0004je3cg9xk6yyx" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-在kerberos-HA环境下的ranger编译安装" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/13/在kerberos-HA环境下的ranger编译安装/" class="article-date">
  <time datetime="2017-04-13T06:44:39.000Z" itemprop="datePublished">2017-04-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/部署文档/">部署文档</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/13/在kerberos-HA环境下的ranger编译安装/">原创-在kerberos+HA环境下的ranger编译安装</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      <span id="busuanzi_container_page_pv">
        <br /><br /> 阅读次数<span id="busuanzi_value_page_pv"></span>
        </span>
      
        <p>标签（空格分隔）： 部署文档，转载请注明出处</p>
<hr>
<p>1.代码下载&amp;编译</p>
<p>git clone <a href="https://github.com/apache/incubator-ranger.git" target="_blank" rel="external">https://github.com/apache/incubator-ranger.git</a><br>cd incubator-ranger<br>git checkout ranger-0.5</p>
<p>mvn clean compile package assembly:assembly install</p>
<p>下载的过程中遇到python hash库的问题，重新安装下python即可<br>另外经常因为下载库过程超时，重试几次就好了</p>
<p>编译好的目录在target目录下。</p>
<p>2.控制台ranger-admin的安装</p>
<p>1）安装mysql数据库</p>
<p>配置my.cnf:</p>
<p>basedir =/home/bae/dataplatform/jumbo<br>datadir =/home/bae/dataplatform/jumbo/var<br>port = 3309<br>socket = /home/bae/dataplatform/jumbo/var/mysql.sock</p>
<p>启动mysql:<br>./bin/mysql_install_db<br>./share/mysql/mysql.server start</p>
<p>2)生成各个模块的keytab</p>
<p>addprinc -randkey rangeradmin/hostA@EXAMPLE.COM</p>
<p>xst -k /home/bae/dataplatform/kerberos/keytab/rangeradmin.keytab rangeradmin/hostA@EXAMPLE.COM</p>
<p>addprinc -randkey rangerlookup/hostA@EXAMPLE.COM</p>
<p>xst -k /home/bae/dataplatform/kerberos/keytab/rangerlookup.keytab rangerlookup/hostA@EXAMPLE.COM</p>
<p>addprinc -randkey rangerusersync/hostA@EXAMPLE.COM</p>
<p>xst -k /home/bae/dataplatform/kerberos/keytab/rangerusersync.keytab rangerusersync/hostA@EXAMPLE.COM</p>
<p>addprinc -randkey rangertagsync/hostA@EXAMPLE.COM</p>
<p>xst -k /home/bae/dataplatform/kerberos/keytab/rangertagsync.keytab rangertagsync/hostA@EXAMPLE.COM</p>
<p>3)配置ranger-admin</p>
<p>将ranger-0.5.4-SNAPSHOT-admin.tar.gz解压到安装目录下，修改install.properties,需要修改的选项如下：</p>
<pre><code>SQL_CONNECTOR_JAR=/home/bae/dataplatform/jumbo/lib/mysql/mysql-connector-java-5.1.41-bin.jar
db_root_user=root
db_root_password=
db_host=hostMysql:3309

db_name=ranger
db_user=rangeradmin
db_password=123456


audit_store=db
audit_db_name=ranger_audit
audit_db_user=rangerlogger
audit_db_password=123456


policymgr_external_url=http://localhost:8070
policymgr_http_enabled=true

unix_user=work
unix_group=work


spnego_principal=HTTP/hostA@EXAMPLE.COM
spnego_keytab=/home/bae/dataplatform/kerberos/keytab/spnego.service.keytab
token_valid=30
cookie_domain=hostA
cookie_path=/

admin_principal=rangeradmin/hostA@EXAMPLE.COM
admin_keytab=/home/bae/dataplatform/kerberos/keytab/rangeradmin.keytab
lookup_principal=rangerlookup/hostA@EXAMPLE.COM
lookup_keytab=/home/bae/dataplatform/kerberos/keytab/rangerlookup.keytab
</code></pre><p>运行./setup.sh（root运行，否则报groupadd没有权限）</p>
<p>遇到问题：</p>
<p>a）报错：</p>
<p>SQLException : SQL state: 28000 java.sql.SQLException: Access denied for user ‘rangeradmin’@’hostA’ (using password: YES) ErrorCode: 1045</p>
<p>查看user表，该用户已经创建，但是机器没有被授权</p>
<pre><code>create user &apos;rangeradmin&apos;@&apos;hostA&apos; identified by &apos;123456&apos;;
flush privileges;
</code></pre><p>b）修改了policymgr_external_url=<a href="http://localhost:8070端口，发现8070端口没有启动成功" target="_blank" rel="external">http://localhost:8070端口，发现8070端口没有启动成功</a></p>
<p>在conf/ranger-admin-site.xml中发现</p>
<pre><code>&lt;property&gt;
&lt;name&gt;ranger.service.http.port&lt;/name&gt;
&lt;value&gt;6080&lt;/value&gt;
&lt;/property&gt;
</code></pre><p>  这里需要修改.</p>
<p>c）range-admin stop/start重新启动后就可以看到了。注意tomcat的日志在ews/logs/catalina.out当中</p>
<p>验证是否成功：打开<a href="http://localhost:8070，使用admin/admin登录" target="_blank" rel="external">http://localhost:8070，使用admin/admin登录</a></p>
<p>3.安装usersync进程</p>
<p>这个安装的目的是同步unix，或者ldap中的用户到ranger中。</p>
<p>拷贝编译好的ranger-0.5.4-SNAPSHOT-usersync.tar.gz到适当目录并解压</p>
<p>修改install.properties:（同步本机的unix用户）</p>
<pre><code>POLICY_MGR_URL = http://localhost:8070
# sync source,  only unix and ldap are supported at present
# defaults to unix
SYNC_SOURCE = unix
#User and group for the usersync process
unix_user=work
unix_group=work
logdir=/home/bae/dataplatform/ranger-0.5.4-SNAPSHOT-usersync/logs/ranger/usersync
usersync_principal=rangerusersync/hostA@EXAMPLE.COM
usersync_keytab=/home/bae/dataplatform/kerberos/keytab/rangerusersync.keytab
hadoop_conf=/home/bae/dataplatform/hadoop/conf/
</code></pre><p>使用root账号运行./setup.sh<br>启动usersync:/ranger-usersync-services.sh start<br>验证是否成功：在ranger控制台的settings－&gt;Users/Groups信息看本机的账号是否已经被同步上来。</p>
<p>4.hdfs-plugin安装(只需要在对应集群的主备namenode上安装)</p>
<p>为了让ranger能够控制hdfs，需要安装plugin</p>
<p>拷贝ranger-0.5.4-SNAPSHOT-hbase-plugin.tar.gz到对应目录并解压。修改install.properties</p>
<pre><code>POLICY_MGR_URL=http://hostA:8070
SQL_CONNECTOR_JAR=/home/bae/dataplatform/jumbo/lib/mysql/mysql-connector-java-5.1.41-bin.jar
REPOSITORY_NAME=hadoopdev(与后续页面上配置的一致)
XAAUDIT.DB.IS_ENABLED=true
XAAUDIT.DB.FLAVOUR=MYSQL
XAAUDIT.DB.HOSTNAME=hostA:3309
XAAUDIT.DB.DATABASE_NAME=ranger_audit
XAAUDIT.DB.USER_NAME=rangeradmin
XAAUDIT.DB.PASSWORD=123456

CUSTOM_USER=work
CUSTOM_GROUP=work
</code></pre><p>创建到hadoop_conf的软链：<br>ln -s /home/bae/dataplatform/hadoop-2.7.2  /home/bae/dataplatform/hadoop<br>ln -s /home/bae/dataplatform/hadoop-2.7.2/etc/hadoop/ /home/bae/dataplatform/hadoop-2.7.2/conf</p>
<p>确认$HADOOP_HOME下面有lib目录，如果没有需要编译native lib，编译方法：</p>
<p><a href="http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/NativeLibraries.html" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/NativeLibraries.html</a></p>
<p>使用root账号启动hdfs-plugin:</p>
<p>./enable-hdfs-plugin.sh(root身份运行)</p>
<p>重启namenode进程：</p>
<p>将$HADOOP_HOME/lib下面新增的ranger jar添加到hadoop_classpath变量中，</p>
<p>在conf/hadoop-env.sh中添加：</p>
<pre><code>for f in $HADOOP_HOME/lib/*.jar; do
  if [ &quot;$HADOOP_CLASSPATH&quot; ]; then
    export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$f
  else
    export HADOOP_CLASSPATH=$f
  fi
done
</code></pre><p>重新启动namenode（如果报jdbc方法找不到问题，就将mysql-connector-java-5.1.41-bin.jar拷贝到$HADOOP_HOME/lib目录下后重启）</p>
<p>5.在ranger的控制台中增加plugin配置：</p>
<p>首先创建一个kerberos的用户名密码</p>
<p>addprinc -pw password rangeradmin@example.com</p>
<p>修改core-site.xml增加映射：</p>
<pre><code>RULE:[2:$1@$0](rangeradmin@EXAMPLE.COM)s/.*/work/
RULE:[1:$1@$0](rangeradmin@EXAMPLE.COM)s/.*/work/
</code></pre><p>重启namenode使其生效，重启ranger-admin</p>
<p>在Service Manager-&gt;hdfs中增加hadoopdev（名称与hdfs plugin中设置的一致)repo</p>
<pre><code>username:rangeradmin@example.com
password:password
namenode url:hdfs://hostB:8900
Authorization Enabled:yes
Authentication Type:kerberos
hadoop.security.auth_to_local:RULE:[1:$1@$0](rangeradmin@EXAMPLE.COM)s/.*/work/
dfs.datanode.kerberos.principal:dn/_HOST@EXAMPLE.COM
dfs.namenode.kerberos.principal:nn/_HOST@EXAMPLE.COM
dfs.secondary.namenode.kerberos.principal:nn/_HOST@EXAMPLE.COM
RPC Protection Type:Authentication

dfs.nameservices = smallfile
dfs.ha.namenodes.smallfile= nn1,nn2
dfs.namenode.rpc-address.nn1 = hostB:8900
dfs.namenode.rpc-address.nn2 = hostC:8900
dfs.client.failover.proxy.provider.smallfile = org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
</code></pre><p>其中username/password需要是kerberos中有效的用户名密码。</p>
<p>点击test connection如果成功，那么save.</p>
<p>验证plugin是否添加成功：在audit-&gt;plugin目录下是否出现对应的plugin信息。</p>
<p>6.ranger对hdfs授权测试</p>
<p>注意首先要在hdfs上将权限收回，比如把一个目录权限设置成000，这样就完全由ranger policy控制。否则生效的都是hdfs上的大权限。</p>
<p>可以通过audit-&gt;access中得Access Enforcer看生效得是ranger-acl还是hadoop-acl</p>
<p>参考文档：</p>
<p><a href="https://cwiki.apache.org/confluence/display/RANGER/Apache+Ranger+0.5.0+Installation#ApacheRanger0.5.0Installation-InstallandconfigureSolrorSolrCloud" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/RANGER/Apache+Ranger+0.5.0+Installation#ApacheRanger0.5.0Installation-InstallandconfigureSolrorSolrCloud</a></p>
<p>在kerberos环境下安装ranger:</p>
<p><a href="https://cwiki.apache.org/confluence/display/RANGER/Ranger+installation+in+Kerberized++Environment" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/RANGER/Ranger+installation+in+Kerberized++Environment</a></p>
<p><a href="https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.2/bk_Security_Guide/content/hdfs_plugin_kerberos.html" target="_blank" rel="external">https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.2/bk_Security_Guide/content/hdfs_plugin_kerberos.html</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/13/在kerberos-HA环境下的ranger编译安装/" data-id="cje9m0tm20005je3co62eeh46" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>



  

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/性能优化/">性能优化</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/部署文档/">部署文档</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/03/02/spark远程调试/">spark远程调试</a>
          </li>
        
          <li>
            <a href="/2018/01/11/原创－flume写hdfs性能优化/">原创-flume写hdfs性能优化</a>
          </li>
        
          <li>
            <a href="/2017/04/19/spark-on-yarn情况下historyserver的配置/">原创-spark-on-yarn情况下historyserver的配置</a>
          </li>
        
          <li>
            <a href="/2017/04/13/spark中parquet文件写入优化/">原创-spark中parquet文件写入优化</a>
          </li>
        
          <li>
            <a href="/2017/04/13/在kerberos-HA环境下的ranger编译安装/">原创-在kerberos+HA环境下的ranger编译安装</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 XiueChen<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
      <span id="busuanzi_container_site_uv"><br />本站访客数<span id="busuanzi_value_site_uv"></span>人次
      <span id="busuanzi_container_site_pv"><br />本站访问次数<span id="busuanzi_value_site_pv"></span>
      </span>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>