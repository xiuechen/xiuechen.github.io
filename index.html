<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Nora-一个程序媛的窝</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Nora-一个程序媛的窝">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Nora-一个程序媛的窝">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Nora-一个程序媛的窝">
  
    <link rel="alternate" href="/atom.xml" title="Nora-一个程序媛的窝" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Nora-一个程序媛的窝</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="Flux RSS"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Rechercher"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-spark-graphx计算pagerank源代码分析" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/05/17/spark-graphx计算pagerank源代码分析/" class="article-date">
  <time datetime="2020-05-17T02:11:05.000Z" itemprop="datePublished">2020-05-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/17/spark-graphx计算pagerank源代码分析/">spark graphx计算pagerank源代码分析</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      <span id="busuanzi_container_page_pv">
        <br /><br /> 阅读次数<span id="busuanzi_value_page_pv"></span>
        </span>
      
        <p>##入口函数</p>
<pre><code>def pageRank(tol: Double, resetProb: Double = 0.15): Graph[Double, Double] = {                                                      
    PageRank.runUntilConvergence(graph, tol, resetProb)
}
</code></pre><p>其中resetProb的作用可以参考pagerank原理的说明：</p>
<p>##实现函数</p>
<pre><code>196   def runUntilConvergenceWithOptions[VD: ClassTag, ED: ClassTag](
197       graph: Graph[VD, ED], tol: Double, resetProb: Double = 0.15,
198       srcId: Option[VertexId] = None): Graph[Double, Double] =
199   {
//这个用来表示，用户是否自定义了游走的起点
200     val personalized = srcId.isDefined
//默认是随机选择起点
201     val src: VertexId = srcId.getOrElse(-1L)
202 
// Initialize the pagerankGraph with each edge attribute
// having weight 1/outDegree and each vertex with attribute 1.0.
//初始化pagerankGraph,边的属性设置为源节点的出度的倒数。对于启动节点，顶点设置为（resetProb, Double.NegativeInfinity）,其他的顶点设置为（0，0）,图的顶点格式为VertextRDD[(Double,Double)]
205     val pagerankGraph: Graph[(Double, Double), Double] = graph
206       // Associate the degree with each vertex
207       .outerJoinVertices(graph.outDegrees) {
208         (vid, vdata, deg) =&gt; deg.getOrElse(0)
209       }
210       // Set the weight on the edges based on the degree
211       .mapTriplets( e =&gt; 1.0 / e.srcAttr )
212       // Set the vertex attributes to (initalPR, delta = 0)
213       .mapVertices { (id, attr) =&gt;
214         if (id == src) (resetProb, Double.NegativeInfinity) else (0.0, 0.0)
215       }
216       .cache()
217 
// Define the three functions needed to implement PageRank in the GraphX
// version of Pregel

//定义pregel中的vprog，这里的msgSum就是下面map/reduce产出的messages图，每个节点的格式是VertexRDD[double],更新图
220     def vertexProgram(id: VertexId, attr: (Double, Double), msgSum: Double): (Double, Double) = {
221       val (oldPR, lastDelta) = attr
222       val newPR = oldPR + (1.0 - resetProb) * msgSum
223       (newPR, newPR - oldPR)
224     }
//定义指定了启动节点的vprog
226     def personalizedVertexProgram(id: VertexId, attr: (Double, Double),
227       msgSum: Double): (Double, Double) = {
228       val (oldPR, lastDelta) = attr
229       var teleport = oldPR
230       val delta = if (src==id) 1.0 else 0.0
231       teleport = oldPR*delta
232 
233       val newPR = teleport + (1.0 - resetProb) * msgSum
234       val newDelta = if (lastDelta == Double.NegativeInfinity) newPR else newPR - oldPR
235       (newPR, newDelta)
236     }

//定义pregel中的sendmessage，如果源节点的权重&gt;tol的话，则按照边的权重(edge.attr)加权传递过来
238     def sendMessage(edge: EdgeTriplet[(Double, Double), Double]) = {
239       if (edge.srcAttr._2 &gt; tol) {
240         Iterator((edge.dstId, edge.srcAttr._2 * edge.attr))
241       } else {
242         Iterator.empty
243       }
244     }
//定义pregel的reduce程序，将各个节点传递过来的权重相加即可
246     def messageCombiner(a: Double, b: Double): Double = a + b
247 
248     // The initial message received by all vertices in PageRank
249     val initialMessage = if (personalized) 0.0 else resetProb / (1.0 - resetProb)
250 
251     // Execute a dynamic version of Pregel.
252     val vp = if (personalized) {
253       (id: VertexId, attr: (Double, Double), msgSum: Double) =&gt;
254         personalizedVertexProgram(id, attr, msgSum)
255     } else {
256       (id: VertexId, attr: (Double, Double), msgSum: Double) =&gt;
257         vertexProgram(id, attr, msgSum)
258     }
259 
260     Pregel(pagerankGraph, initialMessage, activeDirection = EdgeDirection.Out)(
261       vp, sendMessage, messageCombiner)
262       .mapVertices((vid, attr) =&gt; attr._1)
263   } // end of deltaPageRank
264 
265 } 
</code></pre><p>###pregel的具体实现(一次mapReduceTriplets完成一次全局权重调整，vprog可以理解为两次权重调整的更新)</p>
<pre><code>class GraphOps[VD, ED] {
  def pregel[A]
      (initialMsg: A,
       maxIter: Int = Int.MaxValue,
       activeDir: EdgeDirection = EdgeDirection.Out)
      (vprog: (VertexId, VD, A) =&gt; VD,
       sendMsg: EdgeTriplet[VD, ED] =&gt; Iterator[(VertexId, A)],
       mergeMsg: (A, A) =&gt; A)
    : Graph[VD, ED] = {
    // Receive the initial message at each vertex
    var g = mapVertices( (vid, vdata) =&gt; vprog(vid, vdata, initialMsg) ).cache()  

    // compute the messages
    var messages = GraphXUtils.mapReduceTriplets(g, sendMsg, mergeMsg)  
    var activeMessages = messages.count()
    // Loop until no messages remain or maxIterations is achieved
    var i = 0
    while (activeMessages &gt; 0 &amp;&amp; i &lt; maxIterations) {
      // Receive the messages and update the vertices.
      g = g.joinVertices(messages)(vprog).cache()
      val oldMessages = messages
      // Send new messages, skipping edges where neither side received a message. We must cache
      // messages so it can be materialized on the next line, allowing us to uncache the previous
      // iteration.
      messages = GraphXUtils.mapReduceTriplets(
        g, sendMsg, mergeMsg, Some((oldMessages, activeDirection))).cache()
      activeMessages = messages.count()
      i += 1
    }
    g
  }
}
</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/05/17/spark-graphx计算pagerank源代码分析/" data-id="ckaafcx5k00016y3c5km6hxfi" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-druid安装配置-数据导入" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/29/druid安装配置-数据导入/" class="article-date">
  <time datetime="2018-03-29T11:13:50.000Z" itemprop="datePublished">2018-03-29</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/部署文档/">部署文档</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/29/druid安装配置-数据导入/">druid安装配置&amp;数据导入</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      <span id="busuanzi_container_page_pv">
        <br /><br /> 阅读次数<span id="busuanzi_value_page_pv"></span>
        </span>
      
        <p>一.druid的设计与架构</p>
<p>设计：<a href="http://druid.io/docs/0.9.1.1/design/design.html" target="_blank" rel="external">http://druid.io/docs/0.9.1.1/design/design.html</a><br>白皮书：<a href="http://static.druid.io/docs/druid.pdf" target="_blank" rel="external">http://static.druid.io/docs/druid.pdf</a></p>
<p>(<a href="https://docs.imply.io/on-premise/deploy/cluster" target="_blank" rel="external">https://docs.imply.io/on-premise/deploy/cluster</a>)</p>
<p>五种节点类型：</p>
<p>Historical: 离线节点，加载离线存储的segments。它和coordinator通过zk进行联系。当接收到新的segments加载请求的时候，先查本地，没命中则根据metadata信息从deep storage中加载，加载完成后申报到zk，这时候该segment就可以被查询</p>
<p>Broker:接受查询，根据zk的信息查询segment的位置，将查询路由到正确的位置。最后merge结果返回</p>
<p>Coordinator:协调segment的存储，决定哪些segments应该进historical nodes</p>
<p>Indexing Service:包含三大组件。peon,middle manager,overlord<br>任务从overlord的http提交，由middle manager分配给Peons处理。</p>
<p>Realtime:实时节点</p>
<p>其他名词：</p>
<p>Tranquility： helps you send real-time event streams to Druid and handles partitioning, replication, service discovery, and schema rollover, seamlessly and without downtime</p>
<p>Tranquility server:一个http server，有它就可以不需要写java程序来导数据到druid，而通过http接口就可以</p>
<p>依赖的外围模块：</p>
<p>Deep Storage：<br>Metadata Storage：<br>ZooKeeper：</p>
<hr>
<p>二.安装部署：</p>
<p>1.参考</p>
<p><a href="http://druid.io/docs/0.9.1.1/tutorials/cluster.html（./bin/xxx.sh" target="_blank" rel="external">http://druid.io/docs/0.9.1.1/tutorials/cluster.html（./bin/xxx.sh</a> start启动各个对应的服务）<br><a href="http://www.open-open.com/lib/view/open1447852962978.html" target="_blank" rel="external">http://www.open-open.com/lib/view/open1447852962978.html</a></p>
<p>2.下载druid以及mysql extenstion&amp;tranquility</p>
<p><a href="http://druid.io/downloads.html" target="_blank" rel="external">http://druid.io/downloads.html</a></p>
<p>3.拷贝mysql-metadata-storage-0.12.0.tar.gz到extensions路径下并解压</p>
<p>create database <code>druid</code> DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci;</p>
<p>4.配置conf/druid/_common/common.runtime.properties</p>
<p>5.拷贝hadoop相关配置到conf/druid/_common下（core-site.xml, hdfs-site.xml, yarn-site.xml, mapred-site.xm）</p>
<p>6.启动各个组件：(也可以使用sh bin/XXX.sh start来启动)</p>
<p>启动一个实例就够了<br>java <code>cat conf/druid/coordinator/jvm.config | xargs</code> -cp conf/druid/_common:conf/druid/coordinator:lib/<em> io.druid.cli.Main server coordinator<br>java <code>cat conf/druid/overlord/jvm.config | xargs</code> -cp conf/druid/_common:conf/druid/overlord:lib/</em> io.druid.cli.Main server overlord</p>
<p>可以按需启动多个实例<br>java <code>cat conf/druid/historical/jvm.config | xargs</code> -cp conf/druid/_common:conf/druid/historical:lib/<em> io.druid.cli.Main server historical<br>java <code>cat conf/druid/middleManager/jvm.config | xargs</code> -cp conf/druid/_common:conf/druid/middleManager:lib/</em> io.druid.cli.Main server middleManager<br>java <code>cat conf/druid/broker/jvm.config | xargs</code> -cp conf/druid/_common:conf/druid/broker:lib/* io.druid.cli.Main server broker</p>
<p>遇到问题：</p>
<p>historical node内存启不了</p>
<p>启动得时候报错：</p>
<pre><code>12) Not enough direct memory.  Please adjust -XX:MaxDirectMemorySize, druid.processing.buffer.sizeBytes, druid.processing.numThreads, or druid.processing.numMergeBuffers: maxDirectMemory[2,147,483,648], memoryNeeded[5,368,709,120] = druid.processing.buffer.sizeBytes[536,870,912] * (druid.processing.numMergeBuffers[2] + druid.processing.numThreads[7] + 1)
</code></pre><p>根据提示将maxDirectMemory从2G修改为5G就可以了。。</p>
<p><a href="https://groups.google.com/forum/#!topic/druid-user/j0sFcUIiQiE" target="_blank" rel="external">https://groups.google.com/forum/#!topic/druid-user/j0sFcUIiQiE</a></p>
<p>三.druid的数据导入简介：(分files和stream)</p>
<p>files方式不依赖tranquility，参考<a href="http://druid.io/docs/latest/tutorials/tutorial-batch.html" target="_blank" rel="external">http://druid.io/docs/latest/tutorials/tutorial-batch.html</a></p>
<p>stream数据导入有两种方式：</p>
<p>1.Tranquility (a Druid-aware client) and the indexing service(push方式)</p>
<p>2.Realtime nodes(不推荐，有若干缺点：<a href="http://druid.io/docs/0.9.1.1/ingestion/stream-pull.html#limitations" target="_blank" rel="external">http://druid.io/docs/0.9.1.1/ingestion/stream-pull.html#limitations</a>) (pull)</p>
<p>stream push &amp; stream pull &amp;batch ingestion</p>
<p>stream push有两种方式：</p>
<p>1）通过Tranquility server通过http接口推进去<br><a href="http://druid.io/docs/0.9.1.1/tutorials/tutorial-streams.html" target="_blank" rel="external">http://druid.io/docs/0.9.1.1/tutorials/tutorial-streams.html</a></p>
<p>2）通过Tranquility Kafka推进去</p>
<p>stream pull:</p>
<p>通过realtime node的方式，参考：<br><a href="http://druid.io/docs/latest/ingestion/stream-pull.html" target="_blank" rel="external">http://druid.io/docs/latest/ingestion/stream-pull.html</a></p>
<p>四.druid的数据导入</p>
<p>1.files导入，参考：<a href="http://druid.io/docs/latest/tutorials/tutorial-batch.html" target="_blank" rel="external">http://druid.io/docs/latest/tutorials/tutorial-batch.html</a></p>
<p>curl -X ‘POST’ -H ‘Content-Type:application/json’ -d@./wikiticker-index.json host-170.bjyz:8090/druid/indexer/v1/task</p>
<p>遇到问题：</p>
<p>1)peon启动不起来，报错：</p>
<pre><code>3) Not enough direct memory.  Please adjust -XX:MaxDirectMemorySize, druid.processing.buffer.sizeBytes, druid.processing.numThreads, or druid.processing.numMergeBuffers: maxDirectMemory[1,908,932,608], memoryNeeded[2,684,354,560] = druid.processing.buffer.sizeBytes[536,870,912] * (druid.processing.numMergeBuffers[2] + druid.processing.numThreads[2] + 1)
</code></pre><p>修改：druid.indexer.runner.javaOpts=-server -Xmx2g -XX:MaxDirectMemorySize=2560m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -Djav<br>a.util.logging.manager=org.apache.logging.log4j.jul.LogManager</p>
<p>可以查看状态：</p>
<p><a href="http://host-170.bjyz.baidu.com:8090/console.html" target="_blank" rel="external">http://host-170.bjyz.baidu.com:8090/console.html</a> （overlord得端口,可以查看导入服务得运行状态）</p>
<p>可以看到middlemanager已经启动了一个peon来执行任务：</p>
<p>Main internal peon var/druid/task/index_hadoop_wikiticker_2018-03-27T10:07:12.646Z/task.json var/druid/task/index_hadoop_wikiticker_2018-03-27T10:07:12.646Z/d93e84a0-d9f2-40e4-8b1a-0e24072a00f3/status.json</p>
<p>2)默认的mr jobs是提交到yarn集群的default队列，为了修改该peon所提交得mapreduce job得queue name。设置一下common中得mapred-site.xml得mapreduce.job.queuename参数</p>
<p>3)关于yarn集群java版本和druid依赖java版本不一致得问题：</p>
<p>conf.set(“mapred.child.env”, “JAVA_HOME=/home/iteblog/java/jdk1.8.0_25”);<br>conf.set(“yarn.app.mapreduce.am.env”, “JAVA_HOME=/home/iteblog/java/jdk1.8.0_25”);</p>
<p>可以参考：<a href="https://www.iteblog.com/archives/1883.html" target="_blank" rel="external">https://www.iteblog.com/archives/1883.html</a></p>
<p> 4)遇到JNDI lookup class is not available because this JRE does not support JNDI的问题(这个warning无视就好了。。。哈哈)</p>
<p> 参考<a href="http://druid.io/docs/latest/operations/other-hadoop.html的Tip" target="_blank" rel="external">http://druid.io/docs/latest/operations/other-hadoop.html的Tip</a> 2解决</p>
<p> 5)使用的wikiticker-index.json文件</p>
<pre><code>{
 &quot;type&quot; : &quot;index_hadoop&quot;,
 &quot;spec&quot; : {
   &quot;ioConfig&quot; : {
     &quot;type&quot; : &quot;hadoop&quot;,
     &quot;inputSpec&quot; : {
       &quot;type&quot; : &quot;static&quot;,
       &quot;paths&quot; : &quot;/smallfile/druid/quickstart/wikiticker-2015-09-12-sampled.json.gz&quot;
     }
   },
   &quot;dataSchema&quot; : {
     &quot;dataSource&quot; : &quot;wikiticker&quot;,
     &quot;granularitySpec&quot; : {
       &quot;type&quot; : &quot;uniform&quot;,
       &quot;segmentGranularity&quot; : &quot;day&quot;,
       &quot;queryGranularity&quot; : &quot;none&quot;,
       &quot;intervals&quot; : [&quot;2015-09-12/2015-09-13&quot;]
     },
     &quot;parser&quot; : {
       &quot;type&quot; : &quot;hadoopyString&quot;,
               &quot;parseSpec&quot; : {
         &quot;format&quot; : &quot;json&quot;,
         &quot;dimensionsSpec&quot; : {
           &quot;dimensions&quot; : [
             &quot;channel&quot;,
             &quot;cityName&quot;,
             &quot;comment&quot;,
             &quot;countryIsoCode&quot;,
             &quot;countryName&quot;,
             &quot;isAnonymous&quot;,
             &quot;isMinor&quot;,
             &quot;isNew&quot;,
             &quot;isRobot&quot;,
             &quot;isUnpatrolled&quot;,
             &quot;metroCode&quot;,
             &quot;namespace&quot;,
             &quot;page&quot;,
             &quot;regionIsoCode&quot;,
                           &quot;regionName&quot;,
             &quot;user&quot;
           ]
         },
         &quot;timestampSpec&quot; : {
           &quot;format&quot; : &quot;auto&quot;,
           &quot;column&quot; : &quot;time&quot;
         }
       }
     },
     &quot;metricsSpec&quot; : [
       {
         &quot;name&quot; : &quot;count&quot;,
         &quot;type&quot; : &quot;count&quot;
       },
       {
         &quot;name&quot; : &quot;added&quot;,
         &quot;type&quot; : &quot;longSum&quot;,
         &quot;fieldName&quot; : &quot;added&quot;
       },        {
         &quot;name&quot; : &quot;deleted&quot;,
         &quot;type&quot; : &quot;longSum&quot;,
         &quot;fieldName&quot; : &quot;deleted&quot;
       },
       {
         &quot;name&quot; : &quot;delta&quot;,
         &quot;type&quot; : &quot;longSum&quot;,
         &quot;fieldName&quot; : &quot;delta&quot;
       },
       {
         &quot;name&quot; : &quot;user_unique&quot;,
         &quot;type&quot; : &quot;hyperUnique&quot;,
         &quot;fieldName&quot; : &quot;user&quot;
       }
     ]
   },
   &quot;tuningConfig&quot; : {
     &quot;type&quot; : &quot;hadoop&quot;,
     &quot;partitionsSpec&quot; : {        &quot;type&quot; : &quot;hashed&quot;,
       &quot;targetPartitionSize&quot; : 5000000
     },
     &quot;jobProperties&quot; : {
       &quot;mapreduce.map.java.opts&quot;:&quot;-Duser.timezone=UTC -Dfile.encoding=UTF-8&quot;,
       &quot;mapreduce.reduce.java.opts&quot;:&quot;-Duser.timezone=UTC -Dfile.encoding=UTF-8&quot;,
       &quot;mapred.child.env&quot;:&quot;JAVA_HOME=/home/work/.jumbo/opt/sun-java8&quot;,
       &quot;yarn.app.mapreduce.am.env&quot;:&quot;JAVA_HOME=/home/work/.jumbo/opt/sun-java8&quot;,
       &quot;mapreduce.job.queuename&quot;:&quot;bigJob&quot;,
       &quot;mapreduce.job.classloader&quot;: &quot;true&quot;
       }
   }
 }
</code></pre><p>6）确认任务真的导入成功</p>
<p>通过broker接口查询导入的user有多少<br>curl -X POST ‘host-175.bjyz:8082/druid/v2/?pretty’ -H ‘Content-Type:application/json’ -d @query/useruniq.json</p>
<p>useruniq.json内容：</p>
<pre><code>{
  &quot;queryType&quot;: &quot;timeseries&quot;,
  &quot;dataSource&quot;: &quot;wikiticker&quot;,
  &quot;granularity&quot;: &quot;day&quot;,
  &quot;aggregations&quot;: [
    { &quot;type&quot;: &quot;hyperUnique&quot;, &quot;name&quot;: &quot;user_unique&quot;, &quot;fieldName&quot;: &quot;user_unique&quot; }
  ],
  &quot;intervals&quot;: [ &quot;2015-09-12T00:00:00.000/2015-09-13T00:00:00.000&quot; ],
  &quot;context&quot; : {
    &quot;skipEmptyBuckets&quot;: &quot;true&quot;
  }
}
</code></pre><p>通过broker接口查询导入的meta信息：<br>curl -X POST ‘host-175.bjyz:8082/druid/v2/?pretty’ -H ‘Content-Type:application/json’ -d @query/metadata.json</p>
<pre><code>{
  &quot;queryType&quot;:&quot;segmentMetadata&quot;,
  &quot;dataSource&quot;:&quot;wikiticker&quot;,
  &quot;intervals&quot;:[&quot;2015-09-12/2015-09-13&quot;]
}
</code></pre><p>或者使用自带的查询串<br>curl -X POST ‘host-175.bjyz:8082/druid/v2/?pretty’ -H ‘Content-Type:application/json’ -d @wikiticker-top-pages.json查看被编辑最大的pages</p>
<p>2.stream push方式（参考：<a href="http://druid.io/docs/latest/ingestion/stream-push.html）" target="_blank" rel="external">http://druid.io/docs/latest/ingestion/stream-push.html）</a><br>stream push主要是借助了tranquility,关于tranquility的介绍：<a href="https://github.com/druid-io/tranquility/blob/master/docs/overview.md" target="_blank" rel="external">https://github.com/druid-io/tranquility/blob/master/docs/overview.md</a></p>
<p>tranquility导入数据主要有几种方式：</p>
<p>a)tranquility server (http接口)<br>b)tranquility kafka（用户将数据推入kafka，tranquility写入druid）<br>c)自己写一个依赖tranquility library的JVM app<br>参考：<a href="https://github.com/druid-io/tranquility/blob/master/docs/core.md" target="_blank" rel="external">https://github.com/druid-io/tranquility/blob/master/docs/core.md</a></p>
<p>d)利用tranquility里面实现的各种流连接器，比如spark如何写入druid:<br><a href="https://github.com/druid-io/tranquility/blob/master/docs/spark.md" target="_blank" rel="external">https://github.com/druid-io/tranquility/blob/master/docs/spark.md</a></p>
<p>其中a)b)方案依赖一定第三方服务。c)d)只依赖tranquility的library</p>
<p>针对spark中的计算结果如何写入druid，会另外开一篇文章专门讨论</p>
<p>3.stream pull方式</p>
<p>这种方式需要用到realtime node。貌似不推荐，这里不多研究</p>
<p>四.druid&amp;caravel</p>
<p>当在druid存储了数据后，我们使用caravel页面进行展示</p>
<p>1）add druid cluster</p>
<p>配置以下coordinator以及broker的地址即可<br>配置完成保存后refresh一下druid元数据</p>
<p>然后点击进入datasource就可以愉快地olap了。如果没有数据检查下数据的起始时间。(例如例子中导入的2015年数据，需要选择4 years ago)</p>
<p>2）配置报表</p>
<p>在datasource视图中选择分组，度量查询之后，将结果保存成slice，点击报表标签页面可以看到刚才保存的slice，选择仪表盘页面新建仪表盘，报表就选择刚才保存的slice。。。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/29/druid安装配置-数据导入/" data-id="ckaafcx5a00006y3corrmtr4n" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-spark远程调试" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/02/spark远程调试/" class="article-date">
  <time datetime="2018-03-02T07:12:34.000Z" itemprop="datePublished">2018-03-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/源码调试/">源码调试</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/02/spark远程调试/">spark远程调试</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      <span id="busuanzi_container_page_pv">
        <br /><br /> 阅读次数<span id="busuanzi_value_page_pv"></span>
        </span>
      
        <p>##spark远程调试</p>
<p>深入了解spark少不了改变代码进行调试，因此编译调试是必备技能：</p>
<p>###首先说一下编译：</p>
<p>完整编译：</p>
<pre><code>./build/sbt -Pyarn -Phive -Phive-thriftserver -Dhadoop.version=2.7.3 -DskipTests clean package
</code></pre><p>增量编译：</p>
<pre><code>./build/sbt -Pyarn -Phive -Phive-thriftserver -DskipTests  ~package
export SPARK_PREPEND_CLASSES=1
</code></pre><p>如果使用maven默认就是增量编译</p>
<p>###idea远程调试</p>
<p>先决条件：拥有一个在idea能够完整编译通过的spark项目</p>
<p>####1.在idea中run-&gt;edit configurations设置remote-&gt;remotedebug参数</p>
<p>其中debugger mode选择attach，然后设置要链接的host以及port（也就是spark进程启动的机器以及监听端口）</p>
<p>####2.在远程host上启动spark程序，设置jvm参数</p>
<p>–driver-java-options  “ -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8888”</p>
<p>比如一个提交到yarn集群的例子：</p>
<p>/home/work/dataplatform/spark-2.0.2/bin/spark-submit \<br>  –master yarn\<br>  –deploy-mode client\<br>  –class com.baidu.pcsdata.message.value.analysis.MsgFastCategory \<br>  –driver-cores 8 \<br>  –executor-cores 2\<br>  –num-executors 10 \<br>  –driver-memory 5G\<br>  –principal  horus/pcsdata@PCSDATA.COM\<br>  –keytab  /home/work/chenxiue/horus.keytab\<br>  –executor-memory 6G\<br>  –driver-java-options  “ -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8888”\<br>  /home/work/chenxiue/spark/msg-mining/pcs_fsg-assembly-1.0.jar $inputpath $outputpath $modelpath</p>
<p>####3.在idea上面设置好断点后（把鼠标放在需要断的某行上面，然后左击出现红点）运行<br>run-&gt;debug “remote debug”</p>
<p>好了，现在就能够愉快地修改代码并且编译调试了～</p>
<p>###参考：<br><a href="https://segmentfault.com/a/1190000008867470" target="_blank" rel="external">https://segmentfault.com/a/1190000008867470</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/02/spark远程调试/" data-id="ckaafcx7000076y3cou4zqovq" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-原创－flume写hdfs性能优化" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/11/原创－flume写hdfs性能优化/" class="article-date">
  <time datetime="2018-01-11T03:43:57.000Z" itemprop="datePublished">2018-01-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/性能优化/">性能优化</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/11/原创－flume写hdfs性能优化/">原创-flume写hdfs性能优化</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      <span id="busuanzi_container_page_pv">
        <br /><br /> 阅读次数<span id="busuanzi_value_page_pv"></span>
        </span>
      
        <p>标签（空格分隔）： 未分类</p>
<hr>
<p>flume在写hdfs的时候，每接收到一个event就会调用bucketwriter.java的append(event)方法，在append当中每次都会去检查shouldrotate,我们来看一下这块的逻辑：</p>
<pre><code>private boolean shouldRotate() {
    boolean doRotate = false;

    if (writer.isUnderReplicated()) {
      this.isUnderReplicated = true;
      doRotate = true;
    } else {
      this.isUnderReplicated = false;
    }

    if ((rollCount &gt; 0) &amp;&amp; (rollCount &lt;= eventCounter)) {
      LOG.debug(&quot;rolling: rollCount: {}, events: {}&quot;, rollCount, eventCounter);
      doRotate = true;
    }

    if ((rollSize &gt; 0) &amp;&amp; (rollSize &lt;= processSize)) {
      LOG.debug(&quot;rolling: rollSize: {}, bytes: {}&quot;, rollSize, processSize);
      doRotate = true;
    }

    return doRotate;
  }
</code></pre><p>我们可以看到这里每次都会调用writer.isUnderReplicated()，如果副本数低于预期，那么不管有没有达到rollcount或者rollsize就rotate？？（这块可能是说上一个event如果副本数没有达到预期，那么就需要重新开一个新文件去写入，为啥要这样设计？不太清楚。。。）</p>
<pre><code>public boolean isUnderReplicated() {
    try {
      int numBlocks = getNumCurrentReplicas();
      if (numBlocks == -1) {
        return false;
      }
      int desiredBlocks;
      if (configuredMinReplicas != null) {
        desiredBlocks = configuredMinReplicas;
      } else {
        desiredBlocks = getFsDesiredReplication();
      }
      return numBlocks &lt; desiredBlocks;
    } catch (IllegalAccessException e) {
      logger.error(&quot;Unexpected error while checking replication factor&quot;, e);
    } catch (InvocationTargetException e) {
      logger.error(&quot;Unexpected error while checking replication factor&quot;, e);
    } catch (IllegalArgumentException e) {
      logger.error(&quot;Unexpected error while checking replication factor&quot;, e);
    }
    return false;
  }
</code></pre><p>副本获取的代码如下：</p>
<pre><code>/**
   * This method gets the datanode replication count for the current open file.
   *
   * If the pipeline isn&apos;t started yet or is empty, you will get the default
   * replication factor.
   *
   * &lt;p/&gt;If this function returns -1, it means you
   * are not properly running with the HDFS-826 patch.
   * @throws InvocationTargetException
   * @throws IllegalAccessException
   * @throws IllegalArgumentException
   */
  public int getNumCurrentReplicas()
      throws IllegalArgumentException, IllegalAccessException,
          InvocationTargetException {
    if (refGetNumCurrentReplicas != null &amp;&amp; outputStream != null) {
      OutputStream dfsOutputStream = outputStream.getWrappedStream();
      if (dfsOutputStream != null) {
        Object repl = refGetNumCurrentReplicas.invoke(dfsOutputStream, NO_ARGS);
        if (repl instanceof Integer) {
          return ((Integer)repl).intValue();
        }
      }
    }
    return -1;
  }
</code></pre><p>我们可以看到每一次event写入都要去和hdfs交互一次，这个代价非常高。而实际上在我们的工作当中我们并没有用rollcount和rollsize，而是使用了rollinterval，rollinterval的逻辑并不在这里,在open()里面有这么一段逻辑：</p>
<pre><code>if (rollInterval &gt; 0) {
      Callable&lt;Void&gt; action = new Callable&lt;Void&gt;() {
        public Void call() throws Exception {
          LOG.debug(&quot;Rolling file ({}): Roll scheduled after {} sec elapsed.&quot;,
              bucketPath, rollInterval);
          try {
            // Roll the file and remove reference from sfWriters map.
            close(true);
          } catch(Throwable t) {
            LOG.error(&quot;Unexpected error&quot;, t);
          }
          return null;
        }
      };
      timedRollFuture = timedRollerPool.schedule(action, rollInterval,TimeUnit.SECONDS);
</code></pre><p>我们回过头来看一下hdfseventsink类的process方法：</p>
<pre><code>public Status process() throws EventDeliveryException {
    Channel channel = getChannel();
    Transaction transaction = channel.getTransaction();
    List&lt;BucketWriter&gt; writers = Lists.newArrayList();
    transaction.begin();
    try {
      int txnEventCount = 0;
      for (txnEventCount = 0; txnEventCount &lt; batchSize; txnEventCount++) {
        Event event = channel.take();
        if (event == null) {
          break;
        }

        // reconstruct the path name by substituting place holders
        String realPath = BucketPath.escapeString(filePath, event.getHeaders(),
            timeZone, needRounding, roundUnit, roundValue, useLocalTime);
        String realName = BucketPath.escapeString(fileName, event.getHeaders(),
          timeZone, needRounding, roundUnit, roundValue, useLocalTime);

        String lookupPath = realPath + DIRECTORY_DELIMITER + realName;
        BucketWriter bucketWriter;
        HDFSWriter hdfsWriter = null;
        // Callback to remove the reference to the bucket writer from the
        // sfWriters map so that all buffers used by the HDFS file
        // handles are garbage collected.
        WriterCallback closeCallback = new WriterCallback() {
          @Override
          public void run(String bucketPath) {
            LOG.info(&quot;Writer callback called.&quot;);
            synchronized (sfWritersLock) {
              sfWriters.remove(bucketPath);
            }
          }
        };
        synchronized (sfWritersLock) {
          bucketWriter = sfWriters.get(lookupPath);
          // we haven&apos;t seen this file yet, so open it and cache the handle
          if (bucketWriter == null) {
            hdfsWriter = writerFactory.getWriter(fileType);
            bucketWriter = initializeBucketWriter(realPath, realName,
              lookupPath, hdfsWriter, closeCallback);
            sfWriters.put(lookupPath, bucketWriter);
          }
        }

        // track the buckets getting written in this transaction
        if (!writers.contains(bucketWriter)) {
          writers.add(bucketWriter);
        }

        // Write the data to HDFS
        try {
          bucketWriter.append(event);
        } catch (BucketClosedException ex) {
          LOG.info(&quot;Bucket was closed while trying to append, &quot; +
            &quot;reinitializing bucket and writing event.&quot;);
          hdfsWriter = writerFactory.getWriter(fileType);
          bucketWriter = initializeBucketWriter(realPath, realName,
            lookupPath, hdfsWriter, closeCallback);
          synchronized (sfWritersLock) {
            sfWriters.put(lookupPath, bucketWriter);
          }
          bucketWriter.append(event);
        }
      }

      if (txnEventCount == 0) {
        sinkCounter.incrementBatchEmptyCount();
      } else if (txnEventCount == batchSize) {
        sinkCounter.incrementBatchCompleteCount();
      } else {
        sinkCounter.incrementBatchUnderflowCount();
      }

      // flush all pending buckets before committing the transaction
      for (BucketWriter bucketWriter : writers) {
        bucketWriter.flush();
      }

      transaction.commit();

      if (txnEventCount &lt; 1) {
        return Status.BACKOFF;
      } else {
        sinkCounter.addToEventDrainSuccessCount(txnEventCount);
        return Status.READY;
      }
    } catch (IOException eIO) {
      transaction.rollback();
      LOG.warn(&quot;HDFS IO error&quot;, eIO);
      return Status.BACKOFF;
    } catch (Throwable th) {
      transaction.rollback();
      LOG.error(&quot;process failed&quot;, th);
      if (th instanceof Error) {
        throw (Error) th;
      } else {
        throw new EventDeliveryException(th);
      }
    } finally {
      transaction.close();
    }
  }
</code></pre><p>hdfseventsink当中维护了map<lookuppath,bucketwriter>的sfwrites哈希表，每次event来的时候，会去sfwriters当中寻找对应path(根据hdfs.path以及hdfs.fileprefix来决定)的bucketwriter作为当前写入的handle.如果找不到，就创建一个，并且将新的句炳加入到sfwriters当中去。因此实际上对于上述bucketwriter中的rotate来说，只是针对同一个路径内部的，如果没有一个路径内rollsize和rollcount滚动的需求，以及数据一定要多副本写入的需求，完全可以把bucketwriter内部判断rotate的逻辑注释掉，这样能够大大提升flume写入能力.</lookuppath,bucketwriter></p>
<p>agent.sinks.sk_cloudui.hdfs.path = hdfs://bigfile/om/anticrack/cloudui.log/%Y%m%d/%k%M/<br>agent.sinks.sk_cloudui.hdfs.filePrefix = qd01-pcsdata45.qd01.baidu.com</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/01/11/原创－flume写hdfs性能优化/" data-id="ckaafcx7400086y3c28o7dyid" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-spark-on-yarn情况下historyserver的配置" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/19/spark-on-yarn情况下historyserver的配置/" class="article-date">
  <time datetime="2017-04-19T09:39:44.000Z" itemprop="datePublished">2017-04-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/部署文档/">部署文档</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/19/spark-on-yarn情况下historyserver的配置/">原创-spark-on-yarn情况下historyserver的配置</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      <span id="busuanzi_container_page_pv">
        <br /><br /> 阅读次数<span id="busuanzi_value_page_pv"></span>
        </span>
      
        <p>标签（空格分隔）： 未分类</p>
<hr>
<p>我们经常需要在一个app执行完成之后，去对这个app的执行情况进行分析，不管是分析它如何失败也好，或者是分析这个任务执行过程是否需要优化。那么这时候我们就需要用到historyserver.</p>
<p>首先log分为两种：1)标准输入输出的log 2）spark event log<br>对应yarn页面上的两个按钮：1)logs   2)history</p>
<p>###logs配置<br>对于yarn来说，logs这块需要配置：(所有nodemanager的机器上都需要修改该配置)</p>
<pre><code>&lt;property&gt;
  &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt; 
  &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;
  &lt;value&gt;259200&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;yarn.log.server.url&lt;/name&gt;
    &lt;value&gt;http://hostA:8937/jobhistory/logs/&lt;/value&gt;（yarn会在用户点击历史任务logs的时候跳转到这个url,这个url提供的jobhistory server是mapreduce的功能）
  &lt;/property&gt;

 &lt;property&gt; 
  &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt;
  &lt;value&gt;/tmp/logs/yarn/&lt;/value&gt;(yarn会负责任务结束后将地址转存到这个位置)
  &lt;/property&gt;
</code></pre><p>在hostA上修改mapred-site.xml配置：<br>对于mapred-site.xml来说需要配置：(./sbin/mr-jobhistory-daemon.sh start historyserver)</p>
<pre><code>&lt;property&gt;
&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
&lt;value&gt;hostA:8927&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
&lt;value&gt;hostB:8937&lt;/value&gt;
&lt;/property&gt;
</code></pre><p>(historyserver挂掉，检查相关的几个目录是不是满了)</p>
<p>在hostA上执行$HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver启动historyserver。</p>
<p>这个步骤完成后在yarn页面上点击app的logs按钮就可以看到该app的日志，不管这个app执行完成与否。</p>
<p>###event log配置<br>对于history来说。yarn会负责跳转到app自身指定的history server当中去。(在hostB机器上$SPARK_HOME/sbin/start-history-server.sh启动historyserver)</p>
<pre><code>spark.eventLog.enabled           true
spark.eventLog.dir  hdfs://hostNamenode:8900/spark-event-2.0
spark.yarn.historyServer.address hostB:8651
spark.history.fs.logDirectory hdfs://hostNamenode:8900/spark-event-2.0
spark.history.retainedApplications 1000
spark.history.ui.port 8651
spark.history.fs.cleaner.enabled true
spark.history.fs.cleaner.interval 1d
spark.history.fs.cleaner.maxAge 3d
spark.executor.extraJavaOptions  -XX:+PrintGCDetails -XX:+PrintGCTimeStamps
</code></pre><p>这个完成后，就可以通过history按钮看到spark event log，不管该app执行完成与否。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/19/spark-on-yarn情况下historyserver的配置/" data-id="ckaafcx6s00056y3cen5g2tua" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-spark中parquet文件写入优化" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/13/spark中parquet文件写入优化/" class="article-date">
  <time datetime="2017-04-13T09:22:34.000Z" itemprop="datePublished">2017-04-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/性能优化/">性能优化</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/13/spark中parquet文件写入优化/">原创-spark中parquet文件写入优化</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      <span id="busuanzi_container_page_pv">
        <br /><br /> 阅读次数<span id="busuanzi_value_page_pv"></span>
        </span>
      
        <p>标签（空格分隔）： 性能优化</p>
<hr>
<p>在我们的实际使用中，经常需要将原始的文本文件转换为parquet列存储格式，以便后续查询的时候使用。写parquet能提高后续表查询效率这个事情我们不多说，下面讨论一下写parquet文件的效率问题：</p>
<p>我们来看一下两段程序：</p>
<p>1.使用case class作为df转换</p>
<pre><code>package com.yundata.transtoparquet

import java.lang.Exception
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._
import org.apache.spark.sql._
import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.sql.Row

import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary,Statistics}
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel
import org.apache.spark.mllib.util.MLUtils

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

import org.apache.spark.mllib.tree.GradientBoostedTrees
import org.apache.spark.mllib.tree.configuration.BoostingStrategy
import org.apache.spark.mllib.tree.model.GradientBoostedTreesModel
import org.apache.spark.mllib.util.MLUtils

import org.apache.spark.rdd.PairRDDFunctions

import scala.collection.mutable.ArrayBuffer


import org.apache.spark.sql.types.{StructType,StructField,StringType};
import org.apache.spark.sql.Row;


case class formatrow(fs_id:Long,user_id:Long,app_id:Long,parent_path:String,server_filename:String,s3_handle:String,size:Long,server_mtime:String,server_ctime:Long,local_mtime:Long,local_ctime:Long,isdir:Long,isdelete:Long,status:Long,category:Long,object_key:String,extent_int2:Long,recompute_tag:Long,user_range:Long,md5_range:Long,event_day:String) extends Product;


/**
 * Created by robert on 15-5-18.
 */
class transtoparquet(userConfFile: String,sourceFile:String,destFile:String,event_day:String) extends Serializable{



  def run(): Unit = {

    transtoparquetconf(userConfFile);

    val conf=transtoparquetconf.getSparkConf();
    conf.setAppName(conf.get(&quot;spark.app.name&quot;, this.getClass.getName));

    val sc=new SparkContext(conf);
    val sqlContext = new org.apache.spark.sql.SQLContext(sc);
    import sqlContext.implicits._

    sc.hadoopConfiguration.addResource(&quot;hdfs-site.xml&quot;);
    sc.hadoopConfiguration.set(&quot;parquet.enable.summary-metadata&quot;, &quot;false&quot;)

    System.out.println(sc.hadoopConfiguration.get(&quot;dfs.ha.namenodes.bigfile&quot;));

    System.out.println(sc.hadoopConfiguration.toString);



    val txtfile = sc.textFile(sourceFile);

    txtfile.take(10).foreach(println);
    println(sourceFile);


    val txtdf=txtfile.filter(_.split(&quot;\t&quot;).size==18).map(
      x=&gt;{

            var s3_handle=x.split(&quot;\t&quot;)(5);
            var md5_range=Long2long(0);
            if(x.split(&quot;\t&quot;)(5)==&quot;&quot;)
              {
                s3_handle=(new scala.util.Random).nextInt(99).toString(); //取0-99之间的随机数，保证其散列开来
                md5_range=s3_handle.toLong;
              }
            else if(x.split(&quot;\t&quot;)(5).size==32)
            {
                md5_range=((s3_handle.toLowerCase.substring(0, 24).toList.map(&quot;0123456789abcdef&quot;.indexOf(_)).map(BigInt(_)).reduceLeft( _ * 32 + _))%100).toLong
            }
            else
            {
                md5_range = Long2long(-1);
            }
            var user_range=(x.split(&quot;\t&quot;)(0)).toLong/100000000;


            if(user_range&gt;=100)
            {
                user_range=100;
            }

            if(md5_range == Long2long(-1))
              {
                null
              }


            else {
              formatrow((x.split(&quot;\t&quot;)(0)).toLong, x.split(&quot;\t&quot;)(1).toLong, x.split(&quot;\t&quot;)(2).toLong, x.split(&quot;\t&quot;)(3), x.split(&quot;\t&quot;)(4), s3_handle, (x.split(&quot;\t&quot;)(6)).toLong, x.split(&quot;\t&quot;)(7), (x.split(&quot;\t&quot;)(8)).toLong, (x.split(&quot;\t&quot;)(9)).toLong, (x.split(&quot;\t&quot;)(10)).toLong, (x.split(&quot;\t&quot;)(11)).toLong, (x.split(&quot;\t&quot;)(12)).toLong, (x.split(&quot;\t&quot;)(13)).toLong, (x.split(&quot;\t&quot;)(14)).toLong, x.split(&quot;\t&quot;)(15), (x.split(&quot;\t&quot;)(16)).toLong, (x.split(&quot;\t&quot;)(17)).toLong, user_range, md5_range, event_day);
            }
      }
    ).filter(_ != null).toDF();


    txtdf.repartition(5).write.mode(org.apache.spark.sql.SaveMode.Append).partitionBy(&quot;user_range&quot;,&quot;md5_range&quot;,&quot;event_day&quot;).parquet(destFile);

  }



}



object transtoparquet{

  def main(args: Array[String]): Unit = {
    if (args.size != 4) {
      println(&quot;usage: com.yundata.transtoparquet.transtoparquet config srcfile destfile event_day&quot;)
      return
    }
    new transtoparquet(args(0),args(1),args(2),args(3)).run()
  }
  }
</code></pre><p>这个程序实际测试，原始数据9G，压缩后大概是5.5G左右，使用50个核跑了好几个小时，居然都没有写完数据，看executor日志，几B几B地在往hdfs当中去写日志。崩溃，于是，换了一种写法。</p>
<p>2.使用row作为df转换</p>
<pre><code>package com.yundata.transtoparquet


import java.lang.Exception
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._
import org.apache.spark.sql._
import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.sql.Row

import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary,Statistics}
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel
import org.apache.spark.mllib.util.MLUtils

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

import org.apache.spark.mllib.tree.GradientBoostedTrees
import org.apache.spark.mllib.tree.configuration.BoostingStrategy
import org.apache.spark.mllib.tree.model.GradientBoostedTreesModel
import org.apache.spark.mllib.util.MLUtils

import org.apache.spark.rdd.PairRDDFunctions

import scala.collection.mutable.ArrayBuffer


import org.apache.spark.sql.types.{StructType,StructField,StringType,LongType};
import org.apache.spark.sql.Row;

/**
 * Created by robert on 15-5-18.
 */
class transtoparquet(userConfFile: String,sourceFile:String,destFile:String,event_day:String) extends Serializable{

  def run(): Unit = {

    transtoparquetconf(userConfFile);

    val conf=transtoparquetconf.getSparkConf();
    conf.setAppName(conf.get(&quot;spark.app.name&quot;, this.getClass.getName));

    val sc=new SparkContext(conf);
    val sqlContext = new org.apache.spark.sql.SQLContext(sc);
    import sqlContext.implicits._

    sc.hadoopConfiguration.addResource(&quot;hdfs-site.xml&quot;);
    sc.hadoopConfiguration.set(&quot;parquet.enable.summary-metadata&quot;, &quot;false&quot;)

    System.out.println(sc.hadoopConfiguration.get(&quot;dfs.ha.namenodes.bigfile&quot;));

    System.out.println(sc.hadoopConfiguration.toString);



    val txtfile = sc.textFile(sourceFile);

    txtfile.take(10).foreach(println);
    println(sourceFile);

    val schema=StructType(Array(StructField(&quot;fs_id&quot;,LongType,true), StructField(&quot;user_id&quot;,LongType,true),StructField(&quot;app_id&quot;,LongType,true),StructField(&quot;parent_path&quot;,StringType,true),StructField(&quot;server_filename&quot;,StringType,true),StructField(&quot;s3_handle&quot;,StringType,true),StructField(&quot;size&quot;,LongType,true),StructField(&quot;server_mtime&quot;,StringType,true),StructField(&quot;server_ctime&quot;,LongType,true),StructField(&quot;local_mtime&quot;,LongType,true),StructField(&quot;local_ctime&quot;,LongType,true),StructField(&quot;isdir&quot;,LongType,true),StructField(&quot;isdelete&quot;,LongType,true),StructField(&quot;status&quot;,LongType,true),StructField(&quot;category&quot;,LongType,true),StructField(&quot;object_key&quot;,StringType,true),StructField(&quot;extent_int2&quot;,LongType,true),StructField(&quot;recompute_tag&quot;,LongType,true),StructField(&quot;user_range&quot;,LongType,true),StructField(&quot;md5_range&quot;,LongType,true),StructField(&quot;event_day&quot;,StringType,true)));

    val txtrow=txtfile.filter(_.split(&quot;\t&quot;).size==18).map(
      x=&gt;{

            var s3_handle=x.split(&quot;\t&quot;)(5);
            var md5_range=Long2long(0);
            if(x.split(&quot;\t&quot;)(5)==&quot;&quot;)
              {
                s3_handle=(new scala.util.Random).nextInt(99).toString(); //取0-99之间的随机数，保证其散列开来
                md5_range=s3_handle.toLong;
              }
            else if(x.split(&quot;\t&quot;)(5).size==32)
            {
                md5_range=((s3_handle.toLowerCase.substring(0, 24).toList.map(&quot;0123456789abcdef&quot;.indexOf(_)).map(BigInt(_)).reduceLeft( _ * 32 + _))%100).toLong
            }
            else
            {
                md5_range = Long2long(-1);
            }
            var user_range=(x.split(&quot;\t&quot;)(0)).toLong/100000000;


            if(user_range&gt;=100)
            {
                user_range=100;
            }

            if(md5_range == Long2long(-1))
              {
                null
              }
            else {
              Row((x.split(&quot;\t&quot;)(0)).toLong, x.split(&quot;\t&quot;)(1).toLong, x.split(&quot;\t&quot;)(2).toLong, x.split(&quot;\t&quot;)(3), x.split(&quot;\t&quot;)(4), s3_handle, (x.split(&quot;\t&quot;)(6)).toLong, x.split(&quot;\t&quot;)(7), (x.split(&quot;\t&quot;)(8)).toLong, (x.split(&quot;\t&quot;)(9)).toLong, (x.split(&quot;\t&quot;)(10)).toLong, (x.split(&quot;\t&quot;)(11)).toLong, (x.split(&quot;\t&quot;)(12)).toLong, (x.split(&quot;\t&quot;)(13)).toLong, (x.split(&quot;\t&quot;)(14)).toLong, x.split(&quot;\t&quot;)(15), (x.split(&quot;\t&quot;)(16)).toLong, (x.split(&quot;\t&quot;)(17)).toLong, user_range, md5_range, event_day);
            }
      }
    ).filter(_ != null);




    val txtdf=sqlContext.createDataFrame(txtrow, schema);

    txtdf.repartition(5).write.mode(org.apache.spark.sql.SaveMode.Append).partitionBy(&quot;user_range&quot;,&quot;md5_range&quot;,&quot;event_day&quot;).parquet(destFile);
  }
}



object transtoparquet{

  def main(args: Array[String]): Unit = {

    if (args.size != 4) {
      println(&quot;usage: com.yundata.transtoparquet.transtoparquet config srcfile destfile event_day&quot;)
      return
    }
    new transtoparquet(args(0),args(1),args(2),args(3)).run()
  }
</code></pre><p>   }</p>
<p>使用了这个程序之后，很快，18分钟，就将5G parquet数据全部写入了。</p>
<p>在上述程序中，我使用的配置是：</p>
<pre><code>spark.app.name = TRANSTOPARQUET-JOB
spark.master = spark://sparkmaster:8650
spark.cores.max=50
spark.executor.instances=50
spark.executor.memory=2g
spark.speculation=true
spark.driver.maxResultSize=2g
spark.ui.port=8221
spark.ui.retainedStages=20
spark.ui.retainedJobs=20
spark.sql.parquet.compression.codec=snappy
</code></pre><p>但是这个程序还有一个问题是，如果repartition不设置的话，最后写入的文件数会非常多，大概是num(user_range)<em>num(md5_range)</em>num(event_day)*num(repartition)，很可能会瞬间打爆namenode的内存。因此repartition要设置得非常小，这又导致了整个程序会非常慢。</p>
<p>3)优化一下repartition的方式</p>
<p>原先使用repartiton(5)的方式的时候，是随机分区，导致所有的task都基本可能有每一个分区的数据，所以导致每个分区下面都有5个文件，但是如果我按照需要的分区来作哈希的话，例如repartition(user_range,md5_range,event_day)来的话，那么每个分区的数据只会存在在一个最后写入的task任务中，也就保证了整个任务产生的分区数最大是num(user_range)<em>num(md5_range)</em>num(event_day)</p>
<p>而不是原来的num(repartition)<em>num(user_range)</em>num(md5_range)*num(event_day)</p>
<p>因此我们可以随意启并发数。</p>
<p>将写入代码修改为：</p>
<p>txtdf.repartition(txtdf(“user_range”), txtdf(“md5_range”), txtdf(“event_day”)).write.mode(org.apache.spark.sql.SaveMode.Append).partitionBy(“user_range”,”md5_range”,”event_day”).parquet(destFile)</p>
<p>这样修改了之后。数据5分钟左右就全部写入了。</p>
<p>  9208         4556         3297227427 /horus/users/chenxiue</p>
<p>数据的大小也比之前小了一些，因为文件更加集中了。</p>
<p>下面我们对比一下gzip压缩和snappy压缩的效果：</p>
<p>1）压缩比率</p>
<p>原始文件大小：</p>
<p>1          517         9359700501 hdfs://namenode:8700/pika_data/file_meta_data_20170101_part4/2017011421/1483873213877</p>
<p>snappy压缩产出的文件格式类似：part-r-00000-b3ff5d89-8885-42f1-bb3d-e8dc6fb692a0.snappy.parquet</p>
<p>压缩后的文件大小：<br>9206        22157         5501724946 /horus/users/chenxiue</p>
<p>大概花了18分钟左右。</p>
<p>gzip压缩是类似：part-r-00003-be2d54a8-088e-4970-be30-363747930a6e.gz.parquet这样的文件</p>
<p>大概也花了18分钟左右<br>9206        22157         3958666080 /horus/users/chenxiue1</p>
<p>可以看出gzip的压缩比更加大一些。</p>
<p>2）在随机分区得情况下，我们尝试加大最后写入的并发度，看看会不会有加速？</p>
<p>txtdf.repartition(10).write.mode(org.apache.spark.sql.SaveMode.Append).partitionBy(“user_range”,”md5_range”,”event_day”).parquet(destFile);</p>
<p>结果写入花了19分钟？？？为啥？？<br>9208        44158         4259882305 /horus/users/chenxiue<br>发现写入的大小比原先的稍微大一些。</p>
<p>怀疑是数据太小，主要时间花在建立文件上。repartiton越大的话，文件数就越多。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/13/spark中parquet文件写入优化/" data-id="ckaafcx6p00046y3c0clxrcug" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-在kerberos-HA环境下的ranger编译安装" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/13/在kerberos-HA环境下的ranger编译安装/" class="article-date">
  <time datetime="2017-04-13T06:44:39.000Z" itemprop="datePublished">2017-04-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/部署文档/">部署文档</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/13/在kerberos-HA环境下的ranger编译安装/">原创-在kerberos+HA环境下的ranger编译安装</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      <span id="busuanzi_container_page_pv">
        <br /><br /> 阅读次数<span id="busuanzi_value_page_pv"></span>
        </span>
      
        <p>标签（空格分隔）： 部署文档，转载请注明出处</p>
<hr>
<p>1.代码下载&amp;编译</p>
<p>git clone <a href="https://github.com/apache/incubator-ranger.git" target="_blank" rel="external">https://github.com/apache/incubator-ranger.git</a><br>cd incubator-ranger<br>git checkout ranger-0.5</p>
<p>mvn clean compile package assembly:assembly install</p>
<p>下载的过程中遇到python hash库的问题，重新安装下python即可<br>另外经常因为下载库过程超时，重试几次就好了</p>
<p>编译好的目录在target目录下。</p>
<p>2.控制台ranger-admin的安装</p>
<p>1）安装mysql数据库</p>
<p>配置my.cnf:</p>
<p>basedir =/home/bae/dataplatform/jumbo<br>datadir =/home/bae/dataplatform/jumbo/var<br>port = 3309<br>socket = /home/bae/dataplatform/jumbo/var/mysql.sock</p>
<p>启动mysql:<br>./bin/mysql_install_db<br>./share/mysql/mysql.server start</p>
<p>2)生成各个模块的keytab</p>
<p>addprinc -randkey rangeradmin/hostA@EXAMPLE.COM</p>
<p>xst -k /home/bae/dataplatform/kerberos/keytab/rangeradmin.keytab rangeradmin/hostA@EXAMPLE.COM</p>
<p>addprinc -randkey rangerlookup/hostA@EXAMPLE.COM</p>
<p>xst -k /home/bae/dataplatform/kerberos/keytab/rangerlookup.keytab rangerlookup/hostA@EXAMPLE.COM</p>
<p>addprinc -randkey rangerusersync/hostA@EXAMPLE.COM</p>
<p>xst -k /home/bae/dataplatform/kerberos/keytab/rangerusersync.keytab rangerusersync/hostA@EXAMPLE.COM</p>
<p>addprinc -randkey rangertagsync/hostA@EXAMPLE.COM</p>
<p>xst -k /home/bae/dataplatform/kerberos/keytab/rangertagsync.keytab rangertagsync/hostA@EXAMPLE.COM</p>
<p>3)配置ranger-admin</p>
<p>将ranger-0.5.4-SNAPSHOT-admin.tar.gz解压到安装目录下，修改install.properties,需要修改的选项如下：</p>
<pre><code>SQL_CONNECTOR_JAR=/home/bae/dataplatform/jumbo/lib/mysql/mysql-connector-java-5.1.41-bin.jar
db_root_user=root
db_root_password=
db_host=hostMysql:3309

db_name=ranger
db_user=rangeradmin
db_password=123456


audit_store=db
audit_db_name=ranger_audit
audit_db_user=rangerlogger
audit_db_password=123456


policymgr_external_url=http://localhost:8070
policymgr_http_enabled=true

unix_user=work
unix_group=work


spnego_principal=HTTP/hostA@EXAMPLE.COM
spnego_keytab=/home/bae/dataplatform/kerberos/keytab/spnego.service.keytab
token_valid=30
cookie_domain=hostA
cookie_path=/

admin_principal=rangeradmin/hostA@EXAMPLE.COM
admin_keytab=/home/bae/dataplatform/kerberos/keytab/rangeradmin.keytab
lookup_principal=rangerlookup/hostA@EXAMPLE.COM
lookup_keytab=/home/bae/dataplatform/kerberos/keytab/rangerlookup.keytab
</code></pre><p>运行./setup.sh（root运行，否则报groupadd没有权限）</p>
<p>遇到问题：</p>
<p>a）报错：</p>
<p>SQLException : SQL state: 28000 java.sql.SQLException: Access denied for user ‘rangeradmin’@’hostA’ (using password: YES) ErrorCode: 1045</p>
<p>查看user表，该用户已经创建，但是机器没有被授权</p>
<pre><code>create user &apos;rangeradmin&apos;@&apos;hostA&apos; identified by &apos;123456&apos;;
flush privileges;
</code></pre><p>b）修改了policymgr_external_url=<a href="http://localhost:8070端口，发现8070端口没有启动成功" target="_blank" rel="external">http://localhost:8070端口，发现8070端口没有启动成功</a></p>
<p>在conf/ranger-admin-site.xml中发现</p>
<pre><code>&lt;property&gt;
&lt;name&gt;ranger.service.http.port&lt;/name&gt;
&lt;value&gt;6080&lt;/value&gt;
&lt;/property&gt;
</code></pre><p>  这里需要修改.</p>
<p>c）range-admin stop/start重新启动后就可以看到了。注意tomcat的日志在ews/logs/catalina.out当中</p>
<p>验证是否成功：打开<a href="http://localhost:8070，使用admin/admin登录" target="_blank" rel="external">http://localhost:8070，使用admin/admin登录</a></p>
<p>3.安装usersync进程</p>
<p>这个安装的目的是同步unix，或者ldap中的用户到ranger中。</p>
<p>拷贝编译好的ranger-0.5.4-SNAPSHOT-usersync.tar.gz到适当目录并解压</p>
<p>修改install.properties:（同步本机的unix用户）</p>
<pre><code>POLICY_MGR_URL = http://localhost:8070
# sync source,  only unix and ldap are supported at present
# defaults to unix
SYNC_SOURCE = unix
#User and group for the usersync process
unix_user=work
unix_group=work
logdir=/home/bae/dataplatform/ranger-0.5.4-SNAPSHOT-usersync/logs/ranger/usersync
usersync_principal=rangerusersync/hostA@EXAMPLE.COM
usersync_keytab=/home/bae/dataplatform/kerberos/keytab/rangerusersync.keytab
hadoop_conf=/home/bae/dataplatform/hadoop/conf/
</code></pre><p>使用root账号运行./setup.sh<br>启动usersync:/ranger-usersync-services.sh start<br>验证是否成功：在ranger控制台的settings－&gt;Users/Groups信息看本机的账号是否已经被同步上来。</p>
<p>4.hdfs-plugin安装(只需要在对应集群的主备namenode上安装)</p>
<p>为了让ranger能够控制hdfs，需要安装plugin</p>
<p>拷贝ranger-0.5.4-SNAPSHOT-hbase-plugin.tar.gz到对应目录并解压。修改install.properties</p>
<pre><code>POLICY_MGR_URL=http://hostA:8070
SQL_CONNECTOR_JAR=/home/bae/dataplatform/jumbo/lib/mysql/mysql-connector-java-5.1.41-bin.jar
REPOSITORY_NAME=hadoopdev(与后续页面上配置的一致)
XAAUDIT.DB.IS_ENABLED=true
XAAUDIT.DB.FLAVOUR=MYSQL
XAAUDIT.DB.HOSTNAME=hostA:3309
XAAUDIT.DB.DATABASE_NAME=ranger_audit
XAAUDIT.DB.USER_NAME=rangeradmin
XAAUDIT.DB.PASSWORD=123456

CUSTOM_USER=work
CUSTOM_GROUP=work
</code></pre><p>创建到hadoop_conf的软链：<br>ln -s /home/bae/dataplatform/hadoop-2.7.2  /home/bae/dataplatform/hadoop<br>ln -s /home/bae/dataplatform/hadoop-2.7.2/etc/hadoop/ /home/bae/dataplatform/hadoop-2.7.2/conf</p>
<p>确认$HADOOP_HOME下面有lib目录，如果没有需要编译native lib，编译方法：</p>
<p><a href="http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/NativeLibraries.html" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/NativeLibraries.html</a></p>
<p>使用root账号启动hdfs-plugin:</p>
<p>./enable-hdfs-plugin.sh(root身份运行)</p>
<p>重启namenode进程：</p>
<p>将$HADOOP_HOME/lib下面新增的ranger jar添加到hadoop_classpath变量中，</p>
<p>在conf/hadoop-env.sh中添加：</p>
<pre><code>for f in $HADOOP_HOME/lib/*.jar; do
  if [ &quot;$HADOOP_CLASSPATH&quot; ]; then
    export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$f
  else
    export HADOOP_CLASSPATH=$f
  fi
done
</code></pre><p>重新启动namenode（如果报jdbc方法找不到问题，就将mysql-connector-java-5.1.41-bin.jar拷贝到$HADOOP_HOME/lib目录下后重启）</p>
<p>5.在ranger的控制台中增加plugin配置：</p>
<p>首先创建一个kerberos的用户名密码</p>
<p>addprinc -pw password rangeradmin@example.com</p>
<p>修改core-site.xml增加映射：</p>
<pre><code>RULE:[2:$1@$0](rangeradmin@EXAMPLE.COM)s/.*/work/
RULE:[1:$1@$0](rangeradmin@EXAMPLE.COM)s/.*/work/
</code></pre><p>重启namenode使其生效，重启ranger-admin</p>
<p>在Service Manager-&gt;hdfs中增加hadoopdev（名称与hdfs plugin中设置的一致)repo</p>
<pre><code>username:rangeradmin@example.com
password:password
namenode url:hdfs://hostB:8900
Authorization Enabled:yes
Authentication Type:kerberos
hadoop.security.auth_to_local:RULE:[1:$1@$0](rangeradmin@EXAMPLE.COM)s/.*/work/
dfs.datanode.kerberos.principal:dn/_HOST@EXAMPLE.COM
dfs.namenode.kerberos.principal:nn/_HOST@EXAMPLE.COM
dfs.secondary.namenode.kerberos.principal:nn/_HOST@EXAMPLE.COM
RPC Protection Type:Authentication

dfs.nameservices = smallfile
dfs.ha.namenodes.smallfile= nn1,nn2
dfs.namenode.rpc-address.nn1 = hostB:8900
dfs.namenode.rpc-address.nn2 = hostC:8900
dfs.client.failover.proxy.provider.smallfile = org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
</code></pre><p>其中username/password需要是kerberos中有效的用户名密码。</p>
<p>点击test connection如果成功，那么save.</p>
<p>验证plugin是否添加成功：在audit-&gt;plugin目录下是否出现对应的plugin信息。</p>
<p>6.ranger对hdfs授权测试</p>
<p>注意首先要在hdfs上将权限收回，比如把一个目录权限设置成000，这样就完全由ranger policy控制。否则生效的都是hdfs上的大权限。</p>
<p>可以通过audit-&gt;access中得Access Enforcer看生效得是ranger-acl还是hadoop-acl</p>
<p>参考文档：</p>
<p><a href="https://cwiki.apache.org/confluence/display/RANGER/Apache+Ranger+0.5.0+Installation#ApacheRanger0.5.0Installation-InstallandconfigureSolrorSolrCloud" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/RANGER/Apache+Ranger+0.5.0+Installation#ApacheRanger0.5.0Installation-InstallandconfigureSolrorSolrCloud</a></p>
<p>在kerberos环境下安装ranger:</p>
<p><a href="https://cwiki.apache.org/confluence/display/RANGER/Ranger+installation+in+Kerberized++Environment" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/RANGER/Ranger+installation+in+Kerberized++Environment</a></p>
<p><a href="https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.2/bk_Security_Guide/content/hdfs_plugin_kerberos.html" target="_blank" rel="external">https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.2/bk_Security_Guide/content/hdfs_plugin_kerberos.html</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/13/在kerberos-HA环境下的ranger编译安装/" data-id="ckaafcx79000a6y3c44ssyu9u" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>



  

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Catégories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/性能优化/">性能优化</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/源码调试/">源码调试</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/部署文档/">部署文档</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Articles récents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/05/17/spark-graphx计算pagerank源代码分析/">spark graphx计算pagerank源代码分析</a>
          </li>
        
          <li>
            <a href="/2018/03/29/druid安装配置-数据导入/">druid安装配置&amp;数据导入</a>
          </li>
        
          <li>
            <a href="/2018/03/02/spark远程调试/">spark远程调试</a>
          </li>
        
          <li>
            <a href="/2018/01/11/原创－flume写hdfs性能优化/">原创-flume写hdfs性能优化</a>
          </li>
        
          <li>
            <a href="/2017/04/19/spark-on-yarn情况下historyserver的配置/">原创-spark-on-yarn情况下historyserver的配置</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 XiueChen<br>
      Propulsé by <a href="http://hexo.io/" target="_blank">Hexo</a>
      <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
      <span id="busuanzi_container_site_uv"><br />本站访客数<span id="busuanzi_value_site_uv"></span>人次
      <span id="busuanzi_container_site_pv"><br />本站访问次数<span id="busuanzi_value_site_pv"></span>
      </span>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>