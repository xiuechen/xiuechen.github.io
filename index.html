<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Nora-ä¸€ä¸ªç¨‹åºåª›çš„çª</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Nora-ä¸€ä¸ªç¨‹åºåª›çš„çª">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Nora-ä¸€ä¸ªç¨‹åºåª›çš„çª">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Nora-ä¸€ä¸ªç¨‹åºåª›çš„çª">
  
    <link rel="alternate" href="/atom.xml" title="Nora-ä¸€ä¸ªç¨‹åºåª›çš„çª" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Nora-ä¸€ä¸ªç¨‹åºåª›çš„çª</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-hugegraph-cassadraåº•å±‚å­˜å‚¨å®ç°" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/05/17/hugegraph-cassadraåº•å±‚å­˜å‚¨å®ç°/" class="article-date">
  <time datetime="2020-05-17T02:16:53.000Z" itemprop="datePublished">2020-05-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/17/hugegraph-cassadraåº•å±‚å­˜å‚¨å®ç°/">hugegraph+cassadraåº•å±‚å­˜å‚¨å®ç°</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      <span id="busuanzi_container_page_pv">
        <br /><br /> é˜…è¯»æ¬¡æ•°<span id="busuanzi_value_page_pv"></span>
        </span>
      
        <p>å‚è€ƒæ–‡æ¡£ï¼š</p>
<p>1.hugegraphçš„ç´¢å¼•ï¼š<br><a href="https://blog.csdn.net/it1993/article/details/89492296" target="_blank" rel="external">https://blog.csdn.net/it1993/article/details/89492296</a></p>
<p>2.åº•å±‚å­˜å‚¨ç»“æ„<br><a href="https://www.jianshu.com/p/c5b1d59b1fcb" target="_blank" rel="external">https://www.jianshu.com/p/c5b1d59b1fcb</a></p>
<p>###cassadraä¸­çš„åº•å±‚å­˜å‚¨è¡¨</p>
<p>ï¼ˆå…¶ä¸­g<em>xå¼€å¤´çš„åŸºæœ¬æ˜¯ç»™ä¸šåŠ¡æ•°æ®ç”¨çš„è¡¨ï¼Œè€Œs</em>å¼€å¤´çš„åŸºæœ¬æ˜¯ç³»ç»Ÿç”¨çš„è¡¨.16å¼ æ˜¯ç´¢å¼•ç›¸å…³çš„è¡¨ï¼Œ3å¼ ç³»ç»Ÿå¼‚æ­¥ä»»åŠ¡è¡¨ï¼Œ1å¼ counterè¡¨ï¼Œ7å¼ å­˜å‚¨è¡¨ï¼‰</p>
<pre><code>cqlsh&gt; desc schgraphï¼ˆæµ‹è¯•ç”¨çš„keyspace,æ ¹æ®è‡ªå·±æƒ…å†µä¿®æ”¹ï¼‰
CREATE TABLE schgraph.pk (    //å±æ€§å®šä¹‰è¡¨
    id int PRIMARY KEY,
    aggregate_type tinyint,
    cardinality tinyint,   //è¡¨ç¤ºè¯¥å±æ€§æ˜¯å•ä¸ªå€¼ï¼Œè¿˜æ˜¯é›†åˆï¼Ÿ
    data_type tinyint,
    name text,
    properties set&lt;int&gt;,
    status tinyint,
    user_data map&lt;text, text&gt;
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;
CREATE INDEX pk_name_index ON schgraph.pk (name);

CREATE TABLE schgraph.vl (    //é¡¶ç‚¹å®šä¹‰è¡¨ 
    id int PRIMARY KEY,
    enable_label_index boolean,
    id_strategy tinyint,
    index_labels set&lt;int&gt;,    //å…³è”åˆ°index_labelè¿™å¼ è¡¨
    name text,
    nullable_keys set&lt;int&gt;,
    primary_keys list&lt;int&gt;,   
    properties set&lt;int&gt;,    //å…³è”åˆ°å±æ€§è¡¨
    status tinyint,
    user_data map&lt;text, text&gt;
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;
CREATE INDEX vl_name_index ON schgraph.vl (name);




CREATE TABLE schgraph.il (   //index_labelè¡¨ï¼Œå­˜å‚¨ç´¢å¼•å®šä¹‰
    id int PRIMARY KEY,
    base_type tinyint,
    base_value int,
    fields list&lt;int&gt;,
    index_type tinyint,
    name text,
    status tinyint
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;
CREATE INDEX il_name_index ON schgraph.il (name);




CREATE TABLE schgraph.el (    //edge_labelè¡¨ï¼Œå­˜å‚¨è¾¹çš„å®šä¹‰
    id int PRIMARY KEY,
    enable_label_index boolean,
    frequency tinyint,
    index_labels set&lt;int&gt;,
    name text,
    nullable_keys set&lt;int&gt;,
    properties set&lt;int&gt;,
    sort_keys list&lt;int&gt;,
    source_label int,
    status tinyint,
    target_label int,
    user_data map&lt;text, text&gt;
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;
CREATE INDEX el_name_index ON schgraph.el (name);


 CREATE TABLE schgraph.g_v (  //graph_verticesé¡¶ç‚¹çš„å…·ä½“å­˜å‚¨
    id blob PRIMARY KEY,
    label int,
    properties map&lt;int, blob&gt;
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;
CREATE INDEX g_v_label_index ON schgraph.g_v (label);


CREATE TABLE schgraph.g_ie (   //è¾¹çš„å…·ä½“å­˜å‚¨
    owner_vertex blob,
    direction tinyint,
    label int,
    sort_values text,
    other_vertex blob,
    properties map&lt;int, blob&gt;,
    PRIMARY KEY (owner_vertex, direction, label, sort_values, other_vertex)
) WITH CLUSTERING ORDER BY (direction ASC, label ASC, sort_values ASC, other_vertex ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;        

CREATE TABLE schgraph.g_oe (  //è¾¹çš„å…·ä½“å­˜å‚¨
    owner_vertex blob,
    direction tinyint,
    label int,
    sort_values text,
    other_vertex blob,
    properties map&lt;int, blob&gt;,
    PRIMARY KEY (owner_vertex, direction, label, sort_values, other_vertex)
) WITH CLUSTERING ORDER BY (direction ASC, label ASC, sort_values ASC, other_vertex ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;
CREATE INDEX g_oe_label_index ON schgraph.g_oe (label);



CREATE TABLE schgraph.s_v ( //æ•´ä¸ªç³»ç»Ÿçš„ä»»åŠ¡taskæ‰§è¡Œæƒ…å†µï¼Œç³»ç»Ÿä¸­å­˜åœ¨å¾ˆå¤šå¼‚æ­¥ä»»åŠ¡ï¼Œæ¯”å¦‚æ–°å¢èŠ‚ç‚¹ï¼Œä¿®æ”¹èŠ‚ç‚¹éƒ½ä¼šè§¦å‘rebuild indexè¿™ç±»ä»»åŠ¡
    id blob PRIMARY KEY,
    label int,
    properties map&lt;int, blob&gt;
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;
CREATE INDEX s_v_label_index ON schgraph.s_v (label);

CREATE KEYSPACE schgraph WITH replication = {&apos;class&apos;: &apos;SimpleStrategy&apos;, &apos;replication_factor&apos;: &apos;3&apos;}  AND durable_writes = true;



CREATE TABLE schgraph.s_ie (   //system task
    owner_vertex blob,
    direction tinyint,
    label int,
    sort_values text,
    other_vertex blob,
    properties map&lt;int, blob&gt;,
    PRIMARY KEY (owner_vertex, direction, label, sort_values, other_vertex)
) WITH CLUSTERING ORDER BY (direction ASC, label ASC, sort_values ASC, other_vertex ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;





CREATE TABLE schgraph.s_oe (
    owner_vertex blob,
    direction tinyint,
    label int,
    sort_values text,
    other_vertex blob,
    properties map&lt;int, blob&gt;,
    PRIMARY KEY (owner_vertex, direction, label, sort_values, other_vertex)
) WITH CLUSTERING ORDER BY (direction ASC, label ASC, sort_values ASC, other_vertex ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;
CREATE INDEX s_oe_label_index ON schgraph.s_oe (label);





CREATE TABLE schgraph.c (   //counterè¡¨
    schema_type text PRIMARY KEY,
    id counter
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;





CREATE TABLE schgraph.s_ii (
    index_label_id int,
    field_values int,
    element_ids blob,
    PRIMARY KEY (index_label_id, field_values, element_ids)
) WITH CLUSTERING ORDER BY (field_values ASC, element_ids ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;

CREATE TABLE schgraph.g_di (  //å±æ€§å€¼ä¸ºdoubleï¼Œå¯¹åº”çš„ç´¢å¼•è¡¨ï¼Ÿ
    index_label_id int,
    field_values double,
    element_ids blob,
    PRIMARY KEY (index_label_id, field_values, element_ids)
) WITH CLUSTERING ORDER BY (field_values ASC, element_ids ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;

CREATE TABLE schgraph.g_fi (  //å±æ€§å€¼ä¸ºfloatå¯¹åº”çš„ç´¢å¼•è¡¨ï¼Ÿ
    index_label_id int,
    field_values float,
    element_ids blob,
    PRIMARY KEY (index_label_id, field_values, element_ids)
) WITH CLUSTERING ORDER BY (field_values ASC, element_ids ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;



CREATE TABLE schgraph.g_hi (  //å±æ€§ä¸ºtextå¯¹åº”çš„ç´¢å¼•è¡¨ï¼Ÿï¼Ÿ
    index_label_id int,
    field_values text,
    element_ids blob,
    PRIMARY KEY (index_label_id, field_values, element_ids)
) WITH CLUSTERING ORDER BY (field_values ASC, element_ids ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;

CREATE TABLE schgraph.s_ai (
    field_values text,
    index_label_id int,
    element_ids blob,
    PRIMARY KEY (field_values, index_label_id, element_ids)
) WITH CLUSTERING ORDER BY (index_label_id ASC, element_ids ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;



CREATE TABLE schgraph.s_hi (
    index_label_id int,
    field_values text,
    element_ids blob,
    PRIMARY KEY (index_label_id, field_values, element_ids)
) WITH CLUSTERING ORDER BY (field_values ASC, element_ids ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;



CREATE TABLE schgraph.s_si (
    field_values text,
    index_label_id int,
    element_ids blob,
    PRIMARY KEY (field_values, index_label_id, element_ids)
) WITH CLUSTERING ORDER BY (index_label_id ASC, element_ids ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;

CREATE TABLE schgraph.s_di (
    index_label_id int,
    field_values double,
    element_ids blob,
    PRIMARY KEY (index_label_id, field_values, element_ids)
) WITH CLUSTERING ORDER BY (field_values ASC, element_ids ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;

CREATE TABLE schgraph.s_li (
    index_label_id int,
    field_values bigint,
    element_ids blob,
    PRIMARY KEY (index_label_id, field_values, element_ids)
) WITH CLUSTERING ORDER BY (field_values ASC, element_ids ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;

CREATE TABLE schgraph.s_ui (
    field_values text,
    index_label_id int,
    element_ids blob,
    PRIMARY KEY (field_values, index_label_id, element_ids)
) WITH CLUSTERING ORDER BY (index_label_id ASC, element_ids ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;

CREATE TABLE schgraph.g_li (  //å±æ€§å€¼ä¸ºlongå¯¹åº”çš„ç´¢å¼•è¡¨ï¼Ÿ
    index_label_id int,  //å¯¹åº”index_labelé‡Œé¢çš„ç´¢å¼•id
    field_values bigint,
    element_ids blob,
    PRIMARY KEY (index_label_id, field_values, element_ids)
) WITH CLUSTERING ORDER BY (field_values ASC, element_ids ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;



CREATE TABLE schgraph.g_ii (  //å±æ€§å€¼ä¸ºintå¯¹åº”çš„ç´¢å¼•è¡¨ï¼Ÿ
    index_label_id int,
    field_values int,
    element_ids blob,
    PRIMARY KEY (index_label_id, field_values, element_ids)
) WITH CLUSTERING ORDER BY (field_values ASC, element_ids ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;

CREATE TABLE schgraph.g_ai (  //å’Œg_hiçš„åŒºåˆ«åœ¨å“ªé‡Œï¼Ÿ
    field_values text,
    index_label_id int,
    element_ids blob,
    PRIMARY KEY (field_values, index_label_id, element_ids)
) WITH CLUSTERING ORDER BY (index_label_id ASC, element_ids ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;

CREATE TABLE schgraph.s_fi (
    index_label_id int,
    field_values float,
    element_ids blob,
    PRIMARY KEY (index_label_id, field_values, element_ids)
) WITH CLUSTERING ORDER BY (field_values ASC, element_ids ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;


    CREATE TABLE schgraph.g_si (  //ğŸ§s
    field_values text,
    index_label_id int,
    element_ids blob,
    PRIMARY KEY (field_values, index_label_id, element_ids)
) WITH CLUSTERING ORDER BY (index_label_id ASC, element_ids ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;



    CREATE TABLE schgraph.g_ui (
    field_values text,
    index_label_id int,
    element_ids blob,
    PRIMARY KEY (field_values, index_label_id, element_ids)
) WITH CLUSTERING ORDER BY (index_label_id ASC, element_ids ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {&apos;keys&apos;: &apos;ALL&apos;, &apos;rows_per_partition&apos;: &apos;NONE&apos;}
    AND comment = &apos;&apos;
    AND compaction = {&apos;class&apos;: &apos;org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy&apos;, &apos;max_threshold&apos;: &apos;32&apos;, &apos;min_threshold&apos;: &apos;4&apos;}
    AND compression = {&apos;chunk_length_in_kb&apos;: &apos;64&apos;, &apos;class&apos;: &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &apos;99PERCENTILE&apos;;
</code></pre><p>###è¯¦ç»†ç¤ºä¾‹</p>
<p>####1.å›¾å®šä¹‰</p>
<pre><code>graph.schema().propertyKey(&quot;uid&quot;).asText().ifNotExist().create();
graph.schema().propertyKey(&quot;shorturl&quot;).asText().ifNotExist().create();
graph.schema().propertyKey(&quot;timestamp&quot;).asLong().ifNotExist().create();
graph.schema().vertexLabel(&quot;person&quot;).properties(&quot;uid&quot;).primaryKeys(&quot;uid&quot;).ifNotExist().create();
graph.schema().vertexLabel(&quot;shorturl&quot;).properties(&quot;shorturl&quot;).primaryKeys(&quot;shorturl&quot;).ifNotExist().create();
graph.schema().indexLabel(&quot;createByTime&quot;).onE(&quot;create&quot;).by(&quot;timestamp&quot;).range().ifNotExist().create();



cqlsh:hgraphtest&gt; select * from pk;                 

 id  | aggregate_type | cardinality | data_type | name               | properties | status | user_data
-----+----------------+-------------+-----------+--------------------+------------+--------+----------------------
   1 |              0 |           1 |         8 |                uid |       null |      1 |                 null
    2 |              0 |           1 |         8 |           shorturl |       null |      1 |                 null
    3 |              0 |           1 |         5 |          timestamp |       null |      1 |                 null



    cqlsh:hgraphtest&gt; select * from vl;  

 id  | enable_label_index | id_strategy | index_labels | name     | nullable_keys             | primary_keys | properties                                                   | status | user_data
-----+--------------------+-------------+--------------+----------+---------------------------+--------------+--------------------------------------------------------------+--------+-----------
   1 |               True |           2 |         null |   person |                      null |          [1] |                                                          {1} |      1 |      null
   2 |               True |           2 |         null | shorturl |                      null |          [2] |                                                          {2} |      1 |      null  

cqlsh:hgraphtest&gt; select * from il;

 id  | base_type | base_value | fields | index_type | name                        | status
-----+-----------+------------+--------+------------+-----------------------------+--------
   1 |         2 |          1 |    [3] |         23 |                createByTime |      2



    cqlsh:hgraphtest&gt; select * from el;

 id | enable_label_index | frequency | index_labels | name   | nullable_keys | properties | sort_keys | source_label | status | target_label | user_data
----+--------------------+-----------+--------------+--------+---------------+------------+-----------+--------------+--------+--------------+-----------
  1 |               True |         1 |          {1} | create |          null |        {3} |      null |            1 |      1 |            2 |      null

(1 rows)
</code></pre><p>å› ä¸ºæˆ‘ä»¬æ²¡æœ‰æ’å…¥é¡¶ç‚¹å’Œè¾¹ï¼Œè¿™æ—¶å€™g_v,g_ie,g_oeéƒ½ä¸ºç©º,ç»Ÿè®¡è¡¨å¦‚å›¾ï¼š</p>
<p>cqlsh:hgraphtest&gt; select * from c;   </p>
<pre><code> schema_type  | id
--------------+----
 PROPERTY_KEY |  3
 VERTEX_LABEL |  2
   SYS_SCHEMA | 47
  INDEX_LABEL |  1
   EDGE_LABEL |  1
         TASK |  1
</code></pre><p>####2.æ’å…¥é¡¶ç‚¹</p>
<p>marko = graph.addVertex(T.label, â€œpersonâ€, â€œuidâ€,â€2229095301â€)<br>url=graph.addVertex(T.label, â€œshorturlâ€, â€œshorturlâ€,â€MfDALoIBLMXbuQIezN6T2Qâ€)</p>
<p>  cqlsh:hgraphtest&gt; select * from g_v;</p>
<p> id                                                   | label | properties<br>â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”+â€”â€”-+â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”-<br>                         0x8b313a32323239303935333031 |     1 |                         {1: 0x0a32323239303935333031}<br> 0x97323a4d6644414c6f49424c4d5862755149657a4e36543251 |     2 | {2: 0x164d6644414c6f49424c4d5862755149657a4e36543251}</p>
<p>####3.æ’å…¥è¾¹</p>
<p>marko.addEdge(â€œcreateâ€, url, â€œtimestampâ€, 1587779170)</p>
<pre><code>cqlsh:hgraphtest&gt; select * from g_oe;

 owner_vertex                 | direction | label | sort_values | other_vertex                                         | properties
------------------------------+-----------+-------+-------------+------------------------------------------------------+-------------------
 0x8b313a32323239303935333031 |      -126 |     1 |             | 0x97323a4d6644414c6f49424c4d5862755149657a4e36543251 | {3: 0x85f58eac62}

(1 rows)
cqlsh:hgraphtest&gt; select * from g_ie;

 owner_vertex                                         | direction | label | sort_values | other_vertex                 | properties
------------------------------------------------------+-----------+-------+-------------+------------------------------+-------------------
 0x97323a4d6644414c6f49424c4d5862755149657a4e36543251 |      -116 |     1 |             | 0x8b313a32323239303935333031 | {3: 0x85f58eac62}

(1 rows)


cqlsh:hgraphtest&gt; select * from g_li;(timestampå¯¹åº”çš„é‚£ä¸ªrangeç´¢å¼•)

 index_label_id | field_values | element_ids
----------------+--------------+------------------------------------------------------------------------------------------
              1 |   1587779170 | 0x7e8b313a32323239303935333031820801ff97323a4d6644414c6f49424c4d5862755149657a4e36543251
</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/05/17/hugegraph-cassadraåº•å±‚å­˜å‚¨å®ç°/" data-id="ckaafjsg10001ci3cw8puodmm" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-spark-graphxè®¡ç®—pagerankæºä»£ç åˆ†æ" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/05/17/spark-graphxè®¡ç®—pagerankæºä»£ç åˆ†æ/" class="article-date">
  <time datetime="2020-05-17T02:11:05.000Z" itemprop="datePublished">2020-05-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/17/spark-graphxè®¡ç®—pagerankæºä»£ç åˆ†æ/">spark graphxè®¡ç®—pagerankæºä»£ç åˆ†æ</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      <span id="busuanzi_container_page_pv">
        <br /><br /> é˜…è¯»æ¬¡æ•°<span id="busuanzi_value_page_pv"></span>
        </span>
      
        <p>##å…¥å£å‡½æ•°</p>
<pre><code>def pageRank(tol: Double, resetProb: Double = 0.15): Graph[Double, Double] = {                                                      
    PageRank.runUntilConvergence(graph, tol, resetProb)
}
</code></pre><p>å…¶ä¸­resetProbçš„ä½œç”¨å¯ä»¥å‚è€ƒpagerankåŸç†çš„è¯´æ˜ï¼š</p>
<p>##å®ç°å‡½æ•°</p>
<pre><code>196   def runUntilConvergenceWithOptions[VD: ClassTag, ED: ClassTag](
197       graph: Graph[VD, ED], tol: Double, resetProb: Double = 0.15,
198       srcId: Option[VertexId] = None): Graph[Double, Double] =
199   {
//è¿™ä¸ªç”¨æ¥è¡¨ç¤ºï¼Œç”¨æˆ·æ˜¯å¦è‡ªå®šä¹‰äº†æ¸¸èµ°çš„èµ·ç‚¹
200     val personalized = srcId.isDefined
//é»˜è®¤æ˜¯éšæœºé€‰æ‹©èµ·ç‚¹
201     val src: VertexId = srcId.getOrElse(-1L)
202 
// Initialize the pagerankGraph with each edge attribute
// having weight 1/outDegree and each vertex with attribute 1.0.
//åˆå§‹åŒ–pagerankGraph,è¾¹çš„å±æ€§è®¾ç½®ä¸ºæºèŠ‚ç‚¹çš„å‡ºåº¦çš„å€’æ•°ã€‚å¯¹äºå¯åŠ¨èŠ‚ç‚¹ï¼Œé¡¶ç‚¹è®¾ç½®ä¸ºï¼ˆresetProb, Double.NegativeInfinityï¼‰,å…¶ä»–çš„é¡¶ç‚¹è®¾ç½®ä¸ºï¼ˆ0ï¼Œ0ï¼‰,å›¾çš„é¡¶ç‚¹æ ¼å¼ä¸ºVertextRDD[(Double,Double)]
205     val pagerankGraph: Graph[(Double, Double), Double] = graph
206       // Associate the degree with each vertex
207       .outerJoinVertices(graph.outDegrees) {
208         (vid, vdata, deg) =&gt; deg.getOrElse(0)
209       }
210       // Set the weight on the edges based on the degree
211       .mapTriplets( e =&gt; 1.0 / e.srcAttr )
212       // Set the vertex attributes to (initalPR, delta = 0)
213       .mapVertices { (id, attr) =&gt;
214         if (id == src) (resetProb, Double.NegativeInfinity) else (0.0, 0.0)
215       }
216       .cache()
217 
// Define the three functions needed to implement PageRank in the GraphX
// version of Pregel

//å®šä¹‰pregelä¸­çš„vprogï¼Œè¿™é‡Œçš„msgSumå°±æ˜¯ä¸‹é¢map/reduceäº§å‡ºçš„messageså›¾ï¼Œæ¯ä¸ªèŠ‚ç‚¹çš„æ ¼å¼æ˜¯VertexRDD[double],æ›´æ–°å›¾
220     def vertexProgram(id: VertexId, attr: (Double, Double), msgSum: Double): (Double, Double) = {
221       val (oldPR, lastDelta) = attr
222       val newPR = oldPR + (1.0 - resetProb) * msgSum
223       (newPR, newPR - oldPR)
224     }
//å®šä¹‰æŒ‡å®šäº†å¯åŠ¨èŠ‚ç‚¹çš„vprog
226     def personalizedVertexProgram(id: VertexId, attr: (Double, Double),
227       msgSum: Double): (Double, Double) = {
228       val (oldPR, lastDelta) = attr
229       var teleport = oldPR
230       val delta = if (src==id) 1.0 else 0.0
231       teleport = oldPR*delta
232 
233       val newPR = teleport + (1.0 - resetProb) * msgSum
234       val newDelta = if (lastDelta == Double.NegativeInfinity) newPR else newPR - oldPR
235       (newPR, newDelta)
236     }

//å®šä¹‰pregelä¸­çš„sendmessageï¼Œå¦‚æœæºèŠ‚ç‚¹çš„æƒé‡&gt;tolçš„è¯ï¼Œåˆ™æŒ‰ç…§è¾¹çš„æƒé‡(edge.attr)åŠ æƒä¼ é€’è¿‡æ¥
238     def sendMessage(edge: EdgeTriplet[(Double, Double), Double]) = {
239       if (edge.srcAttr._2 &gt; tol) {
240         Iterator((edge.dstId, edge.srcAttr._2 * edge.attr))
241       } else {
242         Iterator.empty
243       }
244     }
//å®šä¹‰pregelçš„reduceç¨‹åºï¼Œå°†å„ä¸ªèŠ‚ç‚¹ä¼ é€’è¿‡æ¥çš„æƒé‡ç›¸åŠ å³å¯
246     def messageCombiner(a: Double, b: Double): Double = a + b
247 
248     // The initial message received by all vertices in PageRank
249     val initialMessage = if (personalized) 0.0 else resetProb / (1.0 - resetProb)
250 
251     // Execute a dynamic version of Pregel.
252     val vp = if (personalized) {
253       (id: VertexId, attr: (Double, Double), msgSum: Double) =&gt;
254         personalizedVertexProgram(id, attr, msgSum)
255     } else {
256       (id: VertexId, attr: (Double, Double), msgSum: Double) =&gt;
257         vertexProgram(id, attr, msgSum)
258     }
259 
260     Pregel(pagerankGraph, initialMessage, activeDirection = EdgeDirection.Out)(
261       vp, sendMessage, messageCombiner)
262       .mapVertices((vid, attr) =&gt; attr._1)
263   } // end of deltaPageRank
264 
265 } 
</code></pre><p>###pregelçš„å…·ä½“å®ç°(ä¸€æ¬¡mapReduceTripletså®Œæˆä¸€æ¬¡å…¨å±€æƒé‡è°ƒæ•´ï¼Œvprogå¯ä»¥ç†è§£ä¸ºä¸¤æ¬¡æƒé‡è°ƒæ•´çš„æ›´æ–°)</p>
<pre><code>class GraphOps[VD, ED] {
  def pregel[A]
      (initialMsg: A,
       maxIter: Int = Int.MaxValue,
       activeDir: EdgeDirection = EdgeDirection.Out)
      (vprog: (VertexId, VD, A) =&gt; VD,
       sendMsg: EdgeTriplet[VD, ED] =&gt; Iterator[(VertexId, A)],
       mergeMsg: (A, A) =&gt; A)
    : Graph[VD, ED] = {
    // Receive the initial message at each vertex
    var g = mapVertices( (vid, vdata) =&gt; vprog(vid, vdata, initialMsg) ).cache()  

    // compute the messages
    var messages = GraphXUtils.mapReduceTriplets(g, sendMsg, mergeMsg)  
    var activeMessages = messages.count()
    // Loop until no messages remain or maxIterations is achieved
    var i = 0
    while (activeMessages &gt; 0 &amp;&amp; i &lt; maxIterations) {
      // Receive the messages and update the vertices.
      g = g.joinVertices(messages)(vprog).cache()
      val oldMessages = messages
      // Send new messages, skipping edges where neither side received a message. We must cache
      // messages so it can be materialized on the next line, allowing us to uncache the previous
      // iteration.
      messages = GraphXUtils.mapReduceTriplets(
        g, sendMsg, mergeMsg, Some((oldMessages, activeDirection))).cache()
      activeMessages = messages.count()
      i += 1
    }
    g
  }
}
</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/05/17/spark-graphxè®¡ç®—pagerankæºä»£ç åˆ†æ/" data-id="ckaafjsge0003ci3c4j50a3lv" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-druidå®‰è£…é…ç½®-æ•°æ®å¯¼å…¥" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/29/druidå®‰è£…é…ç½®-æ•°æ®å¯¼å…¥/" class="article-date">
  <time datetime="2018-03-29T11:13:50.000Z" itemprop="datePublished">2018-03-29</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/éƒ¨ç½²æ–‡æ¡£/">éƒ¨ç½²æ–‡æ¡£</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/29/druidå®‰è£…é…ç½®-æ•°æ®å¯¼å…¥/">druidå®‰è£…é…ç½®&amp;æ•°æ®å¯¼å…¥</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      <span id="busuanzi_container_page_pv">
        <br /><br /> é˜…è¯»æ¬¡æ•°<span id="busuanzi_value_page_pv"></span>
        </span>
      
        <p>ä¸€.druidçš„è®¾è®¡ä¸æ¶æ„</p>
<p>è®¾è®¡ï¼š<a href="http://druid.io/docs/0.9.1.1/design/design.html" target="_blank" rel="external">http://druid.io/docs/0.9.1.1/design/design.html</a><br>ç™½çš®ä¹¦ï¼š<a href="http://static.druid.io/docs/druid.pdf" target="_blank" rel="external">http://static.druid.io/docs/druid.pdf</a></p>
<p>(<a href="https://docs.imply.io/on-premise/deploy/cluster" target="_blank" rel="external">https://docs.imply.io/on-premise/deploy/cluster</a>)</p>
<p>äº”ç§èŠ‚ç‚¹ç±»å‹ï¼š</p>
<p>Historical: ç¦»çº¿èŠ‚ç‚¹ï¼ŒåŠ è½½ç¦»çº¿å­˜å‚¨çš„segmentsã€‚å®ƒå’Œcoordinatoré€šè¿‡zkè¿›è¡Œè”ç³»ã€‚å½“æ¥æ”¶åˆ°æ–°çš„segmentsåŠ è½½è¯·æ±‚çš„æ—¶å€™ï¼Œå…ˆæŸ¥æœ¬åœ°ï¼Œæ²¡å‘½ä¸­åˆ™æ ¹æ®metadataä¿¡æ¯ä»deep storageä¸­åŠ è½½ï¼ŒåŠ è½½å®Œæˆåç”³æŠ¥åˆ°zkï¼Œè¿™æ—¶å€™è¯¥segmentå°±å¯ä»¥è¢«æŸ¥è¯¢</p>
<p>Broker:æ¥å—æŸ¥è¯¢ï¼Œæ ¹æ®zkçš„ä¿¡æ¯æŸ¥è¯¢segmentçš„ä½ç½®ï¼Œå°†æŸ¥è¯¢è·¯ç”±åˆ°æ­£ç¡®çš„ä½ç½®ã€‚æœ€åmergeç»“æœè¿”å›</p>
<p>Coordinator:åè°ƒsegmentçš„å­˜å‚¨ï¼Œå†³å®šå“ªäº›segmentsåº”è¯¥è¿›historical nodes</p>
<p>Indexing Service:åŒ…å«ä¸‰å¤§ç»„ä»¶ã€‚peon,middle manager,overlord<br>ä»»åŠ¡ä»overlordçš„httpæäº¤ï¼Œç”±middle manageråˆ†é…ç»™Peonså¤„ç†ã€‚</p>
<p>Realtime:å®æ—¶èŠ‚ç‚¹</p>
<p>å…¶ä»–åè¯ï¼š</p>
<p>Tranquilityï¼š helps you send real-time event streams to Druid and handles partitioning, replication, service discovery, and schema rollover, seamlessly and without downtime</p>
<p>Tranquility server:ä¸€ä¸ªhttp serverï¼Œæœ‰å®ƒå°±å¯ä»¥ä¸éœ€è¦å†™javaç¨‹åºæ¥å¯¼æ•°æ®åˆ°druidï¼Œè€Œé€šè¿‡httpæ¥å£å°±å¯ä»¥</p>
<p>ä¾èµ–çš„å¤–å›´æ¨¡å—ï¼š</p>
<p>Deep Storageï¼š<br>Metadata Storageï¼š<br>ZooKeeperï¼š</p>
<hr>
<p>äºŒ.å®‰è£…éƒ¨ç½²ï¼š</p>
<p>1.å‚è€ƒ</p>
<p><a href="http://druid.io/docs/0.9.1.1/tutorials/cluster.htmlï¼ˆ./bin/xxx.sh" target="_blank" rel="external">http://druid.io/docs/0.9.1.1/tutorials/cluster.htmlï¼ˆ./bin/xxx.sh</a> startå¯åŠ¨å„ä¸ªå¯¹åº”çš„æœåŠ¡ï¼‰<br><a href="http://www.open-open.com/lib/view/open1447852962978.html" target="_blank" rel="external">http://www.open-open.com/lib/view/open1447852962978.html</a></p>
<p>2.ä¸‹è½½druidä»¥åŠmysql extenstion&amp;tranquility</p>
<p><a href="http://druid.io/downloads.html" target="_blank" rel="external">http://druid.io/downloads.html</a></p>
<p>3.æ‹·è´mysql-metadata-storage-0.12.0.tar.gzåˆ°extensionsè·¯å¾„ä¸‹å¹¶è§£å‹</p>
<p>create database <code>druid</code> DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci;</p>
<p>4.é…ç½®conf/druid/_common/common.runtime.properties</p>
<p>5.æ‹·è´hadoopç›¸å…³é…ç½®åˆ°conf/druid/_commonä¸‹ï¼ˆcore-site.xml, hdfs-site.xml, yarn-site.xml, mapred-site.xmï¼‰</p>
<p>6.å¯åŠ¨å„ä¸ªç»„ä»¶ï¼š(ä¹Ÿå¯ä»¥ä½¿ç”¨sh bin/XXX.sh startæ¥å¯åŠ¨)</p>
<p>å¯åŠ¨ä¸€ä¸ªå®ä¾‹å°±å¤Ÿäº†<br>java <code>cat conf/druid/coordinator/jvm.config | xargs</code> -cp conf/druid/_common:conf/druid/coordinator:lib/<em> io.druid.cli.Main server coordinator<br>java <code>cat conf/druid/overlord/jvm.config | xargs</code> -cp conf/druid/_common:conf/druid/overlord:lib/</em> io.druid.cli.Main server overlord</p>
<p>å¯ä»¥æŒ‰éœ€å¯åŠ¨å¤šä¸ªå®ä¾‹<br>java <code>cat conf/druid/historical/jvm.config | xargs</code> -cp conf/druid/_common:conf/druid/historical:lib/<em> io.druid.cli.Main server historical<br>java <code>cat conf/druid/middleManager/jvm.config | xargs</code> -cp conf/druid/_common:conf/druid/middleManager:lib/</em> io.druid.cli.Main server middleManager<br>java <code>cat conf/druid/broker/jvm.config | xargs</code> -cp conf/druid/_common:conf/druid/broker:lib/* io.druid.cli.Main server broker</p>
<p>é‡åˆ°é—®é¢˜ï¼š</p>
<p>historical nodeå†…å­˜å¯ä¸äº†</p>
<p>å¯åŠ¨å¾—æ—¶å€™æŠ¥é”™ï¼š</p>
<pre><code>12) Not enough direct memory.  Please adjust -XX:MaxDirectMemorySize, druid.processing.buffer.sizeBytes, druid.processing.numThreads, or druid.processing.numMergeBuffers: maxDirectMemory[2,147,483,648], memoryNeeded[5,368,709,120] = druid.processing.buffer.sizeBytes[536,870,912] * (druid.processing.numMergeBuffers[2] + druid.processing.numThreads[7] + 1)
</code></pre><p>æ ¹æ®æç¤ºå°†maxDirectMemoryä»2Gä¿®æ”¹ä¸º5Gå°±å¯ä»¥äº†ã€‚ã€‚</p>
<p><a href="https://groups.google.com/forum/#!topic/druid-user/j0sFcUIiQiE" target="_blank" rel="external">https://groups.google.com/forum/#!topic/druid-user/j0sFcUIiQiE</a></p>
<p>ä¸‰.druidçš„æ•°æ®å¯¼å…¥ç®€ä»‹ï¼š(åˆ†fileså’Œstream)</p>
<p>filesæ–¹å¼ä¸ä¾èµ–tranquilityï¼Œå‚è€ƒ<a href="http://druid.io/docs/latest/tutorials/tutorial-batch.html" target="_blank" rel="external">http://druid.io/docs/latest/tutorials/tutorial-batch.html</a></p>
<p>streamæ•°æ®å¯¼å…¥æœ‰ä¸¤ç§æ–¹å¼ï¼š</p>
<p>1.Tranquility (a Druid-aware client) and the indexing service(pushæ–¹å¼)</p>
<p>2.Realtime nodes(ä¸æ¨èï¼Œæœ‰è‹¥å¹²ç¼ºç‚¹ï¼š<a href="http://druid.io/docs/0.9.1.1/ingestion/stream-pull.html#limitations" target="_blank" rel="external">http://druid.io/docs/0.9.1.1/ingestion/stream-pull.html#limitations</a>) (pull)</p>
<p>stream push &amp; stream pull &amp;batch ingestion</p>
<p>stream pushæœ‰ä¸¤ç§æ–¹å¼ï¼š</p>
<p>1ï¼‰é€šè¿‡Tranquility serveré€šè¿‡httpæ¥å£æ¨è¿›å»<br><a href="http://druid.io/docs/0.9.1.1/tutorials/tutorial-streams.html" target="_blank" rel="external">http://druid.io/docs/0.9.1.1/tutorials/tutorial-streams.html</a></p>
<p>2ï¼‰é€šè¿‡Tranquility Kafkaæ¨è¿›å»</p>
<p>stream pull:</p>
<p>é€šè¿‡realtime nodeçš„æ–¹å¼ï¼Œå‚è€ƒï¼š<br><a href="http://druid.io/docs/latest/ingestion/stream-pull.html" target="_blank" rel="external">http://druid.io/docs/latest/ingestion/stream-pull.html</a></p>
<p>å››.druidçš„æ•°æ®å¯¼å…¥</p>
<p>1.fileså¯¼å…¥ï¼Œå‚è€ƒï¼š<a href="http://druid.io/docs/latest/tutorials/tutorial-batch.html" target="_blank" rel="external">http://druid.io/docs/latest/tutorials/tutorial-batch.html</a></p>
<p>curl -X â€˜POSTâ€™ -H â€˜Content-Type:application/jsonâ€™ -d@./wikiticker-index.json host-170.bjyz:8090/druid/indexer/v1/task</p>
<p>é‡åˆ°é—®é¢˜ï¼š</p>
<p>1)peonå¯åŠ¨ä¸èµ·æ¥ï¼ŒæŠ¥é”™ï¼š</p>
<pre><code>3) Not enough direct memory.  Please adjust -XX:MaxDirectMemorySize, druid.processing.buffer.sizeBytes, druid.processing.numThreads, or druid.processing.numMergeBuffers: maxDirectMemory[1,908,932,608], memoryNeeded[2,684,354,560] = druid.processing.buffer.sizeBytes[536,870,912] * (druid.processing.numMergeBuffers[2] + druid.processing.numThreads[2] + 1)
</code></pre><p>ä¿®æ”¹ï¼šdruid.indexer.runner.javaOpts=-server -Xmx2g -XX:MaxDirectMemorySize=2560m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -Djav<br>a.util.logging.manager=org.apache.logging.log4j.jul.LogManager</p>
<p>å¯ä»¥æŸ¥çœ‹çŠ¶æ€ï¼š</p>
<p><a href="http://host-170.bjyz.baidu.com:8090/console.html" target="_blank" rel="external">http://host-170.bjyz.baidu.com:8090/console.html</a> ï¼ˆoverlordå¾—ç«¯å£,å¯ä»¥æŸ¥çœ‹å¯¼å…¥æœåŠ¡å¾—è¿è¡ŒçŠ¶æ€ï¼‰</p>
<p>å¯ä»¥çœ‹åˆ°middlemanagerå·²ç»å¯åŠ¨äº†ä¸€ä¸ªpeonæ¥æ‰§è¡Œä»»åŠ¡ï¼š</p>
<p>Main internal peon var/druid/task/index_hadoop_wikiticker_2018-03-27T10:07:12.646Z/task.json var/druid/task/index_hadoop_wikiticker_2018-03-27T10:07:12.646Z/d93e84a0-d9f2-40e4-8b1a-0e24072a00f3/status.json</p>
<p>2)é»˜è®¤çš„mr jobsæ˜¯æäº¤åˆ°yarné›†ç¾¤çš„defaulté˜Ÿåˆ—ï¼Œä¸ºäº†ä¿®æ”¹è¯¥peonæ‰€æäº¤å¾—mapreduce jobå¾—queue nameã€‚è®¾ç½®ä¸€ä¸‹commonä¸­å¾—mapred-site.xmlå¾—mapreduce.job.queuenameå‚æ•°</p>
<p>3)å…³äºyarné›†ç¾¤javaç‰ˆæœ¬å’Œdruidä¾èµ–javaç‰ˆæœ¬ä¸ä¸€è‡´å¾—é—®é¢˜ï¼š</p>
<p>conf.set(â€œmapred.child.envâ€, â€œJAVA_HOME=/home/iteblog/java/jdk1.8.0_25â€);<br>conf.set(â€œyarn.app.mapreduce.am.envâ€, â€œJAVA_HOME=/home/iteblog/java/jdk1.8.0_25â€);</p>
<p>å¯ä»¥å‚è€ƒï¼š<a href="https://www.iteblog.com/archives/1883.html" target="_blank" rel="external">https://www.iteblog.com/archives/1883.html</a></p>
<p> 4)é‡åˆ°JNDI lookup class is not available because this JRE does not support JNDIçš„é—®é¢˜(è¿™ä¸ªwarningæ— è§†å°±å¥½äº†ã€‚ã€‚ã€‚å“ˆå“ˆ)</p>
<p> å‚è€ƒ<a href="http://druid.io/docs/latest/operations/other-hadoop.htmlçš„Tip" target="_blank" rel="external">http://druid.io/docs/latest/operations/other-hadoop.htmlçš„Tip</a> 2è§£å†³</p>
<p> 5)ä½¿ç”¨çš„wikiticker-index.jsonæ–‡ä»¶</p>
<pre><code>{
 &quot;type&quot; : &quot;index_hadoop&quot;,
 &quot;spec&quot; : {
   &quot;ioConfig&quot; : {
     &quot;type&quot; : &quot;hadoop&quot;,
     &quot;inputSpec&quot; : {
       &quot;type&quot; : &quot;static&quot;,
       &quot;paths&quot; : &quot;/smallfile/druid/quickstart/wikiticker-2015-09-12-sampled.json.gz&quot;
     }
   },
   &quot;dataSchema&quot; : {
     &quot;dataSource&quot; : &quot;wikiticker&quot;,
     &quot;granularitySpec&quot; : {
       &quot;type&quot; : &quot;uniform&quot;,
       &quot;segmentGranularity&quot; : &quot;day&quot;,
       &quot;queryGranularity&quot; : &quot;none&quot;,
       &quot;intervals&quot; : [&quot;2015-09-12/2015-09-13&quot;]
     },
     &quot;parser&quot; : {
       &quot;type&quot; : &quot;hadoopyString&quot;,
               &quot;parseSpec&quot; : {
         &quot;format&quot; : &quot;json&quot;,
         &quot;dimensionsSpec&quot; : {
           &quot;dimensions&quot; : [
             &quot;channel&quot;,
             &quot;cityName&quot;,
             &quot;comment&quot;,
             &quot;countryIsoCode&quot;,
             &quot;countryName&quot;,
             &quot;isAnonymous&quot;,
             &quot;isMinor&quot;,
             &quot;isNew&quot;,
             &quot;isRobot&quot;,
             &quot;isUnpatrolled&quot;,
             &quot;metroCode&quot;,
             &quot;namespace&quot;,
             &quot;page&quot;,
             &quot;regionIsoCode&quot;,
                           &quot;regionName&quot;,
             &quot;user&quot;
           ]
         },
         &quot;timestampSpec&quot; : {
           &quot;format&quot; : &quot;auto&quot;,
           &quot;column&quot; : &quot;time&quot;
         }
       }
     },
     &quot;metricsSpec&quot; : [
       {
         &quot;name&quot; : &quot;count&quot;,
         &quot;type&quot; : &quot;count&quot;
       },
       {
         &quot;name&quot; : &quot;added&quot;,
         &quot;type&quot; : &quot;longSum&quot;,
         &quot;fieldName&quot; : &quot;added&quot;
       },        {
         &quot;name&quot; : &quot;deleted&quot;,
         &quot;type&quot; : &quot;longSum&quot;,
         &quot;fieldName&quot; : &quot;deleted&quot;
       },
       {
         &quot;name&quot; : &quot;delta&quot;,
         &quot;type&quot; : &quot;longSum&quot;,
         &quot;fieldName&quot; : &quot;delta&quot;
       },
       {
         &quot;name&quot; : &quot;user_unique&quot;,
         &quot;type&quot; : &quot;hyperUnique&quot;,
         &quot;fieldName&quot; : &quot;user&quot;
       }
     ]
   },
   &quot;tuningConfig&quot; : {
     &quot;type&quot; : &quot;hadoop&quot;,
     &quot;partitionsSpec&quot; : {        &quot;type&quot; : &quot;hashed&quot;,
       &quot;targetPartitionSize&quot; : 5000000
     },
     &quot;jobProperties&quot; : {
       &quot;mapreduce.map.java.opts&quot;:&quot;-Duser.timezone=UTC -Dfile.encoding=UTF-8&quot;,
       &quot;mapreduce.reduce.java.opts&quot;:&quot;-Duser.timezone=UTC -Dfile.encoding=UTF-8&quot;,
       &quot;mapred.child.env&quot;:&quot;JAVA_HOME=/home/work/.jumbo/opt/sun-java8&quot;,
       &quot;yarn.app.mapreduce.am.env&quot;:&quot;JAVA_HOME=/home/work/.jumbo/opt/sun-java8&quot;,
       &quot;mapreduce.job.queuename&quot;:&quot;bigJob&quot;,
       &quot;mapreduce.job.classloader&quot;: &quot;true&quot;
       }
   }
 }
</code></pre><p>6ï¼‰ç¡®è®¤ä»»åŠ¡çœŸçš„å¯¼å…¥æˆåŠŸ</p>
<p>é€šè¿‡brokeræ¥å£æŸ¥è¯¢å¯¼å…¥çš„useræœ‰å¤šå°‘<br>curl -X POST â€˜host-175.bjyz:8082/druid/v2/?prettyâ€™ -H â€˜Content-Type:application/jsonâ€™ -d @query/useruniq.json</p>
<p>useruniq.jsonå†…å®¹ï¼š</p>
<pre><code>{
  &quot;queryType&quot;: &quot;timeseries&quot;,
  &quot;dataSource&quot;: &quot;wikiticker&quot;,
  &quot;granularity&quot;: &quot;day&quot;,
  &quot;aggregations&quot;: [
    { &quot;type&quot;: &quot;hyperUnique&quot;, &quot;name&quot;: &quot;user_unique&quot;, &quot;fieldName&quot;: &quot;user_unique&quot; }
  ],
  &quot;intervals&quot;: [ &quot;2015-09-12T00:00:00.000/2015-09-13T00:00:00.000&quot; ],
  &quot;context&quot; : {
    &quot;skipEmptyBuckets&quot;: &quot;true&quot;
  }
}
</code></pre><p>é€šè¿‡brokeræ¥å£æŸ¥è¯¢å¯¼å…¥çš„metaä¿¡æ¯ï¼š<br>curl -X POST â€˜host-175.bjyz:8082/druid/v2/?prettyâ€™ -H â€˜Content-Type:application/jsonâ€™ -d @query/metadata.json</p>
<pre><code>{
  &quot;queryType&quot;:&quot;segmentMetadata&quot;,
  &quot;dataSource&quot;:&quot;wikiticker&quot;,
  &quot;intervals&quot;:[&quot;2015-09-12/2015-09-13&quot;]
}
</code></pre><p>æˆ–è€…ä½¿ç”¨è‡ªå¸¦çš„æŸ¥è¯¢ä¸²<br>curl -X POST â€˜host-175.bjyz:8082/druid/v2/?prettyâ€™ -H â€˜Content-Type:application/jsonâ€™ -d @wikiticker-top-pages.jsonæŸ¥çœ‹è¢«ç¼–è¾‘æœ€å¤§çš„pages</p>
<p>2.stream pushæ–¹å¼ï¼ˆå‚è€ƒï¼š<a href="http://druid.io/docs/latest/ingestion/stream-push.htmlï¼‰" target="_blank" rel="external">http://druid.io/docs/latest/ingestion/stream-push.htmlï¼‰</a><br>stream pushä¸»è¦æ˜¯å€ŸåŠ©äº†tranquility,å…³äºtranquilityçš„ä»‹ç»ï¼š<a href="https://github.com/druid-io/tranquility/blob/master/docs/overview.md" target="_blank" rel="external">https://github.com/druid-io/tranquility/blob/master/docs/overview.md</a></p>
<p>tranquilityå¯¼å…¥æ•°æ®ä¸»è¦æœ‰å‡ ç§æ–¹å¼ï¼š</p>
<p>a)tranquility server (httpæ¥å£)<br>b)tranquility kafkaï¼ˆç”¨æˆ·å°†æ•°æ®æ¨å…¥kafkaï¼Œtranquilityå†™å…¥druidï¼‰<br>c)è‡ªå·±å†™ä¸€ä¸ªä¾èµ–tranquility libraryçš„JVM app<br>å‚è€ƒï¼š<a href="https://github.com/druid-io/tranquility/blob/master/docs/core.md" target="_blank" rel="external">https://github.com/druid-io/tranquility/blob/master/docs/core.md</a></p>
<p>d)åˆ©ç”¨tranquilityé‡Œé¢å®ç°çš„å„ç§æµè¿æ¥å™¨ï¼Œæ¯”å¦‚sparkå¦‚ä½•å†™å…¥druid:<br><a href="https://github.com/druid-io/tranquility/blob/master/docs/spark.md" target="_blank" rel="external">https://github.com/druid-io/tranquility/blob/master/docs/spark.md</a></p>
<p>å…¶ä¸­a)b)æ–¹æ¡ˆä¾èµ–ä¸€å®šç¬¬ä¸‰æ–¹æœåŠ¡ã€‚c)d)åªä¾èµ–tranquilityçš„library</p>
<p>é’ˆå¯¹sparkä¸­çš„è®¡ç®—ç»“æœå¦‚ä½•å†™å…¥druidï¼Œä¼šå¦å¤–å¼€ä¸€ç¯‡æ–‡ç« ä¸“é—¨è®¨è®º</p>
<p>3.stream pullæ–¹å¼</p>
<p>è¿™ç§æ–¹å¼éœ€è¦ç”¨åˆ°realtime nodeã€‚è²Œä¼¼ä¸æ¨èï¼Œè¿™é‡Œä¸å¤šç ”ç©¶</p>
<p>å››.druid&amp;caravel</p>
<p>å½“åœ¨druidå­˜å‚¨äº†æ•°æ®åï¼Œæˆ‘ä»¬ä½¿ç”¨caravelé¡µé¢è¿›è¡Œå±•ç¤º</p>
<p>1ï¼‰add druid cluster</p>
<p>é…ç½®ä»¥ä¸‹coordinatorä»¥åŠbrokerçš„åœ°å€å³å¯<br>é…ç½®å®Œæˆä¿å­˜årefreshä¸€ä¸‹druidå…ƒæ•°æ®</p>
<p>ç„¶åç‚¹å‡»è¿›å…¥datasourceå°±å¯ä»¥æ„‰å¿«åœ°olapäº†ã€‚å¦‚æœæ²¡æœ‰æ•°æ®æ£€æŸ¥ä¸‹æ•°æ®çš„èµ·å§‹æ—¶é—´ã€‚(ä¾‹å¦‚ä¾‹å­ä¸­å¯¼å…¥çš„2015å¹´æ•°æ®ï¼Œéœ€è¦é€‰æ‹©4 years ago)</p>
<p>2ï¼‰é…ç½®æŠ¥è¡¨</p>
<p>åœ¨datasourceè§†å›¾ä¸­é€‰æ‹©åˆ†ç»„ï¼Œåº¦é‡æŸ¥è¯¢ä¹‹åï¼Œå°†ç»“æœä¿å­˜æˆsliceï¼Œç‚¹å‡»æŠ¥è¡¨æ ‡ç­¾é¡µé¢å¯ä»¥çœ‹åˆ°åˆšæ‰ä¿å­˜çš„sliceï¼Œé€‰æ‹©ä»ªè¡¨ç›˜é¡µé¢æ–°å»ºä»ªè¡¨ç›˜ï¼ŒæŠ¥è¡¨å°±é€‰æ‹©åˆšæ‰ä¿å­˜çš„sliceã€‚ã€‚ã€‚</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/29/druidå®‰è£…é…ç½®-æ•°æ®å¯¼å…¥/" data-id="ckaafjsfi0000ci3c49hbilpd" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-sparkè¿œç¨‹è°ƒè¯•" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/02/sparkè¿œç¨‹è°ƒè¯•/" class="article-date">
  <time datetime="2018-03-02T07:12:34.000Z" itemprop="datePublished">2018-03-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/æºç è°ƒè¯•/">æºç è°ƒè¯•</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/02/sparkè¿œç¨‹è°ƒè¯•/">sparkè¿œç¨‹è°ƒè¯•</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      <span id="busuanzi_container_page_pv">
        <br /><br /> é˜…è¯»æ¬¡æ•°<span id="busuanzi_value_page_pv"></span>
        </span>
      
        <p>##sparkè¿œç¨‹è°ƒè¯•</p>
<p>æ·±å…¥äº†è§£sparkå°‘ä¸äº†æ”¹å˜ä»£ç è¿›è¡Œè°ƒè¯•ï¼Œå› æ­¤ç¼–è¯‘è°ƒè¯•æ˜¯å¿…å¤‡æŠ€èƒ½ï¼š</p>
<p>###é¦–å…ˆè¯´ä¸€ä¸‹ç¼–è¯‘ï¼š</p>
<p>å®Œæ•´ç¼–è¯‘ï¼š</p>
<pre><code>./build/sbt -Pyarn -Phive -Phive-thriftserver -Dhadoop.version=2.7.3 -DskipTests clean package
</code></pre><p>å¢é‡ç¼–è¯‘ï¼š</p>
<pre><code>./build/sbt -Pyarn -Phive -Phive-thriftserver -DskipTests  ~package
export SPARK_PREPEND_CLASSES=1
</code></pre><p>å¦‚æœä½¿ç”¨mavené»˜è®¤å°±æ˜¯å¢é‡ç¼–è¯‘</p>
<p>###ideaè¿œç¨‹è°ƒè¯•</p>
<p>å…ˆå†³æ¡ä»¶ï¼šæ‹¥æœ‰ä¸€ä¸ªåœ¨ideaèƒ½å¤Ÿå®Œæ•´ç¼–è¯‘é€šè¿‡çš„sparké¡¹ç›®</p>
<p>####1.åœ¨ideaä¸­run-&gt;edit configurationsè®¾ç½®remote-&gt;remotedebugå‚æ•°</p>
<p>å…¶ä¸­debugger modeé€‰æ‹©attachï¼Œç„¶åè®¾ç½®è¦é“¾æ¥çš„hostä»¥åŠportï¼ˆä¹Ÿå°±æ˜¯sparkè¿›ç¨‹å¯åŠ¨çš„æœºå™¨ä»¥åŠç›‘å¬ç«¯å£ï¼‰</p>
<p>####2.åœ¨è¿œç¨‹hostä¸Šå¯åŠ¨sparkç¨‹åºï¼Œè®¾ç½®jvmå‚æ•°</p>
<p>â€“driver-java-options  â€œ -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8888â€</p>
<p>æ¯”å¦‚ä¸€ä¸ªæäº¤åˆ°yarné›†ç¾¤çš„ä¾‹å­ï¼š</p>
<p>/home/work/dataplatform/spark-2.0.2/bin/spark-submit \<br>  â€“master yarn\<br>  â€“deploy-mode client\<br>  â€“class com.baidu.pcsdata.message.value.analysis.MsgFastCategory \<br>  â€“driver-cores 8 \<br>  â€“executor-cores 2\<br>  â€“num-executors 10 \<br>  â€“driver-memory 5G\<br>  â€“principal  horus/pcsdata@PCSDATA.COM\<br>  â€“keytab  /home/work/chenxiue/horus.keytab\<br>  â€“executor-memory 6G\<br>  â€“driver-java-options  â€œ -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8888â€\<br>  /home/work/chenxiue/spark/msg-mining/pcs_fsg-assembly-1.0.jar $inputpath $outputpath $modelpath</p>
<p>####3.åœ¨ideaä¸Šé¢è®¾ç½®å¥½æ–­ç‚¹åï¼ˆæŠŠé¼ æ ‡æ”¾åœ¨éœ€è¦æ–­çš„æŸè¡Œä¸Šé¢ï¼Œç„¶åå·¦å‡»å‡ºç°çº¢ç‚¹ï¼‰è¿è¡Œ<br>run-&gt;debug â€œremote debugâ€</p>
<p>å¥½äº†ï¼Œç°åœ¨å°±èƒ½å¤Ÿæ„‰å¿«åœ°ä¿®æ”¹ä»£ç å¹¶ä¸”ç¼–è¯‘è°ƒè¯•äº†ï½</p>
<p>###å‚è€ƒï¼š<br><a href="https://segmentfault.com/a/1190000008867470" target="_blank" rel="external">https://segmentfault.com/a/1190000008867470</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/02/sparkè¿œç¨‹è°ƒè¯•/" data-id="ckaafjsgn0005ci3cp844pcrt" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-åŸåˆ›ï¼flumeå†™hdfsæ€§èƒ½ä¼˜åŒ–" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/11/åŸåˆ›ï¼flumeå†™hdfsæ€§èƒ½ä¼˜åŒ–/" class="article-date">
  <time datetime="2018-01-11T03:43:57.000Z" itemprop="datePublished">2018-01-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/æ€§èƒ½ä¼˜åŒ–/">æ€§èƒ½ä¼˜åŒ–</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/11/åŸåˆ›ï¼flumeå†™hdfsæ€§èƒ½ä¼˜åŒ–/">åŸåˆ›-flumeå†™hdfsæ€§èƒ½ä¼˜åŒ–</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      <span id="busuanzi_container_page_pv">
        <br /><br /> é˜…è¯»æ¬¡æ•°<span id="busuanzi_value_page_pv"></span>
        </span>
      
        <p>æ ‡ç­¾ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰ï¼š æœªåˆ†ç±»</p>
<hr>
<p>flumeåœ¨å†™hdfsçš„æ—¶å€™ï¼Œæ¯æ¥æ”¶åˆ°ä¸€ä¸ªeventå°±ä¼šè°ƒç”¨bucketwriter.javaçš„append(event)æ–¹æ³•ï¼Œåœ¨appendå½“ä¸­æ¯æ¬¡éƒ½ä¼šå»æ£€æŸ¥shouldrotate,æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹è¿™å—çš„é€»è¾‘ï¼š</p>
<pre><code>private boolean shouldRotate() {
    boolean doRotate = false;

    if (writer.isUnderReplicated()) {
      this.isUnderReplicated = true;
      doRotate = true;
    } else {
      this.isUnderReplicated = false;
    }

    if ((rollCount &gt; 0) &amp;&amp; (rollCount &lt;= eventCounter)) {
      LOG.debug(&quot;rolling: rollCount: {}, events: {}&quot;, rollCount, eventCounter);
      doRotate = true;
    }

    if ((rollSize &gt; 0) &amp;&amp; (rollSize &lt;= processSize)) {
      LOG.debug(&quot;rolling: rollSize: {}, bytes: {}&quot;, rollSize, processSize);
      doRotate = true;
    }

    return doRotate;
  }
</code></pre><p>æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™é‡Œæ¯æ¬¡éƒ½ä¼šè°ƒç”¨writer.isUnderReplicated()ï¼Œå¦‚æœå‰¯æœ¬æ•°ä½äºé¢„æœŸï¼Œé‚£ä¹ˆä¸ç®¡æœ‰æ²¡æœ‰è¾¾åˆ°rollcountæˆ–è€…rollsizeå°±rotateï¼Ÿï¼Ÿï¼ˆè¿™å—å¯èƒ½æ˜¯è¯´ä¸Šä¸€ä¸ªeventå¦‚æœå‰¯æœ¬æ•°æ²¡æœ‰è¾¾åˆ°é¢„æœŸï¼Œé‚£ä¹ˆå°±éœ€è¦é‡æ–°å¼€ä¸€ä¸ªæ–°æ–‡ä»¶å»å†™å…¥ï¼Œä¸ºå•¥è¦è¿™æ ·è®¾è®¡ï¼Ÿä¸å¤ªæ¸…æ¥šã€‚ã€‚ã€‚ï¼‰</p>
<pre><code>public boolean isUnderReplicated() {
    try {
      int numBlocks = getNumCurrentReplicas();
      if (numBlocks == -1) {
        return false;
      }
      int desiredBlocks;
      if (configuredMinReplicas != null) {
        desiredBlocks = configuredMinReplicas;
      } else {
        desiredBlocks = getFsDesiredReplication();
      }
      return numBlocks &lt; desiredBlocks;
    } catch (IllegalAccessException e) {
      logger.error(&quot;Unexpected error while checking replication factor&quot;, e);
    } catch (InvocationTargetException e) {
      logger.error(&quot;Unexpected error while checking replication factor&quot;, e);
    } catch (IllegalArgumentException e) {
      logger.error(&quot;Unexpected error while checking replication factor&quot;, e);
    }
    return false;
  }
</code></pre><p>å‰¯æœ¬è·å–çš„ä»£ç å¦‚ä¸‹ï¼š</p>
<pre><code>/**
   * This method gets the datanode replication count for the current open file.
   *
   * If the pipeline isn&apos;t started yet or is empty, you will get the default
   * replication factor.
   *
   * &lt;p/&gt;If this function returns -1, it means you
   * are not properly running with the HDFS-826 patch.
   * @throws InvocationTargetException
   * @throws IllegalAccessException
   * @throws IllegalArgumentException
   */
  public int getNumCurrentReplicas()
      throws IllegalArgumentException, IllegalAccessException,
          InvocationTargetException {
    if (refGetNumCurrentReplicas != null &amp;&amp; outputStream != null) {
      OutputStream dfsOutputStream = outputStream.getWrappedStream();
      if (dfsOutputStream != null) {
        Object repl = refGetNumCurrentReplicas.invoke(dfsOutputStream, NO_ARGS);
        if (repl instanceof Integer) {
          return ((Integer)repl).intValue();
        }
      }
    }
    return -1;
  }
</code></pre><p>æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¯ä¸€æ¬¡eventå†™å…¥éƒ½è¦å»å’Œhdfsäº¤äº’ä¸€æ¬¡ï¼Œè¿™ä¸ªä»£ä»·éå¸¸é«˜ã€‚è€Œå®é™…ä¸Šåœ¨æˆ‘ä»¬çš„å·¥ä½œå½“ä¸­æˆ‘ä»¬å¹¶æ²¡æœ‰ç”¨rollcountå’Œrollsizeï¼Œè€Œæ˜¯ä½¿ç”¨äº†rollintervalï¼Œrollintervalçš„é€»è¾‘å¹¶ä¸åœ¨è¿™é‡Œ,åœ¨open()é‡Œé¢æœ‰è¿™ä¹ˆä¸€æ®µé€»è¾‘ï¼š</p>
<pre><code>if (rollInterval &gt; 0) {
      Callable&lt;Void&gt; action = new Callable&lt;Void&gt;() {
        public Void call() throws Exception {
          LOG.debug(&quot;Rolling file ({}): Roll scheduled after {} sec elapsed.&quot;,
              bucketPath, rollInterval);
          try {
            // Roll the file and remove reference from sfWriters map.
            close(true);
          } catch(Throwable t) {
            LOG.error(&quot;Unexpected error&quot;, t);
          }
          return null;
        }
      };
      timedRollFuture = timedRollerPool.schedule(action, rollInterval,TimeUnit.SECONDS);
</code></pre><p>æˆ‘ä»¬å›è¿‡å¤´æ¥çœ‹ä¸€ä¸‹hdfseventsinkç±»çš„processæ–¹æ³•ï¼š</p>
<pre><code>public Status process() throws EventDeliveryException {
    Channel channel = getChannel();
    Transaction transaction = channel.getTransaction();
    List&lt;BucketWriter&gt; writers = Lists.newArrayList();
    transaction.begin();
    try {
      int txnEventCount = 0;
      for (txnEventCount = 0; txnEventCount &lt; batchSize; txnEventCount++) {
        Event event = channel.take();
        if (event == null) {
          break;
        }

        // reconstruct the path name by substituting place holders
        String realPath = BucketPath.escapeString(filePath, event.getHeaders(),
            timeZone, needRounding, roundUnit, roundValue, useLocalTime);
        String realName = BucketPath.escapeString(fileName, event.getHeaders(),
          timeZone, needRounding, roundUnit, roundValue, useLocalTime);

        String lookupPath = realPath + DIRECTORY_DELIMITER + realName;
        BucketWriter bucketWriter;
        HDFSWriter hdfsWriter = null;
        // Callback to remove the reference to the bucket writer from the
        // sfWriters map so that all buffers used by the HDFS file
        // handles are garbage collected.
        WriterCallback closeCallback = new WriterCallback() {
          @Override
          public void run(String bucketPath) {
            LOG.info(&quot;Writer callback called.&quot;);
            synchronized (sfWritersLock) {
              sfWriters.remove(bucketPath);
            }
          }
        };
        synchronized (sfWritersLock) {
          bucketWriter = sfWriters.get(lookupPath);
          // we haven&apos;t seen this file yet, so open it and cache the handle
          if (bucketWriter == null) {
            hdfsWriter = writerFactory.getWriter(fileType);
            bucketWriter = initializeBucketWriter(realPath, realName,
              lookupPath, hdfsWriter, closeCallback);
            sfWriters.put(lookupPath, bucketWriter);
          }
        }

        // track the buckets getting written in this transaction
        if (!writers.contains(bucketWriter)) {
          writers.add(bucketWriter);
        }

        // Write the data to HDFS
        try {
          bucketWriter.append(event);
        } catch (BucketClosedException ex) {
          LOG.info(&quot;Bucket was closed while trying to append, &quot; +
            &quot;reinitializing bucket and writing event.&quot;);
          hdfsWriter = writerFactory.getWriter(fileType);
          bucketWriter = initializeBucketWriter(realPath, realName,
            lookupPath, hdfsWriter, closeCallback);
          synchronized (sfWritersLock) {
            sfWriters.put(lookupPath, bucketWriter);
          }
          bucketWriter.append(event);
        }
      }

      if (txnEventCount == 0) {
        sinkCounter.incrementBatchEmptyCount();
      } else if (txnEventCount == batchSize) {
        sinkCounter.incrementBatchCompleteCount();
      } else {
        sinkCounter.incrementBatchUnderflowCount();
      }

      // flush all pending buckets before committing the transaction
      for (BucketWriter bucketWriter : writers) {
        bucketWriter.flush();
      }

      transaction.commit();

      if (txnEventCount &lt; 1) {
        return Status.BACKOFF;
      } else {
        sinkCounter.addToEventDrainSuccessCount(txnEventCount);
        return Status.READY;
      }
    } catch (IOException eIO) {
      transaction.rollback();
      LOG.warn(&quot;HDFS IO error&quot;, eIO);
      return Status.BACKOFF;
    } catch (Throwable th) {
      transaction.rollback();
      LOG.error(&quot;process failed&quot;, th);
      if (th instanceof Error) {
        throw (Error) th;
      } else {
        throw new EventDeliveryException(th);
      }
    } finally {
      transaction.close();
    }
  }
</code></pre><p>hdfseventsinkå½“ä¸­ç»´æŠ¤äº†map<lookuppath,bucketwriter>çš„sfwriteså“ˆå¸Œè¡¨ï¼Œæ¯æ¬¡eventæ¥çš„æ—¶å€™ï¼Œä¼šå»sfwriterså½“ä¸­å¯»æ‰¾å¯¹åº”path(æ ¹æ®hdfs.pathä»¥åŠhdfs.fileprefixæ¥å†³å®š)çš„bucketwriterä½œä¸ºå½“å‰å†™å…¥çš„handle.å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°±åˆ›å»ºä¸€ä¸ªï¼Œå¹¶ä¸”å°†æ–°çš„å¥ç‚³åŠ å…¥åˆ°sfwriterså½“ä¸­å»ã€‚å› æ­¤å®é™…ä¸Šå¯¹äºä¸Šè¿°bucketwriterä¸­çš„rotateæ¥è¯´ï¼Œåªæ˜¯é’ˆå¯¹åŒä¸€ä¸ªè·¯å¾„å†…éƒ¨çš„ï¼Œå¦‚æœæ²¡æœ‰ä¸€ä¸ªè·¯å¾„å†…rollsizeå’Œrollcountæ»šåŠ¨çš„éœ€æ±‚ï¼Œä»¥åŠæ•°æ®ä¸€å®šè¦å¤šå‰¯æœ¬å†™å…¥çš„éœ€æ±‚ï¼Œå®Œå…¨å¯ä»¥æŠŠbucketwriterå†…éƒ¨åˆ¤æ–­rotateçš„é€»è¾‘æ³¨é‡Šæ‰ï¼Œè¿™æ ·èƒ½å¤Ÿå¤§å¤§æå‡flumeå†™å…¥èƒ½åŠ›.</lookuppath,bucketwriter></p>
<p>agent.sinks.sk_cloudui.hdfs.path = hdfs://bigfile/om/anticrack/cloudui.log/%Y%m%d/%k%M/<br>agent.sinks.sk_cloudui.hdfs.filePrefix = qd01-pcsdata45.qd01.baidu.com</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/01/11/åŸåˆ›ï¼flumeå†™hdfsæ€§èƒ½ä¼˜åŒ–/" data-id="ckaafjsgw0009ci3ciohs8zo5" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-spark-on-yarnæƒ…å†µä¸‹historyserverçš„é…ç½®" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/19/spark-on-yarnæƒ…å†µä¸‹historyserverçš„é…ç½®/" class="article-date">
  <time datetime="2017-04-19T09:39:44.000Z" itemprop="datePublished">2017-04-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/éƒ¨ç½²æ–‡æ¡£/">éƒ¨ç½²æ–‡æ¡£</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/19/spark-on-yarnæƒ…å†µä¸‹historyserverçš„é…ç½®/">åŸåˆ›-spark-on-yarnæƒ…å†µä¸‹historyserverçš„é…ç½®</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      <span id="busuanzi_container_page_pv">
        <br /><br /> é˜…è¯»æ¬¡æ•°<span id="busuanzi_value_page_pv"></span>
        </span>
      
        <p>æ ‡ç­¾ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰ï¼š æœªåˆ†ç±»</p>
<hr>
<p>æˆ‘ä»¬ç»å¸¸éœ€è¦åœ¨ä¸€ä¸ªappæ‰§è¡Œå®Œæˆä¹‹åï¼Œå»å¯¹è¿™ä¸ªappçš„æ‰§è¡Œæƒ…å†µè¿›è¡Œåˆ†æï¼Œä¸ç®¡æ˜¯åˆ†æå®ƒå¦‚ä½•å¤±è´¥ä¹Ÿå¥½ï¼Œæˆ–è€…æ˜¯åˆ†æè¿™ä¸ªä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹æ˜¯å¦éœ€è¦ä¼˜åŒ–ã€‚é‚£ä¹ˆè¿™æ—¶å€™æˆ‘ä»¬å°±éœ€è¦ç”¨åˆ°historyserver.</p>
<p>é¦–å…ˆlogåˆ†ä¸ºä¸¤ç§ï¼š1)æ ‡å‡†è¾“å…¥è¾“å‡ºçš„log 2ï¼‰spark event log<br>å¯¹åº”yarné¡µé¢ä¸Šçš„ä¸¤ä¸ªæŒ‰é’®ï¼š1)logs   2)history</p>
<p>###logsé…ç½®<br>å¯¹äºyarnæ¥è¯´ï¼Œlogsè¿™å—éœ€è¦é…ç½®ï¼š(æ‰€æœ‰nodemanagerçš„æœºå™¨ä¸Šéƒ½éœ€è¦ä¿®æ”¹è¯¥é…ç½®)</p>
<pre><code>&lt;property&gt;
  &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt; 
  &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;
  &lt;value&gt;259200&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;yarn.log.server.url&lt;/name&gt;
    &lt;value&gt;http://hostA:8937/jobhistory/logs/&lt;/value&gt;ï¼ˆyarnä¼šåœ¨ç”¨æˆ·ç‚¹å‡»å†å²ä»»åŠ¡logsçš„æ—¶å€™è·³è½¬åˆ°è¿™ä¸ªurl,è¿™ä¸ªurlæä¾›çš„jobhistory serveræ˜¯mapreduceçš„åŠŸèƒ½ï¼‰
  &lt;/property&gt;

 &lt;property&gt; 
  &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt;
  &lt;value&gt;/tmp/logs/yarn/&lt;/value&gt;(yarnä¼šè´Ÿè´£ä»»åŠ¡ç»“æŸåå°†åœ°å€è½¬å­˜åˆ°è¿™ä¸ªä½ç½®)
  &lt;/property&gt;
</code></pre><p>åœ¨hostAä¸Šä¿®æ”¹mapred-site.xmlé…ç½®ï¼š<br>å¯¹äºmapred-site.xmlæ¥è¯´éœ€è¦é…ç½®ï¼š(./sbin/mr-jobhistory-daemon.sh start historyserver)</p>
<pre><code>&lt;property&gt;
&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
&lt;value&gt;hostA:8927&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
&lt;value&gt;hostB:8937&lt;/value&gt;
&lt;/property&gt;
</code></pre><p>(historyserveræŒ‚æ‰ï¼Œæ£€æŸ¥ç›¸å…³çš„å‡ ä¸ªç›®å½•æ˜¯ä¸æ˜¯æ»¡äº†)</p>
<p>åœ¨hostAä¸Šæ‰§è¡Œ$HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserverå¯åŠ¨historyserverã€‚</p>
<p>è¿™ä¸ªæ­¥éª¤å®Œæˆååœ¨yarné¡µé¢ä¸Šç‚¹å‡»appçš„logsæŒ‰é’®å°±å¯ä»¥çœ‹åˆ°è¯¥appçš„æ—¥å¿—ï¼Œä¸ç®¡è¿™ä¸ªappæ‰§è¡Œå®Œæˆä¸å¦ã€‚</p>
<p>###event logé…ç½®<br>å¯¹äºhistoryæ¥è¯´ã€‚yarnä¼šè´Ÿè´£è·³è½¬åˆ°appè‡ªèº«æŒ‡å®šçš„history serverå½“ä¸­å»ã€‚(åœ¨hostBæœºå™¨ä¸Š$SPARK_HOME/sbin/start-history-server.shå¯åŠ¨historyserver)</p>
<pre><code>spark.eventLog.enabled           true
spark.eventLog.dir  hdfs://hostNamenode:8900/spark-event-2.0
spark.yarn.historyServer.address hostB:8651
spark.history.fs.logDirectory hdfs://hostNamenode:8900/spark-event-2.0
spark.history.retainedApplications 1000
spark.history.ui.port 8651
spark.history.fs.cleaner.enabled true
spark.history.fs.cleaner.interval 1d
spark.history.fs.cleaner.maxAge 3d
spark.executor.extraJavaOptions  -XX:+PrintGCDetails -XX:+PrintGCTimeStamps
</code></pre><p>è¿™ä¸ªå®Œæˆåï¼Œå°±å¯ä»¥é€šè¿‡historyæŒ‰é’®çœ‹åˆ°spark event logï¼Œä¸ç®¡è¯¥appæ‰§è¡Œå®Œæˆä¸å¦ã€‚</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/19/spark-on-yarnæƒ…å†µä¸‹historyserverçš„é…ç½®/" data-id="ckaafjsh5000aci3cva05g30o" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-sparkä¸­parquetæ–‡ä»¶å†™å…¥ä¼˜åŒ–" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/13/sparkä¸­parquetæ–‡ä»¶å†™å…¥ä¼˜åŒ–/" class="article-date">
  <time datetime="2017-04-13T09:22:34.000Z" itemprop="datePublished">2017-04-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/æ€§èƒ½ä¼˜åŒ–/">æ€§èƒ½ä¼˜åŒ–</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/13/sparkä¸­parquetæ–‡ä»¶å†™å…¥ä¼˜åŒ–/">åŸåˆ›-sparkä¸­parquetæ–‡ä»¶å†™å…¥ä¼˜åŒ–</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      <span id="busuanzi_container_page_pv">
        <br /><br /> é˜…è¯»æ¬¡æ•°<span id="busuanzi_value_page_pv"></span>
        </span>
      
        <p>æ ‡ç­¾ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰ï¼š æ€§èƒ½ä¼˜åŒ–</p>
<hr>
<p>åœ¨æˆ‘ä»¬çš„å®é™…ä½¿ç”¨ä¸­ï¼Œç»å¸¸éœ€è¦å°†åŸå§‹çš„æ–‡æœ¬æ–‡ä»¶è½¬æ¢ä¸ºparquetåˆ—å­˜å‚¨æ ¼å¼ï¼Œä»¥ä¾¿åç»­æŸ¥è¯¢çš„æ—¶å€™ä½¿ç”¨ã€‚å†™parquetèƒ½æé«˜åç»­è¡¨æŸ¥è¯¢æ•ˆç‡è¿™ä¸ªäº‹æƒ…æˆ‘ä»¬ä¸å¤šè¯´ï¼Œä¸‹é¢è®¨è®ºä¸€ä¸‹å†™parquetæ–‡ä»¶çš„æ•ˆç‡é—®é¢˜ï¼š</p>
<p>æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ä¸¤æ®µç¨‹åºï¼š</p>
<p>1.ä½¿ç”¨case classä½œä¸ºdfè½¬æ¢</p>
<pre><code>package com.yundata.transtoparquet

import java.lang.Exception
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._
import org.apache.spark.sql._
import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.sql.Row

import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary,Statistics}
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel
import org.apache.spark.mllib.util.MLUtils

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

import org.apache.spark.mllib.tree.GradientBoostedTrees
import org.apache.spark.mllib.tree.configuration.BoostingStrategy
import org.apache.spark.mllib.tree.model.GradientBoostedTreesModel
import org.apache.spark.mllib.util.MLUtils

import org.apache.spark.rdd.PairRDDFunctions

import scala.collection.mutable.ArrayBuffer


import org.apache.spark.sql.types.{StructType,StructField,StringType};
import org.apache.spark.sql.Row;


case class formatrow(fs_id:Long,user_id:Long,app_id:Long,parent_path:String,server_filename:String,s3_handle:String,size:Long,server_mtime:String,server_ctime:Long,local_mtime:Long,local_ctime:Long,isdir:Long,isdelete:Long,status:Long,category:Long,object_key:String,extent_int2:Long,recompute_tag:Long,user_range:Long,md5_range:Long,event_day:String) extends Product;


/**
 * Created by robert on 15-5-18.
 */
class transtoparquet(userConfFile: String,sourceFile:String,destFile:String,event_day:String) extends Serializable{



  def run(): Unit = {

    transtoparquetconf(userConfFile);

    val conf=transtoparquetconf.getSparkConf();
    conf.setAppName(conf.get(&quot;spark.app.name&quot;, this.getClass.getName));

    val sc=new SparkContext(conf);
    val sqlContext = new org.apache.spark.sql.SQLContext(sc);
    import sqlContext.implicits._

    sc.hadoopConfiguration.addResource(&quot;hdfs-site.xml&quot;);
    sc.hadoopConfiguration.set(&quot;parquet.enable.summary-metadata&quot;, &quot;false&quot;)

    System.out.println(sc.hadoopConfiguration.get(&quot;dfs.ha.namenodes.bigfile&quot;));

    System.out.println(sc.hadoopConfiguration.toString);



    val txtfile = sc.textFile(sourceFile);

    txtfile.take(10).foreach(println);
    println(sourceFile);


    val txtdf=txtfile.filter(_.split(&quot;\t&quot;).size==18).map(
      x=&gt;{

            var s3_handle=x.split(&quot;\t&quot;)(5);
            var md5_range=Long2long(0);
            if(x.split(&quot;\t&quot;)(5)==&quot;&quot;)
              {
                s3_handle=(new scala.util.Random).nextInt(99).toString(); //å–0-99ä¹‹é—´çš„éšæœºæ•°ï¼Œä¿è¯å…¶æ•£åˆ—å¼€æ¥
                md5_range=s3_handle.toLong;
              }
            else if(x.split(&quot;\t&quot;)(5).size==32)
            {
                md5_range=((s3_handle.toLowerCase.substring(0, 24).toList.map(&quot;0123456789abcdef&quot;.indexOf(_)).map(BigInt(_)).reduceLeft( _ * 32 + _))%100).toLong
            }
            else
            {
                md5_range = Long2long(-1);
            }
            var user_range=(x.split(&quot;\t&quot;)(0)).toLong/100000000;


            if(user_range&gt;=100)
            {
                user_range=100;
            }

            if(md5_range == Long2long(-1))
              {
                null
              }


            else {
              formatrow((x.split(&quot;\t&quot;)(0)).toLong, x.split(&quot;\t&quot;)(1).toLong, x.split(&quot;\t&quot;)(2).toLong, x.split(&quot;\t&quot;)(3), x.split(&quot;\t&quot;)(4), s3_handle, (x.split(&quot;\t&quot;)(6)).toLong, x.split(&quot;\t&quot;)(7), (x.split(&quot;\t&quot;)(8)).toLong, (x.split(&quot;\t&quot;)(9)).toLong, (x.split(&quot;\t&quot;)(10)).toLong, (x.split(&quot;\t&quot;)(11)).toLong, (x.split(&quot;\t&quot;)(12)).toLong, (x.split(&quot;\t&quot;)(13)).toLong, (x.split(&quot;\t&quot;)(14)).toLong, x.split(&quot;\t&quot;)(15), (x.split(&quot;\t&quot;)(16)).toLong, (x.split(&quot;\t&quot;)(17)).toLong, user_range, md5_range, event_day);
            }
      }
    ).filter(_ != null).toDF();


    txtdf.repartition(5).write.mode(org.apache.spark.sql.SaveMode.Append).partitionBy(&quot;user_range&quot;,&quot;md5_range&quot;,&quot;event_day&quot;).parquet(destFile);

  }



}



object transtoparquet{

  def main(args: Array[String]): Unit = {
    if (args.size != 4) {
      println(&quot;usage: com.yundata.transtoparquet.transtoparquet config srcfile destfile event_day&quot;)
      return
    }
    new transtoparquet(args(0),args(1),args(2),args(3)).run()
  }
  }
</code></pre><p>è¿™ä¸ªç¨‹åºå®é™…æµ‹è¯•ï¼ŒåŸå§‹æ•°æ®9Gï¼Œå‹ç¼©åå¤§æ¦‚æ˜¯5.5Gå·¦å³ï¼Œä½¿ç”¨50ä¸ªæ ¸è·‘äº†å¥½å‡ ä¸ªå°æ—¶ï¼Œå±…ç„¶éƒ½æ²¡æœ‰å†™å®Œæ•°æ®ï¼Œçœ‹executoræ—¥å¿—ï¼Œå‡ Bå‡ Båœ°åœ¨å¾€hdfså½“ä¸­å»å†™æ—¥å¿—ã€‚å´©æºƒï¼Œäºæ˜¯ï¼Œæ¢äº†ä¸€ç§å†™æ³•ã€‚</p>
<p>2.ä½¿ç”¨rowä½œä¸ºdfè½¬æ¢</p>
<pre><code>package com.yundata.transtoparquet


import java.lang.Exception
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._
import org.apache.spark.sql._
import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.sql.Row

import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary,Statistics}
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel
import org.apache.spark.mllib.util.MLUtils

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

import org.apache.spark.mllib.tree.GradientBoostedTrees
import org.apache.spark.mllib.tree.configuration.BoostingStrategy
import org.apache.spark.mllib.tree.model.GradientBoostedTreesModel
import org.apache.spark.mllib.util.MLUtils

import org.apache.spark.rdd.PairRDDFunctions

import scala.collection.mutable.ArrayBuffer


import org.apache.spark.sql.types.{StructType,StructField,StringType,LongType};
import org.apache.spark.sql.Row;

/**
 * Created by robert on 15-5-18.
 */
class transtoparquet(userConfFile: String,sourceFile:String,destFile:String,event_day:String) extends Serializable{

  def run(): Unit = {

    transtoparquetconf(userConfFile);

    val conf=transtoparquetconf.getSparkConf();
    conf.setAppName(conf.get(&quot;spark.app.name&quot;, this.getClass.getName));

    val sc=new SparkContext(conf);
    val sqlContext = new org.apache.spark.sql.SQLContext(sc);
    import sqlContext.implicits._

    sc.hadoopConfiguration.addResource(&quot;hdfs-site.xml&quot;);
    sc.hadoopConfiguration.set(&quot;parquet.enable.summary-metadata&quot;, &quot;false&quot;)

    System.out.println(sc.hadoopConfiguration.get(&quot;dfs.ha.namenodes.bigfile&quot;));

    System.out.println(sc.hadoopConfiguration.toString);



    val txtfile = sc.textFile(sourceFile);

    txtfile.take(10).foreach(println);
    println(sourceFile);

    val schema=StructType(Array(StructField(&quot;fs_id&quot;,LongType,true), StructField(&quot;user_id&quot;,LongType,true),StructField(&quot;app_id&quot;,LongType,true),StructField(&quot;parent_path&quot;,StringType,true),StructField(&quot;server_filename&quot;,StringType,true),StructField(&quot;s3_handle&quot;,StringType,true),StructField(&quot;size&quot;,LongType,true),StructField(&quot;server_mtime&quot;,StringType,true),StructField(&quot;server_ctime&quot;,LongType,true),StructField(&quot;local_mtime&quot;,LongType,true),StructField(&quot;local_ctime&quot;,LongType,true),StructField(&quot;isdir&quot;,LongType,true),StructField(&quot;isdelete&quot;,LongType,true),StructField(&quot;status&quot;,LongType,true),StructField(&quot;category&quot;,LongType,true),StructField(&quot;object_key&quot;,StringType,true),StructField(&quot;extent_int2&quot;,LongType,true),StructField(&quot;recompute_tag&quot;,LongType,true),StructField(&quot;user_range&quot;,LongType,true),StructField(&quot;md5_range&quot;,LongType,true),StructField(&quot;event_day&quot;,StringType,true)));

    val txtrow=txtfile.filter(_.split(&quot;\t&quot;).size==18).map(
      x=&gt;{

            var s3_handle=x.split(&quot;\t&quot;)(5);
            var md5_range=Long2long(0);
            if(x.split(&quot;\t&quot;)(5)==&quot;&quot;)
              {
                s3_handle=(new scala.util.Random).nextInt(99).toString(); //å–0-99ä¹‹é—´çš„éšæœºæ•°ï¼Œä¿è¯å…¶æ•£åˆ—å¼€æ¥
                md5_range=s3_handle.toLong;
              }
            else if(x.split(&quot;\t&quot;)(5).size==32)
            {
                md5_range=((s3_handle.toLowerCase.substring(0, 24).toList.map(&quot;0123456789abcdef&quot;.indexOf(_)).map(BigInt(_)).reduceLeft( _ * 32 + _))%100).toLong
            }
            else
            {
                md5_range = Long2long(-1);
            }
            var user_range=(x.split(&quot;\t&quot;)(0)).toLong/100000000;


            if(user_range&gt;=100)
            {
                user_range=100;
            }

            if(md5_range == Long2long(-1))
              {
                null
              }
            else {
              Row((x.split(&quot;\t&quot;)(0)).toLong, x.split(&quot;\t&quot;)(1).toLong, x.split(&quot;\t&quot;)(2).toLong, x.split(&quot;\t&quot;)(3), x.split(&quot;\t&quot;)(4), s3_handle, (x.split(&quot;\t&quot;)(6)).toLong, x.split(&quot;\t&quot;)(7), (x.split(&quot;\t&quot;)(8)).toLong, (x.split(&quot;\t&quot;)(9)).toLong, (x.split(&quot;\t&quot;)(10)).toLong, (x.split(&quot;\t&quot;)(11)).toLong, (x.split(&quot;\t&quot;)(12)).toLong, (x.split(&quot;\t&quot;)(13)).toLong, (x.split(&quot;\t&quot;)(14)).toLong, x.split(&quot;\t&quot;)(15), (x.split(&quot;\t&quot;)(16)).toLong, (x.split(&quot;\t&quot;)(17)).toLong, user_range, md5_range, event_day);
            }
      }
    ).filter(_ != null);




    val txtdf=sqlContext.createDataFrame(txtrow, schema);

    txtdf.repartition(5).write.mode(org.apache.spark.sql.SaveMode.Append).partitionBy(&quot;user_range&quot;,&quot;md5_range&quot;,&quot;event_day&quot;).parquet(destFile);
  }
}



object transtoparquet{

  def main(args: Array[String]): Unit = {

    if (args.size != 4) {
      println(&quot;usage: com.yundata.transtoparquet.transtoparquet config srcfile destfile event_day&quot;)
      return
    }
    new transtoparquet(args(0),args(1),args(2),args(3)).run()
  }
</code></pre><p>   }</p>
<p>ä½¿ç”¨äº†è¿™ä¸ªç¨‹åºä¹‹åï¼Œå¾ˆå¿«ï¼Œ18åˆ†é’Ÿï¼Œå°±å°†5G parquetæ•°æ®å…¨éƒ¨å†™å…¥äº†ã€‚</p>
<p>åœ¨ä¸Šè¿°ç¨‹åºä¸­ï¼Œæˆ‘ä½¿ç”¨çš„é…ç½®æ˜¯ï¼š</p>
<pre><code>spark.app.name = TRANSTOPARQUET-JOB
spark.master = spark://sparkmaster:8650
spark.cores.max=50
spark.executor.instances=50
spark.executor.memory=2g
spark.speculation=true
spark.driver.maxResultSize=2g
spark.ui.port=8221
spark.ui.retainedStages=20
spark.ui.retainedJobs=20
spark.sql.parquet.compression.codec=snappy
</code></pre><p>ä½†æ˜¯è¿™ä¸ªç¨‹åºè¿˜æœ‰ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œå¦‚æœrepartitionä¸è®¾ç½®çš„è¯ï¼Œæœ€åå†™å…¥çš„æ–‡ä»¶æ•°ä¼šéå¸¸å¤šï¼Œå¤§æ¦‚æ˜¯num(user_range)<em>num(md5_range)</em>num(event_day)*num(repartition)ï¼Œå¾ˆå¯èƒ½ä¼šç¬é—´æ‰“çˆ†namenodeçš„å†…å­˜ã€‚å› æ­¤repartitionè¦è®¾ç½®å¾—éå¸¸å°ï¼Œè¿™åˆå¯¼è‡´äº†æ•´ä¸ªç¨‹åºä¼šéå¸¸æ…¢ã€‚</p>
<p>3)ä¼˜åŒ–ä¸€ä¸‹repartitionçš„æ–¹å¼</p>
<p>åŸå…ˆä½¿ç”¨repartiton(5)çš„æ–¹å¼çš„æ—¶å€™ï¼Œæ˜¯éšæœºåˆ†åŒºï¼Œå¯¼è‡´æ‰€æœ‰çš„taskéƒ½åŸºæœ¬å¯èƒ½æœ‰æ¯ä¸€ä¸ªåˆ†åŒºçš„æ•°æ®ï¼Œæ‰€ä»¥å¯¼è‡´æ¯ä¸ªåˆ†åŒºä¸‹é¢éƒ½æœ‰5ä¸ªæ–‡ä»¶ï¼Œä½†æ˜¯å¦‚æœæˆ‘æŒ‰ç…§éœ€è¦çš„åˆ†åŒºæ¥ä½œå“ˆå¸Œçš„è¯ï¼Œä¾‹å¦‚repartition(user_range,md5_range,event_day)æ¥çš„è¯ï¼Œé‚£ä¹ˆæ¯ä¸ªåˆ†åŒºçš„æ•°æ®åªä¼šå­˜åœ¨åœ¨ä¸€ä¸ªæœ€åå†™å…¥çš„taskä»»åŠ¡ä¸­ï¼Œä¹Ÿå°±ä¿è¯äº†æ•´ä¸ªä»»åŠ¡äº§ç”Ÿçš„åˆ†åŒºæ•°æœ€å¤§æ˜¯num(user_range)<em>num(md5_range)</em>num(event_day)</p>
<p>è€Œä¸æ˜¯åŸæ¥çš„num(repartition)<em>num(user_range)</em>num(md5_range)*num(event_day)</p>
<p>å› æ­¤æˆ‘ä»¬å¯ä»¥éšæ„å¯å¹¶å‘æ•°ã€‚</p>
<p>å°†å†™å…¥ä»£ç ä¿®æ”¹ä¸ºï¼š</p>
<p>txtdf.repartition(txtdf(â€œuser_rangeâ€), txtdf(â€œmd5_rangeâ€), txtdf(â€œevent_dayâ€)).write.mode(org.apache.spark.sql.SaveMode.Append).partitionBy(â€œuser_rangeâ€,â€md5_rangeâ€,â€event_dayâ€).parquet(destFile)</p>
<p>è¿™æ ·ä¿®æ”¹äº†ä¹‹åã€‚æ•°æ®5åˆ†é’Ÿå·¦å³å°±å…¨éƒ¨å†™å…¥äº†ã€‚</p>
<p>  9208         4556         3297227427 /horus/users/chenxiue</p>
<p>æ•°æ®çš„å¤§å°ä¹Ÿæ¯”ä¹‹å‰å°äº†ä¸€äº›ï¼Œå› ä¸ºæ–‡ä»¶æ›´åŠ é›†ä¸­äº†ã€‚</p>
<p>ä¸‹é¢æˆ‘ä»¬å¯¹æ¯”ä¸€ä¸‹gzipå‹ç¼©å’Œsnappyå‹ç¼©çš„æ•ˆæœï¼š</p>
<p>1ï¼‰å‹ç¼©æ¯”ç‡</p>
<p>åŸå§‹æ–‡ä»¶å¤§å°ï¼š</p>
<p>1          517         9359700501 hdfs://namenode:8700/pika_data/file_meta_data_20170101_part4/2017011421/1483873213877</p>
<p>snappyå‹ç¼©äº§å‡ºçš„æ–‡ä»¶æ ¼å¼ç±»ä¼¼ï¼špart-r-00000-b3ff5d89-8885-42f1-bb3d-e8dc6fb692a0.snappy.parquet</p>
<p>å‹ç¼©åçš„æ–‡ä»¶å¤§å°ï¼š<br>9206        22157         5501724946 /horus/users/chenxiue</p>
<p>å¤§æ¦‚èŠ±äº†18åˆ†é’Ÿå·¦å³ã€‚</p>
<p>gzipå‹ç¼©æ˜¯ç±»ä¼¼ï¼špart-r-00003-be2d54a8-088e-4970-be30-363747930a6e.gz.parquetè¿™æ ·çš„æ–‡ä»¶</p>
<p>å¤§æ¦‚ä¹ŸèŠ±äº†18åˆ†é’Ÿå·¦å³<br>9206        22157         3958666080 /horus/users/chenxiue1</p>
<p>å¯ä»¥çœ‹å‡ºgzipçš„å‹ç¼©æ¯”æ›´åŠ å¤§ä¸€äº›ã€‚</p>
<p>2ï¼‰åœ¨éšæœºåˆ†åŒºå¾—æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°è¯•åŠ å¤§æœ€åå†™å…¥çš„å¹¶å‘åº¦ï¼Œçœ‹çœ‹ä¼šä¸ä¼šæœ‰åŠ é€Ÿï¼Ÿ</p>
<p>txtdf.repartition(10).write.mode(org.apache.spark.sql.SaveMode.Append).partitionBy(â€œuser_rangeâ€,â€md5_rangeâ€,â€event_dayâ€).parquet(destFile);</p>
<p>ç»“æœå†™å…¥èŠ±äº†19åˆ†é’Ÿï¼Ÿï¼Ÿï¼Ÿä¸ºå•¥ï¼Ÿï¼Ÿ<br>9208        44158         4259882305 /horus/users/chenxiue<br>å‘ç°å†™å…¥çš„å¤§å°æ¯”åŸå…ˆçš„ç¨å¾®å¤§ä¸€äº›ã€‚</p>
<p>æ€€ç–‘æ˜¯æ•°æ®å¤ªå°ï¼Œä¸»è¦æ—¶é—´èŠ±åœ¨å»ºç«‹æ–‡ä»¶ä¸Šã€‚repartitonè¶Šå¤§çš„è¯ï¼Œæ–‡ä»¶æ•°å°±è¶Šå¤šã€‚</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/13/sparkä¸­parquetæ–‡ä»¶å†™å…¥ä¼˜åŒ–/" data-id="ckaafjsgg0004ci3cmmg8o5qi" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-åœ¨kerberos-HAç¯å¢ƒä¸‹çš„rangerç¼–è¯‘å®‰è£…" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/13/åœ¨kerberos-HAç¯å¢ƒä¸‹çš„rangerç¼–è¯‘å®‰è£…/" class="article-date">
  <time datetime="2017-04-13T06:44:39.000Z" itemprop="datePublished">2017-04-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/éƒ¨ç½²æ–‡æ¡£/">éƒ¨ç½²æ–‡æ¡£</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/13/åœ¨kerberos-HAç¯å¢ƒä¸‹çš„rangerç¼–è¯‘å®‰è£…/">åŸåˆ›-åœ¨kerberos+HAç¯å¢ƒä¸‹çš„rangerç¼–è¯‘å®‰è£…</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      <span id="busuanzi_container_page_pv">
        <br /><br /> é˜…è¯»æ¬¡æ•°<span id="busuanzi_value_page_pv"></span>
        </span>
      
        <p>æ ‡ç­¾ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰ï¼š éƒ¨ç½²æ–‡æ¡£ï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„</p>
<hr>
<p>1.ä»£ç ä¸‹è½½&amp;ç¼–è¯‘</p>
<p>git clone <a href="https://github.com/apache/incubator-ranger.git" target="_blank" rel="external">https://github.com/apache/incubator-ranger.git</a><br>cd incubator-ranger<br>git checkout ranger-0.5</p>
<p>mvn clean compile package assembly:assembly install</p>
<p>ä¸‹è½½çš„è¿‡ç¨‹ä¸­é‡åˆ°python hashåº“çš„é—®é¢˜ï¼Œé‡æ–°å®‰è£…ä¸‹pythonå³å¯<br>å¦å¤–ç»å¸¸å› ä¸ºä¸‹è½½åº“è¿‡ç¨‹è¶…æ—¶ï¼Œé‡è¯•å‡ æ¬¡å°±å¥½äº†</p>
<p>ç¼–è¯‘å¥½çš„ç›®å½•åœ¨targetç›®å½•ä¸‹ã€‚</p>
<p>2.æ§åˆ¶å°ranger-adminçš„å®‰è£…</p>
<p>1ï¼‰å®‰è£…mysqlæ•°æ®åº“</p>
<p>é…ç½®my.cnf:</p>
<p>basedir =/home/bae/dataplatform/jumbo<br>datadir =/home/bae/dataplatform/jumbo/var<br>port = 3309<br>socket = /home/bae/dataplatform/jumbo/var/mysql.sock</p>
<p>å¯åŠ¨mysql:<br>./bin/mysql_install_db<br>./share/mysql/mysql.server start</p>
<p>2)ç”Ÿæˆå„ä¸ªæ¨¡å—çš„keytab</p>
<p>addprinc -randkey rangeradmin/hostA@EXAMPLE.COM</p>
<p>xst -k /home/bae/dataplatform/kerberos/keytab/rangeradmin.keytab rangeradmin/hostA@EXAMPLE.COM</p>
<p>addprinc -randkey rangerlookup/hostA@EXAMPLE.COM</p>
<p>xst -k /home/bae/dataplatform/kerberos/keytab/rangerlookup.keytab rangerlookup/hostA@EXAMPLE.COM</p>
<p>addprinc -randkey rangerusersync/hostA@EXAMPLE.COM</p>
<p>xst -k /home/bae/dataplatform/kerberos/keytab/rangerusersync.keytab rangerusersync/hostA@EXAMPLE.COM</p>
<p>addprinc -randkey rangertagsync/hostA@EXAMPLE.COM</p>
<p>xst -k /home/bae/dataplatform/kerberos/keytab/rangertagsync.keytab rangertagsync/hostA@EXAMPLE.COM</p>
<p>3)é…ç½®ranger-admin</p>
<p>å°†ranger-0.5.4-SNAPSHOT-admin.tar.gzè§£å‹åˆ°å®‰è£…ç›®å½•ä¸‹ï¼Œä¿®æ”¹install.properties,éœ€è¦ä¿®æ”¹çš„é€‰é¡¹å¦‚ä¸‹ï¼š</p>
<pre><code>SQL_CONNECTOR_JAR=/home/bae/dataplatform/jumbo/lib/mysql/mysql-connector-java-5.1.41-bin.jar
db_root_user=root
db_root_password=
db_host=hostMysql:3309

db_name=ranger
db_user=rangeradmin
db_password=123456


audit_store=db
audit_db_name=ranger_audit
audit_db_user=rangerlogger
audit_db_password=123456


policymgr_external_url=http://localhost:8070
policymgr_http_enabled=true

unix_user=work
unix_group=work


spnego_principal=HTTP/hostA@EXAMPLE.COM
spnego_keytab=/home/bae/dataplatform/kerberos/keytab/spnego.service.keytab
token_valid=30
cookie_domain=hostA
cookie_path=/

admin_principal=rangeradmin/hostA@EXAMPLE.COM
admin_keytab=/home/bae/dataplatform/kerberos/keytab/rangeradmin.keytab
lookup_principal=rangerlookup/hostA@EXAMPLE.COM
lookup_keytab=/home/bae/dataplatform/kerberos/keytab/rangerlookup.keytab
</code></pre><p>è¿è¡Œ./setup.shï¼ˆrootè¿è¡Œï¼Œå¦åˆ™æŠ¥groupaddæ²¡æœ‰æƒé™ï¼‰</p>
<p>é‡åˆ°é—®é¢˜ï¼š</p>
<p>aï¼‰æŠ¥é”™ï¼š</p>
<p>SQLException : SQL state: 28000 java.sql.SQLException: Access denied for user â€˜rangeradminâ€™@â€™hostAâ€™ (using password: YES) ErrorCode: 1045</p>
<p>æŸ¥çœ‹userè¡¨ï¼Œè¯¥ç”¨æˆ·å·²ç»åˆ›å»ºï¼Œä½†æ˜¯æœºå™¨æ²¡æœ‰è¢«æˆæƒ</p>
<pre><code>create user &apos;rangeradmin&apos;@&apos;hostA&apos; identified by &apos;123456&apos;;
flush privileges;
</code></pre><p>bï¼‰ä¿®æ”¹äº†policymgr_external_url=<a href="http://localhost:8070ç«¯å£ï¼Œå‘ç°8070ç«¯å£æ²¡æœ‰å¯åŠ¨æˆåŠŸ" target="_blank" rel="external">http://localhost:8070ç«¯å£ï¼Œå‘ç°8070ç«¯å£æ²¡æœ‰å¯åŠ¨æˆåŠŸ</a></p>
<p>åœ¨conf/ranger-admin-site.xmlä¸­å‘ç°</p>
<pre><code>&lt;property&gt;
&lt;name&gt;ranger.service.http.port&lt;/name&gt;
&lt;value&gt;6080&lt;/value&gt;
&lt;/property&gt;
</code></pre><p>  è¿™é‡Œéœ€è¦ä¿®æ”¹.</p>
<p>cï¼‰range-admin stop/starté‡æ–°å¯åŠ¨åå°±å¯ä»¥çœ‹åˆ°äº†ã€‚æ³¨æ„tomcatçš„æ—¥å¿—åœ¨ews/logs/catalina.outå½“ä¸­</p>
<p>éªŒè¯æ˜¯å¦æˆåŠŸï¼šæ‰“å¼€<a href="http://localhost:8070ï¼Œä½¿ç”¨admin/adminç™»å½•" target="_blank" rel="external">http://localhost:8070ï¼Œä½¿ç”¨admin/adminç™»å½•</a></p>
<p>3.å®‰è£…usersyncè¿›ç¨‹</p>
<p>è¿™ä¸ªå®‰è£…çš„ç›®çš„æ˜¯åŒæ­¥unixï¼Œæˆ–è€…ldapä¸­çš„ç”¨æˆ·åˆ°rangerä¸­ã€‚</p>
<p>æ‹·è´ç¼–è¯‘å¥½çš„ranger-0.5.4-SNAPSHOT-usersync.tar.gzåˆ°é€‚å½“ç›®å½•å¹¶è§£å‹</p>
<p>ä¿®æ”¹install.properties:ï¼ˆåŒæ­¥æœ¬æœºçš„unixç”¨æˆ·ï¼‰</p>
<pre><code>POLICY_MGR_URL = http://localhost:8070
# sync source,  only unix and ldap are supported at present
# defaults to unix
SYNC_SOURCE = unix
#User and group for the usersync process
unix_user=work
unix_group=work
logdir=/home/bae/dataplatform/ranger-0.5.4-SNAPSHOT-usersync/logs/ranger/usersync
usersync_principal=rangerusersync/hostA@EXAMPLE.COM
usersync_keytab=/home/bae/dataplatform/kerberos/keytab/rangerusersync.keytab
hadoop_conf=/home/bae/dataplatform/hadoop/conf/
</code></pre><p>ä½¿ç”¨rootè´¦å·è¿è¡Œ./setup.sh<br>å¯åŠ¨usersync:/ranger-usersync-services.sh start<br>éªŒè¯æ˜¯å¦æˆåŠŸï¼šåœ¨rangeræ§åˆ¶å°çš„settingsï¼&gt;Users/Groupsä¿¡æ¯çœ‹æœ¬æœºçš„è´¦å·æ˜¯å¦å·²ç»è¢«åŒæ­¥ä¸Šæ¥ã€‚</p>
<p>4.hdfs-pluginå®‰è£…(åªéœ€è¦åœ¨å¯¹åº”é›†ç¾¤çš„ä¸»å¤‡namenodeä¸Šå®‰è£…)</p>
<p>ä¸ºäº†è®©rangerèƒ½å¤Ÿæ§åˆ¶hdfsï¼Œéœ€è¦å®‰è£…plugin</p>
<p>æ‹·è´ranger-0.5.4-SNAPSHOT-hbase-plugin.tar.gzåˆ°å¯¹åº”ç›®å½•å¹¶è§£å‹ã€‚ä¿®æ”¹install.properties</p>
<pre><code>POLICY_MGR_URL=http://hostA:8070
SQL_CONNECTOR_JAR=/home/bae/dataplatform/jumbo/lib/mysql/mysql-connector-java-5.1.41-bin.jar
REPOSITORY_NAME=hadoopdev(ä¸åç»­é¡µé¢ä¸Šé…ç½®çš„ä¸€è‡´)
XAAUDIT.DB.IS_ENABLED=true
XAAUDIT.DB.FLAVOUR=MYSQL
XAAUDIT.DB.HOSTNAME=hostA:3309
XAAUDIT.DB.DATABASE_NAME=ranger_audit
XAAUDIT.DB.USER_NAME=rangeradmin
XAAUDIT.DB.PASSWORD=123456

CUSTOM_USER=work
CUSTOM_GROUP=work
</code></pre><p>åˆ›å»ºåˆ°hadoop_confçš„è½¯é“¾ï¼š<br>ln -s /home/bae/dataplatform/hadoop-2.7.2  /home/bae/dataplatform/hadoop<br>ln -s /home/bae/dataplatform/hadoop-2.7.2/etc/hadoop/ /home/bae/dataplatform/hadoop-2.7.2/conf</p>
<p>ç¡®è®¤$HADOOP_HOMEä¸‹é¢æœ‰libç›®å½•ï¼Œå¦‚æœæ²¡æœ‰éœ€è¦ç¼–è¯‘native libï¼Œç¼–è¯‘æ–¹æ³•ï¼š</p>
<p><a href="http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/NativeLibraries.html" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/NativeLibraries.html</a></p>
<p>ä½¿ç”¨rootè´¦å·å¯åŠ¨hdfs-plugin:</p>
<p>./enable-hdfs-plugin.sh(rootèº«ä»½è¿è¡Œ)</p>
<p>é‡å¯namenodeè¿›ç¨‹ï¼š</p>
<p>å°†$HADOOP_HOME/libä¸‹é¢æ–°å¢çš„ranger jaræ·»åŠ åˆ°hadoop_classpathå˜é‡ä¸­ï¼Œ</p>
<p>åœ¨conf/hadoop-env.shä¸­æ·»åŠ ï¼š</p>
<pre><code>for f in $HADOOP_HOME/lib/*.jar; do
  if [ &quot;$HADOOP_CLASSPATH&quot; ]; then
    export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$f
  else
    export HADOOP_CLASSPATH=$f
  fi
done
</code></pre><p>é‡æ–°å¯åŠ¨namenodeï¼ˆå¦‚æœæŠ¥jdbcæ–¹æ³•æ‰¾ä¸åˆ°é—®é¢˜ï¼Œå°±å°†mysql-connector-java-5.1.41-bin.jaræ‹·è´åˆ°$HADOOP_HOME/libç›®å½•ä¸‹åé‡å¯ï¼‰</p>
<p>5.åœ¨rangerçš„æ§åˆ¶å°ä¸­å¢åŠ pluginé…ç½®ï¼š</p>
<p>é¦–å…ˆåˆ›å»ºä¸€ä¸ªkerberosçš„ç”¨æˆ·åå¯†ç </p>
<p>addprinc -pw password rangeradmin@example.com</p>
<p>ä¿®æ”¹core-site.xmlå¢åŠ æ˜ å°„ï¼š</p>
<pre><code>RULE:[2:$1@$0](rangeradmin@EXAMPLE.COM)s/.*/work/
RULE:[1:$1@$0](rangeradmin@EXAMPLE.COM)s/.*/work/
</code></pre><p>é‡å¯namenodeä½¿å…¶ç”Ÿæ•ˆï¼Œé‡å¯ranger-admin</p>
<p>åœ¨Service Manager-&gt;hdfsä¸­å¢åŠ hadoopdevï¼ˆåç§°ä¸hdfs pluginä¸­è®¾ç½®çš„ä¸€è‡´)repo</p>
<pre><code>username:rangeradmin@example.com
password:password
namenode url:hdfs://hostB:8900
Authorization Enabled:yes
Authentication Type:kerberos
hadoop.security.auth_to_local:RULE:[1:$1@$0](rangeradmin@EXAMPLE.COM)s/.*/work/
dfs.datanode.kerberos.principal:dn/_HOST@EXAMPLE.COM
dfs.namenode.kerberos.principal:nn/_HOST@EXAMPLE.COM
dfs.secondary.namenode.kerberos.principal:nn/_HOST@EXAMPLE.COM
RPC Protection Type:Authentication

dfs.nameservices = smallfile
dfs.ha.namenodes.smallfile= nn1,nn2
dfs.namenode.rpc-address.nn1 = hostB:8900
dfs.namenode.rpc-address.nn2 = hostC:8900
dfs.client.failover.proxy.provider.smallfile = org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
</code></pre><p>å…¶ä¸­username/passwordéœ€è¦æ˜¯kerberosä¸­æœ‰æ•ˆçš„ç”¨æˆ·åå¯†ç ã€‚</p>
<p>ç‚¹å‡»test connectionå¦‚æœæˆåŠŸï¼Œé‚£ä¹ˆsave.</p>
<p>éªŒè¯pluginæ˜¯å¦æ·»åŠ æˆåŠŸï¼šåœ¨audit-&gt;pluginç›®å½•ä¸‹æ˜¯å¦å‡ºç°å¯¹åº”çš„pluginä¿¡æ¯ã€‚</p>
<p>6.rangerå¯¹hdfsæˆæƒæµ‹è¯•</p>
<p>æ³¨æ„é¦–å…ˆè¦åœ¨hdfsä¸Šå°†æƒé™æ”¶å›ï¼Œæ¯”å¦‚æŠŠä¸€ä¸ªç›®å½•æƒé™è®¾ç½®æˆ000ï¼Œè¿™æ ·å°±å®Œå…¨ç”±ranger policyæ§åˆ¶ã€‚å¦åˆ™ç”Ÿæ•ˆçš„éƒ½æ˜¯hdfsä¸Šçš„å¤§æƒé™ã€‚</p>
<p>å¯ä»¥é€šè¿‡audit-&gt;accessä¸­å¾—Access Enforcerçœ‹ç”Ÿæ•ˆå¾—æ˜¯ranger-aclè¿˜æ˜¯hadoop-acl</p>
<p>å‚è€ƒæ–‡æ¡£ï¼š</p>
<p><a href="https://cwiki.apache.org/confluence/display/RANGER/Apache+Ranger+0.5.0+Installation#ApacheRanger0.5.0Installation-InstallandconfigureSolrorSolrCloud" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/RANGER/Apache+Ranger+0.5.0+Installation#ApacheRanger0.5.0Installation-InstallandconfigureSolrorSolrCloud</a></p>
<p>åœ¨kerberosç¯å¢ƒä¸‹å®‰è£…ranger:</p>
<p><a href="https://cwiki.apache.org/confluence/display/RANGER/Ranger+installation+in+Kerberized++Environment" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/RANGER/Ranger+installation+in+Kerberized++Environment</a></p>
<p><a href="https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.2/bk_Security_Guide/content/hdfs_plugin_kerberos.html" target="_blank" rel="external">https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.2/bk_Security_Guide/content/hdfs_plugin_kerberos.html</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/13/åœ¨kerberos-HAç¯å¢ƒä¸‹çš„rangerç¼–è¯‘å®‰è£…/" data-id="ckaafjsgs0007ci3cq36oe6fu" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>



  

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/æ€§èƒ½ä¼˜åŒ–/">æ€§èƒ½ä¼˜åŒ–</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/æºç è°ƒè¯•/">æºç è°ƒè¯•</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/éƒ¨ç½²æ–‡æ¡£/">éƒ¨ç½²æ–‡æ¡£</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/05/17/hugegraph-cassadraåº•å±‚å­˜å‚¨å®ç°/">hugegraph+cassadraåº•å±‚å­˜å‚¨å®ç°</a>
          </li>
        
          <li>
            <a href="/2020/05/17/spark-graphxè®¡ç®—pagerankæºä»£ç åˆ†æ/">spark graphxè®¡ç®—pagerankæºä»£ç åˆ†æ</a>
          </li>
        
          <li>
            <a href="/2018/03/29/druidå®‰è£…é…ç½®-æ•°æ®å¯¼å…¥/">druidå®‰è£…é…ç½®&amp;æ•°æ®å¯¼å…¥</a>
          </li>
        
          <li>
            <a href="/2018/03/02/sparkè¿œç¨‹è°ƒè¯•/">sparkè¿œç¨‹è°ƒè¯•</a>
          </li>
        
          <li>
            <a href="/2018/01/11/åŸåˆ›ï¼flumeå†™hdfsæ€§èƒ½ä¼˜åŒ–/">åŸåˆ›-flumeå†™hdfsæ€§èƒ½ä¼˜åŒ–</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 XiueChen<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
      <span id="busuanzi_container_site_uv"><br />æœ¬ç«™è®¿å®¢æ•°<span id="busuanzi_value_site_uv"></span>äººæ¬¡
      <span id="busuanzi_container_site_pv"><br />æœ¬ç«™è®¿é—®æ¬¡æ•°<span id="busuanzi_value_site_pv"></span>
      </span>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>